<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Julio Sheinbaum</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Julio Sheinbaum</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./presentation.html"> 
<span class="menu-text">Test Reveal Presentations</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./cvprod.html"> 
<span class="menu-text">CV-PRODUCTS</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./basics_jupyter.html"> 
<span class="menu-text">Matplotlib Demo</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./bickley_julia.html"> 
<span class="menu-text">Bickley Jet from CoherentStructures.jl</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./chap4.html"> 
<span class="menu-text">Notas del Curso Análisis de Datos</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#estadística-y-conceptos-de-probabilidad" id="toc-estadística-y-conceptos-de-probabilidad" class="nav-link active" data-scroll-target="#estadística-y-conceptos-de-probabilidad">Estadística y conceptos de probabilidad</a>
  <ul class="collapse">
  <li><a href="#estadística-básica" id="toc-estadística-básica" class="nav-link" data-scroll-target="#estadística-básica">Estadística básica</a></li>
  </ul></li>
  <li><a href="#probabilidad" id="toc-probabilidad" class="nav-link" data-scroll-target="#probabilidad">Probabilidad</a>
  <ul class="collapse">
  <li><a href="#distribuciones-de-probabilidad" id="toc-distribuciones-de-probabilidad" class="nav-link" data-scroll-target="#distribuciones-de-probabilidad">Distribuciones de probabilidad:</a></li>
  </ul></li>
  <li><a href="#repaso-de-álgebra-lineal" id="toc-repaso-de-álgebra-lineal" class="nav-link" data-scroll-target="#repaso-de-álgebra-lineal">Repaso de álgebra Lineal</a></li>
  <li><a href="#cuadrados-mínimos-y-regresión" id="toc-cuadrados-mínimos-y-regresión" class="nav-link" data-scroll-target="#cuadrados-mínimos-y-regresión">Cuadrados mínimos y regresión</a>
  <ul class="collapse">
  <li><a href="#métodos-de-cuadrados-mínimos" id="toc-métodos-de-cuadrados-mínimos" class="nav-link" data-scroll-target="#métodos-de-cuadrados-mínimos">Métodos de cuadrados mínimos</a></li>
  </ul></li>
  <li><a href="#propagación-de-errores" id="toc-propagación-de-errores" class="nav-link" data-scroll-target="#propagación-de-errores">Propagación de errores</a></li>
  <li><a href="#métodos-de-interpolación" id="toc-métodos-de-interpolación" class="nav-link" data-scroll-target="#métodos-de-interpolación">Métodos de Interpolación</a>
  <ul class="collapse">
  <li><a href="#interpolación-lineal" id="toc-interpolación-lineal" class="nav-link" data-scroll-target="#interpolación-lineal">Interpolación Lineal</a></li>
  <li><a href="#interpolación-polinómica" id="toc-interpolación-polinómica" class="nav-link" data-scroll-target="#interpolación-polinómica">Interpolación polinómica</a></li>
  <li><a href="#spline-cúbico" id="toc-spline-cúbico" class="nav-link" data-scroll-target="#spline-cúbico">Spline cúbico</a></li>
  </ul></li>
  <li><a href="#interpolación-objetiva" id="toc-interpolación-objetiva" class="nav-link" data-scroll-target="#interpolación-objetiva">Interpolación Objetiva</a></li>
  <li><a href="#funciones-empíricas-ortogonales-feos" id="toc-funciones-empíricas-ortogonales-feos" class="nav-link" data-scroll-target="#funciones-empíricas-ortogonales-feos">Funciones Empíricas Ortogonales (FEOs)</a>
  <ul class="collapse">
  <li><a href="#interpretación-de-los-sistemas-propios" id="toc-interpretación-de-los-sistemas-propios" class="nav-link" data-scroll-target="#interpretación-de-los-sistemas-propios">Interpretación de los sistemas `propios’</a></li>
  <li><a href="#definición-de-feos" id="toc-definición-de-feos" class="nav-link" data-scroll-target="#definición-de-feos">Definición de FEOs</a></li>
  <li><a href="#teoría" id="toc-teoría" class="nav-link" data-scroll-target="#teoría">Teoría</a></li>
  </ul></li>
  <li><a href="#análisis-espectral-o-análisis-de-fourier" id="toc-análisis-espectral-o-análisis-de-fourier" class="nav-link" data-scroll-target="#análisis-espectral-o-análisis-de-fourier">Análisis espectral o análisis de Fourier</a>
  <ul class="collapse">
  <li><a href="#introducción" id="toc-introducción" class="nav-link" data-scroll-target="#introducción">Introducción</a></li>
  <li><a href="#serie-de-fourier" id="toc-serie-de-fourier" class="nav-link" data-scroll-target="#serie-de-fourier">Serie de Fourier</a></li>
  </ul></li>
  <li><a href="#métodos-de-filtrado-y-suavizado" id="toc-métodos-de-filtrado-y-suavizado" class="nav-link" data-scroll-target="#métodos-de-filtrado-y-suavizado">Métodos de filtrado y suavizado</a>
  <ul class="collapse">
  <li><a href="#convolución-y-funciones-respuesta-ventanas-espectrales" id="toc-convolución-y-funciones-respuesta-ventanas-espectrales" class="nav-link" data-scroll-target="#convolución-y-funciones-respuesta-ventanas-espectrales">Convolución y funciones respuesta (ventanas espectrales)</a></li>
  <li><a href="#promedio-corrido" id="toc-promedio-corrido" class="nav-link" data-scroll-target="#promedio-corrido">Promedio corrido</a></li>
  <li><a href="#filtros-generales-coseno" id="toc-filtros-generales-coseno" class="nav-link" data-scroll-target="#filtros-generales-coseno">Filtros generales coseno</a></li>
  <li><a href="#filtro-lanczos-pasabaja" id="toc-filtro-lanczos-pasabaja" class="nav-link" data-scroll-target="#filtro-lanczos-pasabaja">Filtro Lanczos pasabaja</a></li>
  <li><a href="#filtro-lanczos-pasabanda" id="toc-filtro-lanczos-pasabanda" class="nav-link" data-scroll-target="#filtro-lanczos-pasabanda">Filtro Lanczos pasabanda</a></li>
  </ul></li>
  <li><a href="#temas-selectos" id="toc-temas-selectos" class="nav-link" data-scroll-target="#temas-selectos">Temas selectos</a>
  <ul class="collapse">
  <li><a href="#análisis-armónico" id="toc-análisis-armónico" class="nav-link" data-scroll-target="#análisis-armónico">Análisis Armónico</a></li>
  <li><a href="#demodulación-compleja" id="toc-demodulación-compleja" class="nav-link" data-scroll-target="#demodulación-compleja">Demodulación compleja</a></li>
  </ul></li>
  <li><a href="#acknowledgments" id="toc-acknowledgments" class="nav-link" data-scroll-target="#acknowledgments">Acknowledgments</a></li>
  <li><a href="#open-research" id="toc-open-research" class="nav-link" data-scroll-target="#open-research">Open research</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">




<p>m– title: “Notas Curso Análisis de datos” format: pdf: agu-pdf: keep-tex: true agu-html: default author: - name: Enric Pallas affiliations: - name: Centro de Investigación Científica y de Educación Superior de Ensenada, CICESE department: Physical Oceanography, address: Carretera Ensenada-Tijuana 3018 city: Ensenada region: Baja California country: MEXICO postal-code: 22860 orcid: 0000-0001-000-000 email: epallas@cicese.mx url: https://cicese.edu.mx/~epallas - name: Julio Sheinbaum affiliations: - name: Centro de Investigación Científica y de Educación Superior de Ensenada, CICESE department: Physical Oceanography, address: Carretera Ensenada-Tijuana 3018 city: Ensenada region: Baja California country: MEXICO postal-code: 22860 orcid: 0000-0001-7031-5225 email: julios@cicese.mx url: https://jsheinbaum.github.io acknowledgements: Translated template to Quarto.</p>
<p>abstract: | En el océano conviven una gran cantidad de corrientes de diferentes escalas espaciales y temporales. Las escalas espaciales típicas de la circulación oceánica son la larga escala, la mesoescala, submesoescala, y microescala. La larga escala es del <span class="math inline">\({\cal O}(1000\,km)\)</span> y esta determinada por la circulación general en el océano como la termohalina y los grandes giros anticiclónicos de las grandes cuencas oceánicas; las escalas temporales de la larga escala varia entre meses y años. La mesoescala esta definida por corrientes del <span class="math inline">\({\cal O}(100\,km)\)</span> como remolinos, corrientes costeras, filamentos, frentes, etc. Son corrientes mas regionales pero pueden tener gran influencia sobre la circulación general o de larga escala. Sus escalas temporales son de semanas a meses. La submesoescala corresponde a corrientes del <span class="math inline">\({\cal O}(10\,km)\)</span> de caracter local remolinos, filamentos, frentes, corrientes en playas, puertos, y estuarios. La submesoscala varía temporalmente con rapidez en tiempos que varían de horas a días. Finalmente podemos hablar de la microescala que son remolinos del orden de centímetros a metros y generalmente es la escala característica de la turbulencia que transfiere energía desde la submesoescala hacia la disipación molecular. Aquí podemos hablar de fenómenos del orden de segundos y minutos.</p>
<p>keywords: [] key-points: - Probar interactividad y teoría - Jupyter,Julia,Pluto. - Vamos viendo. bibliography: bibliography.bib<br>
citation: container-title: Geophysical Research Letters keep-tex: true date: last-modified</p>
<hr>
<!--
\includepdf[pages={-}]{/Users/julios/curso_analisis_de_datos/Instrumentacion_2023.pdf}
-->
<section id="estadística-y-conceptos-de-probabilidad" class="level2">
<h2 class="anchored" data-anchor-id="estadística-y-conceptos-de-probabilidad">Estadística y conceptos de probabilidad</h2>
<p>A pesar de nuestra formación determinista a la hora de resolver problemas matemáticos y aunque consideremos que las ecuaciones de Navier-Stokes que describen el movimiento del océano son deterministas, la estadística es ampliamente utilizada en oceanografía debido a diferentes razones:</p>
<ol type="1">
<li><p>Para una descripción completa del océano es necesario especificar una gran cantidad de variables, muchas de las cuales son desconocidas. Un ejemplo de ello son las parametrizaciones que se hacen en oceanografía para describir variables que no pueden medirse directamente. Una parametrización no es nadammas que un modelo estadístico que explica la evolución de una variable dependiente de otras variables independientes. Por ejemplo, parametrización del esfuerzo del viento en función del corte vertical o parametrización del coeficiente de arrastre en función de la velocidad del viento a 10m de la superfície del océano.</p></li>
<li><p>El océano es altamente no lineal. La evolución de una cierta variable no se puede estudiar de forma aislada.</p></li>
</ol>
<p>Ejemplo:<br>
Supongamos el término de aceleración horizontal en las ecuaciones de Navier Stokes para fluidos incompresibles,</p>
<span class="math display">\[\begin{equation}
\frac{\partial{\mathbf {u}_h}}{\partial{t}} + \mathbf {u}_h \cdot {\nabla}_h \mathbf {{u}_h}
\end{equation}\]</span>
<p>Como ya sabemos por el curso de Mecánica de Fluidos, la aceleración de un fluido es una derivada material y consta de un término local (aceleración local) y de un término advectivo o aceleración advectiva. En general, esta ecuación no se aplica a partículas de agua individuales.</p>
<p>En oceanografía hablamos de continuo. No estamos interesados en las características cinemáticas de las partículas individuales sino en la manifestación promedia del movimiento molecular, es decir, del fluido como un conjunto o contínuo. Es decir, asumimos que el fluido es uniforme en el espacio que ocupa sin considerar la estructura molecular.</p>
<p>Por ello debemos de promediar de alguna forma para explicar el comportamiento conjunto del fluído y no de una partícula de agua específica? Y como se realiza tal promedio? En general, el promediado se realiza de tal forma que nos permite separar la larga escala que trataremos como determinística, de la pequeña escala que consideramos un proceso aleatorio (turbulento). Supongamos entonces la separación de la velocidad horizontal en una velocidad promedio y una velocidad fluctuante alrededor de la media</p>
<p><span class="math display">\[{\mathbf u_h} = &lt;{\mathbf u_h}&gt; + {\mathbf u'_h} \]</span></p>
<p>donde $ &lt; &gt; $ denotan promedio. Si aplicamos esta descomposición a la componente <span class="math inline">\(x\)</span> de la aceleración obtenemos:</p>
<span class="math display">\[\begin{equation}
\frac{\partial &lt;\mathbf {u}&gt;}{\partial t} +  &lt;{\mathbf u}&gt; \cdot  {\nabla} &lt;{\mathbf u} &gt;  + &lt; {\mathbf u'} \cdot {\nabla} \mathbf {u}'&gt;
\end{equation}\]</span>
<span class="math display">\[\begin{equation}
\frac{\partial \mathbf {&lt; u &gt;}}{\partial t} + \nabla \cdot (&lt;{\mathbf u}&gt; \textbf {&lt; u &gt;}) + \nabla \cdot&lt; \mathbf {u'} \mathbf {u}' &gt;
\end{equation}\]</span>
<p>Donde usamos la ecuación de continuidad</p>
<p><span class="math display">\[ \nabla \cdot (&lt; \mathbf {u} &gt; + \mathbf {u}') =0 \]</span></p>
<p>para pasar de la primera a la segunda expresión.<br>
</p>
<!--
comment

$$
\frac{\partial \mathbf{<u>}}{\partial t} + < {\textbf u} > \cdot { \nabla}{ \textbf <u> } + < {\mathbf u'}\cdot {\nabla} {\textbf u}' >
$$

-->
<p>Inevitablemente, las pequeñas escalas o fluctuaciones respecto a la larga escala aparecen en la expresión de la aceleración de larga escala. De forma que la separación que deseamos no es tan simple ya que debemos de conocer la estadística de la pequeña escala para poder describir la circulación media.</p>
<p>El término <span class="math inline">\(&lt;\textbf {u}'_h u'&gt;\)</span> se denomina esfuerzo de Reynolds y nos informa de la correlación entre las componentes fluctuantes (alta frecuencia) de la velocidad. Por ejemplo, <span class="math inline">\(&lt;u'v'&gt; = 0\)</span> significa que no existe i correlación y hablamos de isotropía. Si <span class="math inline">\(&lt;u'v'&gt; &lt; 0\)</span>, significa que las fluctuaciones están inversamente correlacionadas, i.e., anisotropía.</p>
<p>Este es un gran problema no resuelto en la oceanografía física. El esfuerzo de Reynolds aparece porque la advección es no-lineal de tal forma que no podemos estudiar la larga escala sin conocer información de la pequeña escala que es un proceso aleatorio. Por similitud con el flujo laminar, los términos de esfuerzo de Reynolds se parametrizan estadísticamente como proporcionales a los gradientes de velocidad. El factor de proporcionalidad es el coeficiente de viscosidad, en este caso, turbulento. Es aqui donde utilizar herramientas estadísticas tiene sentido.</p>
<ol start="3" type="1">
<li>No podemos controlar las variables oceanográficas; estan en constante cambio a medida que el sistema observado evoluciona.</li>
</ol>
<p>Ejemplo:<br>
En el océano coexisten mareas, ondas internas, remolinos, turbulencia de pequeña escala,…las cuales enmascaran el fenómeno oceanográfico que estamos interesados en estudiar. Estos procesos incontrolables por el oceanógrafo en ocasiones es útil considerarlos aleatorios y utilizar herramientas estadísticas para caracterizarlos.</p>
<p>Imaginemos que queremos conocer cual es la temperatura superficial promedio en la bahía de Todos Santos. Una forma de proceder sería promediar todos los datos de temperatura superficial que disponemos de los últimos 100 años y promediarlos? Pero, ¿es realmente lo que deseamos? ¿Deberíamos de considerar la estaciones del año y obtener un pormedio para cada estación? ¿Qué sucede en años Niño, el cual sabemos que afecta la temperatura del océano? En definitiva, debemos de definir sobre que conjunto de datos vamos a promediar, y dichos promedio va a reflejar efectivamente esa elección.<br>
</p>
<p>Este ejemplo precisa de la distinción entre lo que consideramos nuestra <em>señal</em> (temperatura media) de los procesos que son <em>ruido</em> (Estaciones del año, los años Niño, ondas internas, etc.). De esta forma, al definir el promedio estamos haciendo explícita la separación entre <em>señal</em> y <em>ruido</em>. Finalmente, una vez definido sobre que promediar, existen en literatura una gran cantidad de herramientas estadísticas que podemos utilizar. Definir <em>señal</em> y <em>ruido</em>, y determinar sobre que conjunto de datos vamos a calcular el promedio, es una tarea difícil. Conocer como debemos muestrear el océano también debe hacerse cuidadosamente.<br>
</p>
<p>En oceanografía física se muestrea el océano de forma discontínua, es decir, se obtienen medidas puntuales en el espacio y en el tiempo. Como dijimos anteriormente, el océano contiene procesos de diferentes escalas espaciales y temporales, nolineales, y aleatorios. Es por ello que es sumamente importante saber escojer el intervalo de muestreo <span class="math inline">\(\Delta{t}\)</span> dependiendo del fenómeno que se quiere muestrear. Debemos de tener en mente que la frecuencia mas alta que podemos resolver es la frecuencia de Nyquist:</p>
<p><span class="math display">\[f_N=1/(2\Delta{t})\,.\]</span></p>
<p>Por ejemplo, si medimos a intervalos de <span class="math inline">\(\Delta{t}=5\,{ h}\)</span> podremos como máximo resolver procesos que ocurren con frecuencia <span class="math inline">\(f_N\le1/10\,{ cph}\)</span>. La frecuencia mas baja que podemos resolver va a depender de la longitud del registro. A esa frecuencia le llamamos frecuencia fundamental</p>
<p><span class="math display">\[f_0=1/(\Delta{t}N)\,,\]</span></p>
<p>{}donde <span class="math inline">\(T=\Delta{t} N\)</span> es la duración del muestreo y N es el número de muestras o datos. En general, debemos de medir suficiente tiempo para registrar varios ciclos del fenómeno de estudio para tener significancia estadística. Por lo tanto, nuestra resolución frecuencial va a depender del intervalo y duración del muestreo. El cociente <span class="math inline">\(f_N/f_0=(1/2\Delta{t})/(1/N\Delta{t})=N/2\)</span> indica el número máximo de componentes de Fourier que podemos estimar. Una señal periódica se puede descomponer en la suma de un conjunto (infinito) de funciones oscilatorias de senos y cosenos o componentes de Fourier. Esto lo veremos en el capítulo~7. A cada muestreo de un fenómeno le denominamos realización, y a un conjunto de realizaciones se les denomina ensamble.</p>
<section id="estadística-básica" class="level3">
<h3 class="anchored" data-anchor-id="estadística-básica">Estadística básica</h3>
<p>La estadística trata de describir las características de una población continua a partir de muestras discretas de la misma. Hablamos de población y de muestra de una población. Si calculamos, por ejemplo, la media de una población, estamos calculando un {parámetro}. Cuando calculamos la media de una muestra le llamamos un estadístico de la población.<br>
</p>
<p>La estadística nos ayuda a organizar, analizar, presentar datos, y nos da información de cómo planear la recolección de los mismos, i.e.&nbsp;a muestrear.</p>
<ol type="1">
<li>La media:<br>
</li>
</ol>
<p>La media de una muestra de N valores <span class="math inline">\(x_i=x_1,x_2,...,x_N\)</span> es</p>
<span class="math display">\[\begin{equation}
\bar{x}=\frac{1}{N}\sum^N_{i=1}x_i=&lt;x&gt;\,.
\end{equation}\]</span>
<p>La media debe de diferenciarse de la mediana. La media es el momento de orden cero. La mediana de una población es aquel valor numérico que separa el 50% de valores mas altos del 50% de valores mas bajos. Se puede calcular ordenando de menor a mayor el conjunto de valores y escojer el valor central si el conjunto de datos es impar o el promedio de los dos centrales si es par.<br>
</p>
<ol start="2" type="1">
<li>La varianza:<br>
</li>
</ol>
<p>La varianza de un una muestra de N valores <span class="math inline">\(x_i\)</span> es</p>
<span class="math display">\[\begin{equation}
s^2=\frac{1}{N-1}\sum^N_{i=1}(x_i-\bar{x})^2=&lt;x'^2&gt;\,,
\end{equation}\]</span>
<p>donde las primas indican fluctuaciones alrededor de la media. La varianza es una medida de cuán lejos estan los diferentes puntos de la muestra de la media de la población. La varianza es el segundo momento alrededor de la media. Al dividir por <span class="math inline">\(N\)</span> estamos subestimando la verdaderavarianza de la población. Al dividir por <span class="math inline">\(N-1\)</span> obtenemos un estimador insesgado.<br>
</p>
<p>NOTA: el sesgo de un estimador se refiere a la diferencia entre su esperanza matemática y el valor numérico (real) del parámetro que se estima. Un estimador que no tiene sesgo se dice insesgado. Por ejemplo, para la media:</p>
<p><span class="math display">\[E[x]-\mu \rightarrow {0}\]</span></p>
<p><span class="math display">\[\bar{x}-\mu \rightarrow {0}\]</span></p>
<p>EJERCICIO: Demostrar porqué hay que dividir por <span class="math inline">\(N-1\)</span> en lugar de <span class="math inline">\(N\)</span> para que la definición de varianza sea un estimador insesgado.<br>
</p>
<ol start="3" type="1">
<li>La desviación típica:<br>
</li>
</ol>
<p>Es la raíz cuadrada de la varianza. Se suele escribir como <span class="math inline">\(\sigma\)</span> para referirse a la población o como <span class="math inline">\(s\)</span> en estadística</p>
<span class="math display">\[\begin{equation}
s=\sqrt{s^2}\,.
\end{equation}\]</span>
<ol start="4" type="1">
<li>Momentos de orden superior:<br>
</li>
</ol>
<p>Podemos definir un momento alrededor de la media como:</p>
<span class="math display">\[\begin{equation}
m_p=\frac{1}{N}\sum^N_{i=1}(x_i-\bar{x})^p=&lt;x'^p&gt;\,.
\end{equation}\]</span>
<p>De esta forma <span class="math inline">\(m_2\)</span> es la varianza, <span class="math inline">\(m_3\)</span> es la asimetría, y <span class="math inline">\(m_4\)</span> la curtosis. El momento <span class="math inline">\(m_3\)</span> indica la asimetría de la muestra alrededor de la media (<span class="math inline">\(m_3&gt;0\)</span> implica distribución con cola larga en la parte positiva y viceversa). <span class="math inline">\(m_4\)</span> indica el grado de esparcimiento de las muestras alrededor de la media. Una mayor curtosis indica mayor concentración de puntos alrededor de la media. Los momentos de orden superior (<span class="math inline">\(&gt;2\)</span>) se suelen adimensionalizar dividiendo por la desviación estándar:</p>
<span class="math display">\[\begin{equation}
    m_3=\frac{1}{N}\sum^N_{i=1}\left[\frac{x_i-\bar{x}}{\sigma}\right]^3=&lt;(x/\sigma)'^3&gt;
\end{equation}\]</span>
<span class="math display">\[\begin{equation}
    m_4=\frac{1}{N}\sum^N_{i=1}\left[\frac{x_i-\bar{x}}{\sigma}\right]^4-3=&lt;(x/\sigma)'^4&gt;-3
\end{equation}\]</span>
<p>donde el factor <span class="math inline">\(-3\)</span> hace que la curtosis tome el valor cero para una distribución Normal.</p>
<ol start="5" type="1">
<li>Covarianza y correlación:<br>
</li>
</ol>
<p>La covarianza entre dos variables <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span> puede definirse como un estadístico que relaciona <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span> de la siguiente forma</p>
<p><span class="math display">\[C_{xy}=&lt;x'y'&gt;=&lt;(x-\bar{x})(y-\bar{y})&gt;=\frac{1}{N-1}\sum\limits^N_{i=1} (x_i-\bar{x})(y_i-\bar{y})\,.\]</span></p>
<p>La correlación es la covarianza normalizada</p>
<p><span class="math display">\[\rho_{x y}=\frac{C_{x y}}{s_x s_y}=\frac{&lt;x' y'&gt;}{\sqrt{&lt;x'^2&gt;&lt;y'^2&gt;}}\,. \]</span></p>
<p>Consideremos el modelo estadístico lineal de media cero (es una recta que pasa por <span class="math inline">\((\overline{x},\overline{y})=(0,0)\)</span>)</p>
<p><span class="math display">\[\hat{y}=\alpha x\,,\]</span></p>
<p>donde <span class="math inline">\(\alpha\)</span> es una constante. El error cometido por este estimador se define como el error cuadratico medio</p>
<p><span class="math display">\[\epsilon=&lt;(\hat{y}-y)^2&gt;=\alpha^2&lt;x^2&gt;+&lt;y^2&gt;-2\alpha&lt;xy&gt;\]</span></p>
<p>y si queremos minimizar dicho error entonces tenemos que encontrar que <span class="math inline">\(\alpha\)</span> es el que provoca que la derivada <span class="math inline">\(\partial{\epsilon}/\partial{\alpha}\rightarrow{0}\)</span>. Es decir</p>
<p><span class="math display">\[\partial{\epsilon}/\partial{\alpha}=2\alpha&lt;x^2&gt;-2&lt;xy&gt;=0\,,\]</span></p>
<p>y el <span class="math inline">\(\alpha\)</span> es</p>
<p><span class="math display">\[\alpha=\frac{&lt;xy&gt;}{&lt;x^2&gt;}\,.\]</span></p>
<p>El error cuadrático mínimo se encuentra substituyendo el valor de <span class="math inline">\(\alpha\)</span> en la expresión del error <span class="math inline">\(\epsilon\)</span> de arriba</p>
<p><span class="math display">\[\epsilon=\frac{&lt;xy&gt;^2}{&lt;x^2&gt;} + &lt;y^2&gt; - 2\frac{&lt;xy&gt;^2}{&lt;x^2&gt;}=
           &lt;y^2&gt;\left(\frac{&lt;xy&gt;^2}{&lt;x^2&gt;&lt;y^2&gt;}+1-2\frac{&lt;xy&gt;^2}{&lt;x^2&gt;&lt;y^2&gt;}\right)=\]</span></p>
<p><span class="math display">\[=&lt;y^2&gt;(1-\rho^2_{xy})\,.\]</span></p>
<p>Si <span class="math inline">\(\rho^2_{xy}=1\)</span> entonces el error es cero, es decir, mínimo error. Opuestamente, si <span class="math inline">\(\rho^2_{xy}=0\)</span> entonces el error es igual a la varianza, es decir, máximo error. Si <span class="math inline">\(\rho\)</span> toma valores intermedios, i.e., <span class="math inline">\(\rho^2_{xy}=0.5\)</span>, entonces el error es <span class="math inline">\(\epsilon=0.5&lt;y^2&gt;\)</span>, es decir, el error del modelo lineal es un <span class="math inline">\(50\%\)</span> de la varianza. Por lo tanto, la correlación al cuadrado puede definirse también ciomo la eficiencia relativa del estimador <span class="math inline">\(\hat{y}^2\)</span> o la fracción de varianza explicada por el modelo lineal</p>
<p><span class="math display">\[\rho^2_{xy}=\frac{&lt;\hat{y}^2&gt;}{&lt;y^2&gt;}=\frac{{varianza\,\,\,explicada}}{{ varianza\,\,\,total}}\,.\]</span></p>
<p>A este parámetro se le puede encontrar en literatura inglesa como <em>skill</em> del modelo lineal.</p>
</section>
</section>
<section id="probabilidad" class="level2">
<h2 class="anchored" data-anchor-id="probabilidad">Probabilidad</h2>
<section id="distribuciones-de-probabilidad" class="level3">
<h3 class="anchored" data-anchor-id="distribuciones-de-probabilidad">Distribuciones de probabilidad:</h3>
<p>La {función de distribución acumulativa} <span class="math inline">\(D_x(r)\)</span> se define como la probabilidad que una variable aleatoria <span class="math inline">\(x\)</span> sea menor o igual a <span class="math inline">\(r\)</span>, es decir, <span class="math inline">\(P(x\le r)\)</span>. Matemáticamente:</p>
<p><span class="math display">\[D(x)=\int^r_{-\infty}F(x)dx\,,\]</span></p>
<p>donde</p>
<p><span class="math display">\[F(x)=\frac{d}{dx} D(x)\]</span></p>
<p>es la función de densidad de probabilidad (PDF, por su siglas en inglés). La PDF nos informa de la probabilidad que <span class="math inline">\(x\)</span> sea igual a un cierto valor <span class="math inline">\(r\)</span>, <span class="math inline">\(P(x=r)\)</span>.<br>
</p>
<p>{} Algunas propiedades de <span class="math inline">\(D(x)\)</span>:</p>
<p><br>
* <span class="math inline">\(D(r)\le D(s)\,\,\,{ if}\,\,\,r\le s\)</span><br>
* <span class="math inline">\(D(-\infty)=0\)</span><br>
* <span class="math inline">\(D(\infty)=1\)</span><br>
</p>
<p>Algunas propiedades de <span class="math inline">\(F(x)\)</span>:<br>
</p>
<ol type="1">
<li><span class="math inline">\(F(x)\ge0\)</span><br>
</li>
<li><span class="math inline">\(\int^{\infty}_{-\infty} F(x) dx=1\)</span><br>
</li>
</ol>
<p>La probabilidad que una variable aleatoria <span class="math inline">\(x\)</span> este contenida en el intervalo <span class="math inline">\([r,r+dr]\)</span> es la integral de la función de densidad de probabilidad <span class="math display">\[P(r\le x\le r+dr)=\int^{r+dr}_r F(x)dx\,.\]</span></p>
<p>Ambas definiciones son parecidas aunque no son lo mismo. Para ello veamos el ejemplo de la suma del lanzamiento de dos dados al aire.<br>
</p>
<p>La densidad de probabilidad de que la suma de los dos dados sea 7 es máxima y que sea 2 o 12 es mínima. Este ejemplo describe dos propiedades fundamentales de funciones de probabilidad discretas: (i) <span class="math inline">\(P(X=x) \ge 0\)</span> y (ii)<span class="math inline">\(\sum{P(x)}=1\)</span>. La distribución de probabilidad acumulativa y la función de densidad de probabilidad tienen las siguientes distribuciones<br>
</p>
<p>{} Momentos estadísticos de una función de densidad de probabilidad:<br>
</p>
<p>Los momentos centrados (o alrededor de la media) de una distribución de probabilidad se definen como</p>
<p><span class="math display">\[m_r=E[(x-E[x])^r]=\int^{\infty}_{-\infty} (x-\mu)^rF(x)dx\,.\]</span></p>
<p>Como caso particular, los momentos alrededor del origen (i.e., <span class="math inline">\(\mu=0\)</span>) son: <span class="math display">\[m^0_r=E[x^r]=\int^{\infty}_{-\infty} x^rF(x)dx\,.\]</span></p>
<p>Entonces, los primeros tres momentos centrados se definen como</p>
<p><span class="math display">\[m_0=E[(x-E[x])^0]=E[1]=\int^{\infty}_{-\infty}F(x)dx=1\,,\]</span> <span class="math display">\[m_1=E[(x-E[x])^1]=E[x]-\mu=\int^{\infty}_{-\infty} (x-\mu)^1 F(x)dx=0\,,\]</span> <span class="math display">\[m_2=E[(x-E[x])^2]=E[x^2 + E[x]^2 -2xE[x]]=
      E[x^2]+E[x]^2-2E[x]E[x]=\]</span> <span class="math display">\[E[x^2]-E[x]^2=\underbrace{E[x^2]}_{\sigma^2}-\mu^2=\int^{\infty}_{-\infty} (x-\mu)^2 F(x)dx=\sigma^2-\mu^2\,.\]</span></p>
<p>Los momentos alrededor de cero (<span class="math inline">\(\mu=0\)</span>) tambien pueden ser estandarizados: <span class="math display">\[m^*_r=m_r/\sigma^r=\frac{E[(x-E[x])^r]}{(\underbrace{E[(x-E[x])^2]}_{\sigma^2})^{r/2}}\,.\]</span></p>
<p>Los cuatro primeros momentos estadísticos alrededor de cero estandarizados son: <span class="math display">\[m^*_1=m_1/\sigma^1=\frac{E[(x-\mu)^1]}{(E[(x-\mu)^2])^{1/2}}=\frac{\mu-\mu}{\sqrt{E[(x-\mu)^2]}}=0\,,\]</span> <span class="math display">\[m^*_2=m_2/\sigma^2=\frac{E[(x-\mu)^2]}{(E[(x-\mu)^2])^{2/2}}=1\,,\]</span> <span class="math display">\[m^*_3=m_3/\sigma^3=\frac{E[(x-\mu)^3]}{(E[(x-\mu)^2])^{3/2}}\,,\]</span> <span class="math display">\[m^*_4=m_4/\sigma^4=\frac{E[(x-\mu)^4]}{(E[(x-\mu)^2])^{4/2}}\,.\]</span></p>
<ol type="1">
<li>Distribución uniforme:<br>
La distribución de probabilidad uniforme viene dada por <span class="math display">\[F(x)=\frac{1}{b-a}\,\,\,, a \le x \le b\]</span> <span class="math display">\[=0,\,\,\,\,fuera\,\,del\,\,intervalo\]</span></li>
</ol>
<p>Se deduce de la expresión de área de un cuadrado:</p>
<p><span class="math display">\[Area=base*altura=(b-a)F(x)=1\]</span></p>
<p>La función de distribución acumulativa es <span class="math display">\[D(x)=0,\,\,\,x&lt;a\]</span> <span class="math display">\[D(x)=\frac{x-a}{b-a},\,\,\,a \le x \le b\]</span> <span class="math display">\[D(x)=1,\,\,\,x \ge b\]</span><br>
</p>
<p>La media es <span class="math inline">\(\mu=(b+a)/2\)</span> y la varianza es <span class="math inline">\(\sigma^2=1/3(a^2 + b^2 +ab)\)</span>. Demostración:<br>
</p>
<p>{} Los momentos estadísticos alrededor del origen de la distribución uniforme son <span class="math display">\[m^0_r=E[x^r]=\int^{\infty}_{-\infty} x^rF(x)dx=\int^{b}_{a} \frac{x^r}{b-a}dx=\]</span></p>
<p><span class="math display">\[\frac{1}{b-a}\int^{b}_{a}x^r dx=\frac{1}{b-a}\left[\frac{x^{r+1}}{r+1}\right]^b_a=
      \frac{1}{b-a}\left[\frac{b^{r+1}}{r+1}-\frac{a^{r+1}}{r+1}\right]=\frac{b^{r+1}-a^{r+1}}{(b-a)(r+1)}\]</span></p>
<p>y por lo tanto la media es</p>
<p><span class="math display">\[m^0_1=E(x)=\frac{b^2-a^2}{2(b-a)}=\frac{(b-a)(b+a)}{2(b-a)}=\frac{b+a}{2}\,,\]</span></p>
<p>y la varianza</p>
<p><span class="math display">\[m^0_2=E(x^2)=\frac{b^3-a^3}{3(b-a)}=\frac{(b-a)(a^2+b^2+ab)}{3(b-a)}=\frac{1}{3}(a^2 + b^2 +ab)\,.\]</span></p>
<p>Ejemplo de distribución uniforme: La ruleta rusa. Supongamos que puede tomar 360 valores, es decir, <span class="math inline">\(0 \le x \le 360\)</span>. Entonces</p>
<p><span class="math display">\[F(x)=\frac{1}{360},\,\,\,0 \le x \le 360\,,\]</span></p>
<p>y, por ejemplo, la probabilidad de que la bola caiga entre el 50 y el 360 es</p>
<p><span class="math display">\[P(50\le x \le 360)=\int^{360}_{50}\frac{1}{360}dx=\frac{1}{360}\left[x\right]^{360}_{50}=\frac{310}{360}=0.8611\,(\sim86\%).\]</span></p>
<p>La función de distribución acumulativa es</p>
<p><span class="math display">\[D(x)=0,\,\,\,x&lt;0\]</span> <span class="math display">\[D(x)=\frac{x}{360},\,\,\,0 \le x \le 360\]</span> <span class="math display">\[D(x)=1,\,\,\,x \ge 360\]</span></p>
<ol start="2" type="1">
<li>Distribución normal o Gaussiana:<br>
</li>
</ol>
<p>La distribución normal es una de las distribuciones mas recurrente en la naturaleza. En general cualquier variable aleatoria medida, especialmente aquellas que son suma de otras variables aleatorias, tiene una distribución normal alrededor de la media</p>
<p><span class="math display">\[F(x)=\frac{1}{\sigma\sqrt{2\pi}}{exp}
   \left\{ -\frac{(x-\bar{x})^2}{2\sigma^2} \right\}\,.\]</span></p>
<p>La distribución de función acumulativa normal se obtiene integrando la expresión de arriba. Para ello vamos a realizar el cambio de variable (que no es nada mas que estandarizar la variable aleatoria x)</p>
<p><span class="math display">\[z=\frac{x-\bar{x}}{\sigma\sqrt{2}}\]</span> y <span class="math display">\[dz=\frac{dx}{\sigma\sqrt{2}}\,,\]</span></p>
<p>de lo que se deduce</p>
<p><span class="math display">\[D(z)=\frac{\sigma\sqrt{2}}{\sigma\sqrt{2\pi}}\int^z_{-\infty} {exp}
  \left\{ -z^2 \right\}dz =
  \frac{1}{\sqrt{\pi}}\int^z_{-\infty} {exp}
  \left\{ -z^2 \right\} dz\,,\]</span></p>
<p>donde <span class="math inline">\(\frac{2}{\sqrt{\pi}}\int^z_{0}{exp}\left\{ -t^2 \right\}dt={erf}(z)\,.\)</span></p>
<p>Los momentos estadísticos alrededor del origen de la función de distribución Normal son</p>
<p><span class="math display">\[m^0_r=E[x^r]=\int^{\infty}_{-\infty} x^rF(x)dx=\frac{1}{\sigma\sqrt{2\pi}}\int^{\infty}_{-\infty}x^r{exp}
   \left\{ -\frac{(x-\bar{x})^2}{2\sigma^2} \right\} dx\]</span></p>
<p>Hagamos el cambio de variable</p>
<p><span class="math display">\[u=\frac{x-\bar{x}}{\sigma\sqrt{2}}\]</span> <span class="math display">\[du=\frac{dx}{\sigma\sqrt{2}}\]</span></p>
<p>Si substituimos en la expresión de <span class="math inline">\(m_r\)</span> obtenemos</p>
<p><span class="math display">\[m^0_r=\frac{\sigma\sqrt{2}}{\sigma\sqrt{2\pi}}\int^{\infty}_{-\infty}
  \left( \sigma \sqrt{2}u+\bar{x}\right)^r{e}^{-u^2}du=
  \frac{1}{\sqrt{\pi}}\int^{\infty}_{-\infty}\left( \sigma \sqrt{2}u+\bar{x}\right)^r{e}
   ^{-u^2}du\,.\]</span></p>
<p><strong>Ejercicio</strong>: Deducir los momentos estadísticos de orden 1 y 2 de la distribución Normal, es decir, la media y la varianza. Integrales útiles:</p>
<p><span class="math display">\[\int e^{-ax^2}dx=\frac{\sqrt{\pi}}{2\sqrt{a}}{erf}(x\sqrt{a})\]</span> <span class="math display">\[\int xe^{-ax^2}dx=-\frac{1}{2a}e^{-ax^2}\,,\]</span></p>
<p>donde la función de error se define cómo:</p>
<p><span class="math display">\[erf(z)=\frac{2}{\sqrt{\pi}}\int_0^z e^{-t^2} dt\,.\]</span></p>
<p>La función de error cumple las siguientes identidades: <span class="math display">\[{erf}(0)=\frac{2}{\sqrt{\pi}}\int_0^0 e^{-t^2} dt=0\]</span>, <span class="math display">\[{erf}(\infty)=\frac{2}{\sqrt{\pi}}\underbrace{\int_0^\infty e^{-t^2} dt}_{\frac{\sqrt{\pi}}{2}}=1\]</span>. <span class="math display">\[{erf}(-\infty)=\frac{2}{\sqrt{\pi}}\underbrace{\int_0^{-\infty} e^{-t^2} dt}_{-\frac{\sqrt{\pi}}{2}}=-1\]</span>.</p>
<p><strong>Respuesta</strong>: La media y la varianza son</p>
<p><span class="math display">\[m^0_1=E(x)=\mu\,\,\,\,\,\,;\,\,\,\,\,\,m_1=0\]</span> <span class="math display">\[m^0_2=Var(x)=\mu^2 + \sigma^2\,\,\,\,\,\,;\,\,\,\,\,\,m_2=\sigma^2\]</span></p>
<p>Deducción de la media: El momento de orden 1 centrado es:</p>
<p><span class="math display">\[m_1=\frac{\sigma\sqrt{2}}{\sqrt{\pi}}\int^{\infty}_{-\infty}u{e}
   ^{-u^2}du=-\frac{\sigma\sqrt{2}}{2\sqrt{\pi}}{e}
   ^{-u^2}\Big|^{\infty}_{-\infty}=0\,.\]</span><br>
y alrededor de cero (momentos crudos):</p>
<p><span class="math display">\[m^0_1=\frac{1}{\sqrt{\pi}}\int^{\infty}_{-\infty}(\sigma \sqrt{2} u + \mu){e}
   ^{-u^2}du=\frac{\sigma\sqrt{2}}{\sqrt{\pi}}\int^{\infty}_{-\infty}u{e}
   ^{-u^2}du + \frac{1}{\sqrt{\pi}}\int^{\infty}_{-\infty}\mu{ e}
   ^{-u^2}du=\]</span><br>
<span class="math display">\[=\underbrace{\frac{\sigma\sqrt{2}}{\sqrt{\pi}}\left( -\frac{1}{2}{ e}
   ^{-u^2}\right)}_{0} \Big|^{\infty}_{-\infty}+
   \frac{\mu}{\sqrt{\pi}}\left( \frac{\sqrt{\pi}}{2}{ erf}(u)\right) \Big|^{\infty}_{-\infty}=
   \frac{\mu}{2}[1-(-1)]=\mu\,,\]</span></p>
<p><strong>Deducción de la varianza</strong>:</p>
<p>El momento de orden 2 centrado es:<br>
</p>
<p><span class="math display">\[m_2=\frac{(\sigma\sqrt{2})^2}{\sqrt{\pi}}\int^{\infty}_{-\infty}u^2{ e}^{-u^2}du\]</span></p>
<p><span class="math display">\[x=u \rightarrow dx=du\]</span> <span class="math display">\[dy=u{ e}^{-u^2} \rightarrow y=\frac{1}{2}{ e}^{-u^2}\]</span></p>
<p><span class="math display">\[m_2=\frac{2\sigma^2}{\sqrt{\pi}}\left[ -\frac{1}{2}{ e}^{-u^2}
      \Big|^{\infty}_{-\infty} +\frac{\sqrt{\pi}}{4} { erf}(u)\Big|^{\infty}_{-\infty}\right]=
  \frac{2\sigma^2}{\sqrt{\pi}}\left[0 + \frac{\pi}{4}\left({ erf}(\infty) - { erf}(-\infty) \right)
  \right]=\]</span> <span class="math display">\[=\frac{2\sigma^2}{\sqrt{\pi}}\left[ \frac{\pi}{4}+\frac{\pi}{4}\right]=\sigma^2\]</span><br>
</p>
<p>y alrededor de cero (momentos crudos):</p>
<p><span class="math display">\[m^0_2=\frac{1}{\sqrt{\pi}}\int^{\infty}_{-\infty}(\sigma \sqrt{2} u + \mu)^2{ e}^{-u^2}du=
   \frac{1}{\sqrt{\pi}}\int^{\infty}_{-\infty}2\sigma^2 u^2{ e}^{-u^2}du +\]</span> <span class="math display">\[ +\frac{\mu^2}{\sqrt{\pi}}\int^{\infty}_{-\infty}{ e}^{-u^2}du
   +\frac{2\sigma\sqrt{2}\mu}{\sqrt{\pi}}\int^{\infty}_{-\infty}u{ e}^{-u^2}du=\]</span></p>
<p><span class="math display">\[
   =\underbrace{\frac{2\sigma^2}{\sqrt{\pi}}\int^{\infty}_{-\infty}u^2{ e}^{-u^2}du}_{m_2=\sigma^2}
   +\mu \underbrace{\frac{1}{\sqrt{\pi}}\int^{\infty}_{-\infty}\mu{ e}^{-u^2}du}_{m_1^0=\mu}+
   +2\mu \underbrace{\frac{\sigma\sqrt{2}}{\sqrt{\pi}}\int^{\infty}_{-\infty}u{ e}^{-u^2}du}_{m_1=0}= \sigma^2+\mu^2\,,\]</span><br>
</p>
<p>La probabilidad de que una variable normalmentedistribuida caiga en una desviación estándar de su valor medio viene dada por <span class="math display">\[P(-1\le z \le 1)=\int^{+1}_{-1} F(z) dz=\frac{1}{\sqrt{2\pi}}\int^{+1}_{-1}e^{-\frac{1}{2}z^2}dz=\]</span> <span class="math display">\[=\frac{1}{\sqrt{2\pi}}\frac{\sqrt{\pi}}{2\sqrt{1/2}}\left[{ erf}({z\sqrt{1/2}})\right]^1_{-1}=
   \frac{1}{2}\left[{ erf}({1/\sqrt{2}})-{ erf}({-1/\sqrt{2}}) \right]=\]</span> <span class="math display">\[=\frac{1}{2}\left[0.6827-(-0.6827)\right]=0.6827\,(~68.27\%)\,,\]</span><br>
</p>
<p>y similarmente para 2 y 3 desviaciones estándar<br>
</p>
<p><span class="math display">\[P(-2\le z \le 2)=\int^{+2}_{-2} F_x(z) dz=95.45\%\]</span> <span class="math display">\[P(-3\le z \le 3)=\int^{+3}_{-3} F_x(z) dz=99.73\%\,.\]</span></p>
<p>Entonces solo hay un <span class="math inline">\(4.55\%\)</span> de probabilidad de que una variable normalmente distribuida caiga fuera de dos desviaciones estándar respecto de la media. Puesto que es una probabilidad con 2 colas, la probabilidad de que una variable normal exceda su media por mas de <span class="math inline">\(2\sigma\)</span> es la mitad de esto, es decir <span class="math inline">\(2.275\%\)</span>, ya que la distribución normal es simétrica.<br>
En la práctica una PDF se calcula como un histograma escalado. Es por ello que necesitamos escojer el tamaño y localización de los bins en el histograma. La demo muestra las consecuencias de esta elección (pdf_demo.m).</p>
<ol start="3" type="1">
<li>Distribución de Poisson:<br>
</li>
</ol>
<p>La distribución de Poisson expresa la probabilidad de que ocurra un determinado número de eventos durante un cierto intervalo en el tiempo o distancia en el espacio. Se usa generalmente para la ocurrencia de sucesos con muy poca probabilidad o muy ``raros’’. La expresión para la función acumulativa es:</p>
<p><span class="math display">\[D(x)=P(x \le r)=e^{-\lambda}\sum^{|r|}_{k=0}\frac{\lambda^k}{k!}\,,\]</span></p>
<p>donde <span class="math inline">\(\lambda\)</span> es el valor promedio</p>
<p>y la función de densidad de probabilidad</p>
<p><span class="math display">\[F(x)=P(x=k)=\frac{{\lambda}^k e^{-\lambda}}{k!}\]</span></p>
<p>Los momentos estadísticos alrededor del origen de la función de distribución de Poisson se pueden calcular directamente con sumatorios y expansion de Taylor:</p>
<p><span class="math display">\[m^0_r=E[x^r]=\sum k^r P(X=k)=\sum_{k\ge0} k^rF(x)= \sum_{k\ge0} k^r\frac{{\lambda}^k e^{-\lambda}}{k!}=\]</span> <span class="math display">\[=\lambda e^{-\lambda}\sum_{k \ge 0}\frac{k^r}{k!}\lambda^{k-1}\]</span></p>
<p>{}Veamos el momento de orden 1 alrededor del orígen:</p>
<p><span class="math display">\[m^0_1=E[x]=\lambda e^{-\lambda}\sum_{k \ge 0}\frac{k}{k!}\lambda^{k-1}=
\lambda e^{-\lambda}\sum_{k \ge 1}\frac{k}{(k-1)!k}\lambda^{k-1}=
\lambda e^{-\lambda}\sum_{k \ge 1}\frac{1}{(k-1)!}\lambda^{k-1}\]</span> <span class="math display">\[=\lambda e^{-\lambda}\sum_{j \ge 0}\frac{\lambda^{j}}{j!}\,,\]</span> para <span class="math inline">\(j=k-1\)</span>. Finalmente expandiendo en series de Taylor la función exponencial</p>
<p><span class="math display">\[e^{\lambda}=\sum_{j \ge 0}\frac{1}{j!}\lambda^{j}\]</span>, obtenemos:</p>
<p><span class="math display">\[m^0_1=\lambda e^{-\lambda}e^{\lambda}=\lambda\]</span></p>
<p>Ejercicio: Demostrar que el momento estadístico alrededor del origen de orden 2 de la distribución de Poisson es igual a <span class="math inline">\(m^0_2=\lambda+\lambda^2\)</span>.</p>
<p><span class="math display">\[m^0_2=E[x^2]=\lambda e^{-\lambda}\sum_{k \ge 0}\frac{k^2}{k!}\lambda^{k-1}=
               \lambda e^{-\lambda}\left[ \sum_{k \ge 1}(k-1)\frac{1}{(k-1)!}\lambda^{k-1} +
                                      \sum_{k \ge 1} \frac{1} {(k-1)!} \lambda^{k-1} \right]=\]</span></p>
<p><span class="math display">\[= \lambda e^{-\lambda}\left[ \lambda \sum_{k \ge 2}\frac{1}{(k-2)!}\lambda^{k-2} + \sum_{k \ge 1}\frac{1}{(k-1)!}\lambda^{k-1}\right]=\]</span> <span class="math display">\[= \lambda e^{-\lambda}\left[ \lambda \sum_{j \ge 0}\frac{1}{j!}\lambda^{j} + \sum_{i \ge 0}\frac{1}{(i)!}\lambda^{i}\right]=
\lambda e^{-\lambda} \left[\lambda e^{\lambda} + e^{\lambda}\right]=\]</span></p>
<p><span class="math display">\[=\lambda (\lambda +1)=\lambda^2 + \lambda\,.\]</span></p>
<p>En el casso que fuera el momento de orden 2 centrado se escribiría:</p>
<p><span class="math display">\[E[(x-E[x])^2]=E[x^2]-(E[x])^2=\lambda^2 + \lambda - (\lambda)^2=\lambda\,.\]</span></p>
<p><strong>Ejemplo</strong>: En los últimos 160 años, han sucedido 680 tormentas intensas en el Golfo de México, incluyendo depresiones, tormentas tropicales, y huracanes. Asumimos que la frecuencia de ocurrencia de una tormenta intensa en el Golfo de México sigue una distribución de Poisson (eventos “raros”, poco frecuentes). Calcula la probabilidad de que ocurran 2 huracanes en 1 año:<br>
</p>
<ol type="a">
<li><p>El número promedio de tormentas intensas por año es: <span class="math inline">\(\mu=680/160=4.25\)</span> huracanes/año.</p></li>
<li><p>La probabilidad de que ocurran 2 huracanes en 1 año es:</p></li>
</ol>
<p><span class="math display">\[P(x=2)=\frac{{\lambda}^2 e^{-\lambda}}{2!}=\frac{{4.25}^2 e^{-4.25}}{2!}=0.1288\,(\sim12\%)\]</span></p>
<p>La probabilidad es muy baja debido a que exigimos que sean exactamente 2 huracanes en un año y no, por ejemplo, <span class="math inline">\(&gt;2\)</span>. En el segundo caso, la probabilidad aumentaría considerablemente</p>
<p><span class="math display">\[P(x&gt;2)=P(x=3)+P(x=4)+....=1-P(x\le 2)=1-[P(x=0)+P(x=1)+P(x=2)]=\]</span> <span class="math display">\[=1-[0.0143+0.0606+0.1288]=1-0.2037=0.7963\,(\sim80\%)\]</span></p>
<ol start="4" type="1">
<li>Distribución Binomial:</li>
</ol>
<p>Supongamos que tenemos un conjunto de <span class="math inline">\(n\)</span> tiradas en los cuales pueden suceder únicamente dos cosas: <code>acierto' o</code>fallo’. La probabilidad de acertar en una tirada es p=P. Si <span class="math inline">\(X\)</span> es el número total de aciertos en <span class="math inline">\(n\)</span> tiradas, entonces la probabilidad de que el número de aciertos sea <span class="math inline">\(k\)</span> es:</p>
<p><span class="math display">\[P(X=k)=\left( \begin{array}{c}
n \\ k
       \end{array} \right)
p^k (1-p)^{n-k}, \, k=0,1,2,3,....,n\,,\]</span></p>
<p>}donde la expresión</p>
<p><span class="math display">\[\left( \begin{array}{c}
n \\ k
       \end{array} \right)=C(n,k)\equiv\frac{n!}{(n-k)!k!}\,,\]</span><br>
</p>
<p>es el número de diferentes combinaciones de grupos de k objetos que pueden ser elegidos de un conjunto total de n objetos. Estos números se denominan coeficientes binomiales. La probabilidad de que el número de aciertos caiga en un rango de valores es</p>
<p><span class="math display">\[P(a\le X\le b)=\sum^b_a P(X)\]</span></p>
<p>Ejemplo 1: ¿Cual es la probabilidad de obtener exactamente 6 caras de 10 lanzamientos de moneda? Respuesta:</p>
<p><span class="math display">\[P(x=6) = C(10,6)0.5^6(1-0.5)^{10-6} = \frac{10!}{(10-6)!6!}0.5^6(1-0.5)^{10-6} \simeq 0.205\]</span></p>
<p>{Ejemplo 2:} ¿Cual es la probabilidad de obtener mas de 15 caras de 20 lanzamientos de moneda? Respuesta:</p>
<p><span class="math display">\[\sum^{20}_{k=16} \left( \begin{array}{c} 20 \\ k
       \end{array} \right) 0.5^k(1-0.5)^{20-k}=0.006\,.\]</span></p>
<p>Si realizas esta operación a mano se vuelve muy tediosa. Es por ello que se utiliza la aproximación Normal a la distribución Binomial (DeMoivre-Laplace).</p>
<p>{} <strong>Teorema de DeMoivere-Laplace</strong> (aproximación de Binomial a Normal)<br>
</p>
<p>La distribución binomial de una variable X definida por n tiradas independientes cada una de las cuales tienen una probabilidad <span class="math inline">\(p\)</span> de acertar, es aproximadamente una distribución Normal de media <span class="math inline">\(np\)</span> y desviación típica <span class="math inline">\(\sqrt{np(1-p)}\)</span>, cuando n es suficientemente grande. Entonces se deduce que para cualquier número a y b,</p>
<p><span class="math display">\[lim_{n\rightarrow\infty} P \left( a&lt;\frac{X-np}{\sqrt{np(1-p)}}&lt;b\right)= \frac{1}{\sqrt{2\pi np(1-p)}}\int^b_a exp-\left[\frac{(x-np)^2}{2np(1-p)}\right]dx\,.\]</span><br>
</p>
<p>Esto significa que el estadístico, <span class="math inline">\(\frac{X-np}{\sqrt{np(1-p)}}\)</span> , tiene una distribución Normal. Este teorema es un caso particular del teorema del límite central y nos permite de simplificar la solución de un problema binomial.</p>
<p>{} <strong>Ejemplo de la aproximación Normal a la distribución Binomial</strong>:<br>
</p>
<p>El 2% de los XBTs fabricados por una empresa presentan defectos. Si hemos adquirido 2000 XBTs, ¿Cual es la probabilidad de que haya menos de 50 defectuosos?<br>
</p>
<p>Respuesta: Se trata de una distribución binomial ya que solo pueden ser defectuosos o no defectuosos. La probabilidad que sea defectuoso es <span class="math inline">\(p=0.02\)</span> (2%) y <span class="math inline">\(n=2000\)</span>, lo que nos da una distribución Binomial <span class="math inline">\(B(2000,0.02)\)</span>. Puesto que la <span class="math inline">\(n\)</span> es grande podemos hacer una aproximación a la distribución Normal. Calculamos la media y desviación estándar de la distribución Normal <span class="math inline">\(\mu=np=200*0.02=40\)</span> y <span class="math inline">\(\sigma=\sqrt{np(1-p)}=\sqrt{2000*0.02*(1-0.02)}=6.26\)</span> <span class="math inline">\(x\)</span> es <span class="math inline">\(B(2000,0.02)\)</span> y <span class="math inline">\(x_N\)</span> es <span class="math inline">\(N(40,6.26)\)</span>.<br>
La probabilidad que <span class="math inline">\(x&lt;50\)</span> es <span class="math display">\[p(x&lt;50)=p(x_N\le 49)\,,\]</span> y si estandarizamos <span class="math display">\[p(x_N\le 49)=p\left(z\le \frac{49-40}{6.26} \right)=p(z\le 1.44)=0.9251\,.\]</span></p>
<p>{} EJERCICIOS de estadística y probabilidad:<br>
</p>
<p>Ejercicio 1: Calcule E[x] si x tiene la función de densidad de probabilidad</p>
<p><span class="math display">\[ f(x)=\Bigg(\begin{array}{c}
\frac{1}{4}xe^{-\frac{x}{2}}\,\,\,\,\,,x&gt;0 \\ 0 \,\,\,\,\,,otherwise
\end{array})\,.\]</span></p>
<p>La esperanza E[x] de la función <span class="math inline">\(f(x)\)</span> es entonces</p>
<p><span class="math display">\[E[x]=\int^\infty_0 x\left(\frac{1}{4}xe^{-\frac{x}{2}}\right)dx
      =\frac{1}{4}\int^\infty_0 x^2e^{-\frac{x}{2}}dx\,.\]</span></p>
<p>Definamos <span class="math inline">\(y=x/2\)</span>; entonces <span class="math inline">\(x=2y\)</span> y <span class="math inline">\(dx=2dy\)</span> y obtenemos</p>
<p><span class="math display">\[E[x]=\frac{1}{4}\int^\infty_0 x^2e^{-\frac{x}{2}}dx
        =\frac{1}{4}\int^\infty_0 (2y)^2e^{-y}2dy
    =2\int^\infty_0y^2e^{-y}dy\,.\]</span> Vamos ahora a resolver la integral por partes. Hacemos la siguiente sustitución: <span class="math inline">\(u=y^2\)</span>, <span class="math inline">\(dv=e^{-y}\)</span> y por ende <span class="math inline">\(du=2ydy\)</span> y <span class="math inline">\(v=-e^{-y}\)</span>. La integral se puede reescribir usando la expresión general de integración por partes <span class="math inline">\(h(x)=uv-\int vdv\)</span>:</p>
<p><span class="math display">\[E[x]=2\int^\infty_0y^2e^{-y}dy=2\left[ -y^2e^{-y}-\int-e^{-y}(2y)dy\right]
      =2\left[ -y^2e^{-y}+2\int ye^{-y}dy\right]\,.\]</span></p>
<p>Integramos de nuevo por partes. Usa <span class="math inline">\(u=y\)</span>, <span class="math inline">\(dv=e^{-y}\)</span> y entonces <span class="math inline">\(du=dy\)</span> y <span class="math inline">\(v=-e^{-y}\)</span></p>
<p><span class="math display">\[E[x]=2\left[ -y^2 e^{-y} + 2 \Big\{ -y e^{-y} - \int -e^{-y}dy \Big\} \right]=\]</span> <span class="math display">\[=2\left[ -y^2 e^{-y} + 2 \Big\{ -y e^{-y} - e^{-y}         \Big\} \right]=\]</span></p>
<p><span class="math display">\[=-2y^2e^{-y}-4ye^{-y}-4e^{-y}=\]</span> <span class="math display">\[=\left[ -2e^{-y}(y^2+2y+2) \right]^{\infty}_0=\]</span> <span class="math display">\[=\lim_{n \to\infty}\left[ -2e^{-y}(y^2+2y+2) \right]-\left[ -2e^{-y}(y^2+2y+2) \right]_{y=0}\,.\]</span></p>
<p>El límite es ahora del tipo <span class="math inline">\(\infty/\infty\)</span> y entonces usamos la regla de l’Hopital <span class="math display">\[E[x]=-2\lim_{y \to\infty}\frac{2y+2}{e^y}+2\left[e^{-y}(y^2+2y+2) \right]_{y=0}\]</span> Usamos la regla de l’Hopital de nuevo</p>
<p><span class="math display">\[E[x]=-2\lim_{y \to\infty}\frac{2}{e^y}+2\left[e^{-y}(y^2+2y+2) \right]_{y=0}=0+2\left[e^{-y}(y^2+2y+2) \right]_{y=0}=4\]</span></p>
<p>{} Ejercicio 2: Calcule E[x] si x tiene la función de densidad de probabilidad</p>
<p><span class="math display">\[f(x)=\Bigg\{\begin{array}{c}
c(1-x^2)\,\,\,\,\,,-1&lt;x&lt;1 \\ 0 \,\,\,\,\,,otherwise
       \end{array} \,.\]</span></p>
<p><span class="math display">\[E[x]=\int^1_{-1} x[c(1-x^2)]dx=c \int^1_{-1} x[(1-x^2)]dx=\]</span> <span class="math display">\[=c \int^1_{-1} x-x^3dx=c\left[\frac{x^2}{2}+\frac{x^4}{4}\right]^{1}_{-1}=0\]</span></p>
<p>{} Ejercicio 3: Calcule E[x] si x tiene la función de densidad de probabilidad</p>
<p><span class="math display">\[f(x)=\Bigg\{\begin{array}{c}
\frac{5}{x^2}\,\,\,\,\,,x&gt;5 \\ 0 \,\,\,\,\,,x\le5
       \end{array} \,.\]</span></p>
<p><span class="math display">\[E[x]=\int^{\infty}_{5}x\frac{5}{x^2}dx=\int^{\infty}_{5}\frac{5}{x}dx=5\int^{\infty}_{5}\frac{1}{x}dx\]</span> <span class="math display">\[=5[lnx]^{\infty}_5=5\left[\Big(\lim_{x \to\infty} ln{x}\Big)-ln{5}\right]\rightarrow \infty\]</span></p>
<p> La variable aleatoria <span class="math inline">\(x\)</span> tiene la siguiente función de densidad de probabilidad</p>
<p><span class="math display">\[f(x)=\Bigg\{\begin{array}{c}
          k(2x+3)\,\,\,\,\,-1\le x \le 2 \\ 0 \,\,\,\,\,otherwise
         \end{array} \,.\]</span></p>
<p> Sea la función <span class="math inline">\(g(x)\)</span> dada por</p>
<p><span class="math display">\[g(x)=\Bigg\{\begin{array}{c}
x+2\alpha\,\,\,\,\,,x\le-\alpha \\
x \,\,\,\,\,,-\alpha \le x \le \alpha \\
x-2 \alpha \,\,\,\,\,,x&gt;\alpha
       \end{array} \,,\]</span></p>
<p>donde asumimos que x esta normalmente distribuida. Calcula la media de <span class="math inline">\(g(x)\)</span>.</p>
<p><span class="math display">\[E[g(x)]=\int^{\infty}_{-\infty} g(x) F(x) dx=\int^{-\alpha}_{-\infty} (x+2\alpha) F(x) dx +
\int^{\alpha}_{-\alpha} x F(x) dx + \int^{\infty}_{\alpha} (x-2\alpha) F(x) dx=\]</span> <span class="math display">\[=\int^{-\alpha}_{-\infty} x F(x) dx + \int^{-\alpha}_{-\infty} 2 \alpha F(x) dx +
   \int^{\alpha}_{-\alpha} x F(x) dx + \int^{\infty}_{\alpha} x F(x) - \int^{\infty}_{\alpha} 2\alpha F(x)dx=\]</span> <span class="math display">\[=\int^{-\infty}_{-\infty} x F(x) dx +  2\alpha \left[ \int^{-\alpha}_{-\infty} F(x) dx - \int^{\infty}_{\alpha}  F(x)\right]=\]</span> <span class="math display">\[=\int^{-\infty}_{-\infty} x F(x) dx +  2\alpha \left[ D(x=-\alpha) - \Bigg(1-\int^{-\alpha}_{-\infty} F(x) dx\Bigg)\right]=\]</span> <span class="math display">\[=\int^{-\infty}_{-\infty} x F(x) dx +  2\alpha \left[ D(x=-\alpha) - \Bigg(1-D(x=\alpha)\Bigg)\right]=\]</span> <span class="math display">\[=\mu + 2 \alpha \left[ D(-\alpha) - 1 + D(\alpha) \right]\,.\]</span></p>
<p>donde la media de la distribución Normal es</p>
<p><span class="math display">\[E[x]=\int^{-\infty}_{-\infty} x F(x) dx=\int^{-\infty}_{-\infty} x \frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{1}{2} \big( \frac{x-\mu}{\sigma} \big)^2 } dx=\mu\,.\]</span></p>
<p>{} Teorema del límite central:</p>
<p>Definición 1: Sea <span class="math inline">\(X_1,\,X_2,\,X_3,...,X_n\)</span> un conjunto de variables aleatorias, independientes e idénticamente distribuidas con media <span class="math inline">\(\mu\)</span> y varianza <span class="math inline">\(\sigma^2\)</span> distinta de cero. Sea <span class="math display">\[S_n=X_1+X_2+....+X_n\,,\]</span> entonces <span class="math display">\[\lim_{n \to\infty} Pr (Z_n\le z)= \Phi(z)\,,\]</span> donde <span class="math inline">\(\Phi(z)\)</span> es una distribución Normal estándar y <span class="math inline">\(Z_n=\frac{S_n - n\mu} {\sigma\sqrt{n}} = \frac{\bar{X} - \mu} {\sigma/\sqrt{n}}\)</span> es una estandarización del sumatorio <span class="math inline">\(S_n\)</span> de tal forma que la media de la nueva variable <span class="math inline">\(Z_n\)</span> sea cero y su desviación estándard sea igual a 1. De esta forma, las variables <span class="math inline">\(Z_n\)</span> convergerán a una distribución normal estándar <span class="math inline">\(N(0,1)\)</span>, cuando <span class="math inline">\(n\)</span> tienda a infinito.<br>
</p>
<p>Definición 2: Sea <span class="math inline">\(X_1,\,X_2,\,X_3,...,X_n\)</span> un conjunto de variables aleatorias, independientes e idénticamente distribuidas con media <span class="math inline">\(\mu\)</span> y varianza <span class="math inline">\(\sigma^2\)</span> distinta de cero. Entonces, si <span class="math inline">\(n\)</span> es suficientemente grande, la variable aleatoria</p>
<p><span class="math display">\[\bar{X}=\frac{1}{n}\sum^n_{i=1}{X_i}\]</span></p>
<p>tiene aproximadamente una distribución normal con media <span class="math inline">\(\mu(\bar{X})=\mu\)</span> y desviación típica <span class="math inline">\(\sigma(\bar{X})=\sigma/\sqrt{n}\)</span>.<br>
</p>
<p><em>NOTA</em>: Es importante remarcar que el teorema del límite central no dice nada acerca de la distribución de <span class="math inline">\(X_i\)</span>, solo de la distibución de su media muestral <span class="math inline">\(\bar{X}\)</span>.<br>
</p>
<p>Aplicación 1: Calculo de probabilidades sobre la media muestral.<br>
</p>
<p>Ejemplo: La recolección de muestras de agua con una roseta es una variable aleatoria con media <span class="math inline">\(\mu=150\,{ ml}\)</span> y varianza de <span class="math inline">\(\sigma^2=120\,{ ml}^2\)</span>. Si tomamos <span class="math inline">\(n=40\)</span> muestras aleatorias de agua. (a) ?`Cual es la media y la desviación estándar de la media muestral?, (b) ¿Cual es la probabilidad de que la media muestral contenga entre <span class="math inline">\(145\)</span> y <span class="math inline">\(153\,{ ml}\)</span> de agua?<br>
</p>
<ol type="a">
<li><span class="math inline">\(\mu(\bar{X})=150\,{ ml}\)</span> y <span class="math inline">\(\sigma(\bar{X})=\sigma/\sqrt{n}=\sqrt{120/40}= \sqrt{3}\,{ ml}\)</span><br>
</li>
<li>Queremos calcular <span class="math inline">\(Pr(145 \le \bar{X} \le 153)\)</span>. Si escribimos la probabilidad en forma estandarizada, entonces:</li>
</ol>
<p><span class="math display">\[Pr(145 \le \bar{X} \le 153) =
      Pr\left( \frac{145-150}{\sqrt{3}} \le Z \le \frac{153-150}{\sqrt{3}}\right)
       \simeq Pr(-2.89 \le Z \le 1.73)=\]</span></p>
<p><span class="math display">\[=Pr(Z \le 1.73)- Pr(Z \le -2.89)=0.9582-(1-0.9981)=0.9582-0.0019=0.9563\]</span></p>
<p>{} Función de densidad de probabilidad conjunta<br>
</p>
<p>La probabilidad que dos variables aleatorias <span class="math inline">\((x,y)\)</span> caigan en la región <span class="math inline">\(R\)</span> (como por ejemplo un rectángulo) se obtiene integrando su función de probabilidad conjunta</p>
<p><span class="math display">\[P((x,y)\in R)=\int\int_{R} F(x,y) dx dy\,.\]</span></p>
<p>En particular, si <span class="math inline">\(R\)</span> es un rectángulo 2d <span class="math inline">\({(x,y):r\le x \le r+dr, s \le y \le s+ds}\)</span>, entonces</p>
<p><span class="math display">\[P((x,y)\in R)=P(r\le x \le r+dr, s \le y \le s+ds)=\int^{r+dr}_r\int^{s+ds}_{s} F(x,y) dx dy\,.\]</span></p>
<p>Algunas propiedades:<br>
</p>
<ol type="1">
<li><span class="math inline">\(F(x,y)\ge0\)</span> para todo x,y.<br>
</li>
<li><span class="math inline">\(\int^{\infty}_{-\infty}\int^{\infty}_{-\infty} F(x,y) dx dy=1\)</span><br>
</li>
</ol>
<p>Definición: La función de densidad de probabilidad marginal de variables aleatorias <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span> son:<br>
<span class="math inline">\(Fx(x)=\int^{\infty}_{-\infty} F(x,y)dy\)</span> y <span class="math inline">\(Fy(y)=\int^{\infty}_{-\infty} F(x,y)dx\,.\)</span></p>
<p>{} Ejemplo del uso de la función de densidad de probabilidad conjunta<br>
</p>
<p>Imaginemos que una empresa de instrumentación oceanográfica fabrica boyas Lagrangianas de grosor <span class="math inline">\(x\)</span> y diámetro <span class="math inline">\(y\)</span>, los cuales varian de una boya a la otra. Imaginemos que la función de densidad de probabilidad conjunta de la variable aleatoria “dimensión del instrumento oceanográfico” es:</p>
<p><span class="math display">\[F(x,y)=\frac{1}{6}(r+s)\,\,\,si\,\,\,(x,y)\in R=\{1\le x \le 2 ; 4 \le y \le 5\}\]</span></p>
<p><span class="math display">\[F(x,y)=0\,\,\,si\,\,\,(x,y)\,fuera\,de\,R\]</span></p>
<p>Ahora queremos saber que probabilidad hay de que una boya tenga un grosor <span class="math inline">\(1 \le x \le 1.5m\)</span> y un diámetro <span class="math inline">\(4.5 \le y \le 5m\)</span>, es decir <span class="math display">\[P(1 \le x \le 1.5, 4.5 \le y \le 5)=\int^{1.5}_{1} \int^{5}_{4.5} \frac{1}{6}(r+s) ds dr = 0.253= 25\%\]</span></p>
<p>{} Significancia estadística utilizando la distribución Normal<br>
</p>
<p>Como vimos en el teorema central del límite, para una población infinita (<span class="math inline">\(N\rightarrow\infty\)</span>) la desviación estándar de la distribución de las medias muestrales es:</p>
<p><span class="math display">\[\sigma(\bar{x})=\frac{\sigma}{\sqrt{N}}=error\,estándar\,del\,estimado\,de\,la\,media\,.\]</span></p>
<p>Aquí, <span class="math inline">\(\sigma\)</span> es la desviación estándar de la población y <span class="math inline">\(N\)</span> es el número de datos (independoentes) utilizado para calcular la media muestral. Entonces, si promediamos observaciones de una población de desviación estándar <span class="math inline">\(\sigma\)</span>, la desviación estándar de esos promedios disminuye como el inverso de la raíz cuadrada del tamaño muestral <span class="math inline">\(N\)</span>.<br>
</p>
<p>Si <span class="math inline">\(N\)</span> es suficientemente grande podemos usar las estimaciones de <span class="math inline">\(\sigma\)</span> y <span class="math inline">\(\bar{x}\)</span> para calcular el denominado estadístico <span class="math inline">\(z\)</span> que corresponde a una distribución normal estandarizada de media <span class="math inline">\(\mu=0\)</span> y <span class="math inline">\(\sigma=1\)</span></p>
<p><span class="math display">\[z=\frac{\bar{x}-\mu}{\sigma(\bar{x})}=\frac{\bar{x}-\mu}{\frac{\sigma}{\sqrt{N}}}\]</span><br>
</p>
<p>La fórmula de arriba puede modificarse convenientemente para darnos un test de significancia estadística para la diferencia entre medias muestrales con tamaños muestrales y desviación estándar diferentes:</p>
<p><span class="math display">\[z=\frac{\bar{x}_1-\bar{x}_2-\Delta_{1,2}}
  {\frac{\sigma_1^2}{\sqrt{N_1}} + \frac{\sigma_2^2}{\sqrt{N_2}}}\,,\]</span></p>
<p>donde <span class="math inline">\(\Delta_{1,2}\)</span> es la diferencia esperada entre las dos medias, lo que se suele asumir cero en la práctica.<br>
</p>
<p>Si el tamaño muestral <span class="math inline">\(N\)</span> es menor de <span class="math inline">\(30\)</span> entonces no podemos usar el estadístico <span class="math inline">\(z\)</span>, pero podemos utilizar la distribución <em>t-student</em>; o cuando queremos comparar varianzas, podemos usar la distribución <em>chi-cuadrada</em>. La <em>t-student</em> converge a una distribución normal para largos tamaños muestrales y se define como</p>
<p><span class="math display">\[t=\frac{\bar{x}-\mu}{\frac{s}{\sqrt{N-1}}}=\frac{\bar{x}-\mu}{\frac{\hat{s}}{\sqrt{N}}};
\hat{s}=\sqrt{\frac{N}{N-1}s}\,.\]</span></p>
<p>Si consideramos una población normalmente distribuida de media <span class="math inline">\(\mu\)</span> la función de densidad de probabilidad de la <em>t-student</em> es</p>
<p><span class="math display">\[F_{x}(t)=\frac{f_0(\nu)}{\left(1+\frac{t^2}{\nu} \right)^{\frac{\nu+1}{2}}}\,,\]</span></p>
<p>donde <span class="math inline">\(\nu=N-1\)</span> es el número de grados de libertad y <span class="math inline">\(f_0(\nu)\)</span> es una constante que depende en <span class="math inline">\(\nu\)</span> y permite que el área bajo la curva <span class="math inline">\(F_x(t)\)</span> sea igual a la unidad. Los grados de libertad se definen como el número de muestras independientes <span class="math inline">\(N\)</span> menos el número de parámetros del estadístico que queremos estimar.<br>
</p>
<p>A diferencia del estadístico <span class="math inline">\(z\)</span>, la <em>t-student</em> depende del número de grados de libertad; la cola de la distribución es larga para números de grados de libertad bajos (o <span class="math inline">\(N\)</span> pequeña). Para números altos de grados de libertad (o <span class="math inline">\(N\)</span> grande), la distribución <em>t-student</em> se acerca al estadístico <span class="math inline">\(z\)</span> o distribución Normal.</p>
<p><strong>Intervalos de confianza</strong><br>
</p>
<p>Para calcular valores de los estadísticos <span class="math inline">\(z\)</span> y <em>t-student</em> debemos de fijar el nivel de confianza definido como <span class="math inline">\(1-\alpha\)</span>; porcentaje del nivel de confianza <span class="math inline">\(100(1-\alpha)\%\)</span>. Esto se puede escribir simbolicamente cómo</p>
<p><span class="math display">\[P(-z_{\alpha/2}&lt;z&lt;z_{\alpha/2})=1-\alpha\]</span></p>
<p><span class="math display">\[P(-t_{\alpha/2}&lt;t&lt;t_{\alpha/2})=1-\alpha\,.\]</span></p>
<p>Una vez definido el nivel de confianza y los grados de libertad <span class="math inline">\(\nu\)</span> (para la <em>t-student</em>) podemos leer el valor de dichos estadísticos en tablas. En esas tablas <span class="math inline">\(z_{\alpha/2}\)</span> es el valor de <span class="math inline">\(z\)</span> para el cual solo el <span class="math inline">\(100*{\alpha/2}\%\)</span> de los valores de <span class="math inline">\(z\)</span> es esperado ser mas grande (cola de la derecha de la distribución). Igualmente, <span class="math inline">\(z_{-\alpha/2}=-z_{\alpha/2}\)</span> es el valor de <span class="math inline">\(z\)</span> para el cual solo el <span class="math inline">\(100*{\alpha/2}\%\)</span> de los valores de <span class="math inline">\(z\)</span> es esperado ser mas pequeño (cola de la izquierda de la distribución). O dicho de otra forma,<span class="math inline">\(z_{\alpha/2}\)</span> es el valor por encima del cual existe un área bajo la curva de <span class="math inline">\(\alpha/2\)</span>. Los valores de <span class="math inline">\(z\)</span> y <span class="math inline">\(t\)</span> son las integrales bajo las correspondientes funciones de densidad de probabilidad.</p>
<ol type="1">
<li><strong>Intervalo de confianza</strong> para <span class="math inline">\(\mu\)</span> (<span class="math inline">\(N&gt;30\)</span>, <span class="math inline">\(\sigma\)</span> conocida)<br>
</li>
</ol>
<p>Cuando <span class="math inline">\(N&gt;30\)</span> y <span class="math inline">\(\sigma\)</span> es conocida, podemos usar el estadístico <span class="math inline">\(z\)</span> para encontrar el intervalo de confianza para <span class="math inline">\(\mu\)</span>. Hay un <span class="math inline">\(100*(1-\alpha)\%\)</span> que cualquier estadístico <span class="math inline">\(z\)</span> caiga en el intervalo</p>
<p><span class="math display">\[z_{-\alpha/2}&lt;\frac{\bar{x}-\mu}{\sigma}\sqrt{N}&lt;z_{\alpha/2}\]</span> <span class="math display">\[\frac{\sigma}{\sqrt{N}}z_{-\alpha/2}&lt;{\bar{x}-\mu}&lt;\frac{\sigma}{\sqrt{N}}z_{\alpha/2}\]</span> <span class="math display">\[-1\frac{\sigma}{\sqrt{N}}z_{-\alpha/2}&gt;{\mu-\bar{x}}&gt;-1\frac{\sigma}{\sqrt{N}}z_{\alpha/2}\]</span> <span class="math display">\[\bar{x}-1\frac{\sigma}{\sqrt{N}}z_{-\alpha/2}&gt;\mu&gt;\bar{x}-1\frac{\sigma}{\sqrt{N}}z_{\alpha/2}\]</span></p>
<p>y sabiendo que es simétrica <span class="math inline">\(-z_{\alpha/2}=z_{-\alpha/2}\)</span>:</p>
<p><span class="math display">\[\bar{x}-z_{\alpha/2}\frac{\sigma}{\sqrt{N}} &lt; \mu &lt;
   \bar{x}+z_{\alpha/2}\frac{\sigma}{\sqrt{N}}\,.\]</span></p>
<p>Supongamos que queremos encontrar el intervalo de confianza de <span class="math inline">\(\mu\)</span> al <span class="math inline">\(95\%\)</span> de confianza, es decir, entonces <span class="math inline">\(\alpha=0.05\)</span>. Entonces <span class="math inline">\(z_{\alpha/2}=1.96\)</span> (de tablas estadísticas).<br>
</p>
<p><strong>Ejemplo</strong>: <span class="math inline">\(N=40\)</span>, <span class="math inline">\(\sigma=0.5^\circ{C}\)</span>, y <span class="math inline">\(\bar{x}=12.7^\circ{C}\)</span>:</p>
<p><span class="math display">\[\bar{x}-z_{0.025}\frac{\sigma}{\sqrt{N}}&lt;\mu&lt;\bar{x}+z_{0.025}\frac{\sigma}{\sqrt{N}}\,,\]</span></p>
<p><span class="math display">\[\left[12.7-(1.96)0.5/\sqrt{40}\right]\,^\circ{C}&lt;\mu&lt;\left[12.7+(1.96)0.5/\sqrt{40}\right]\,^\circ{C}\]</span> <span class="math display">\[12.54^\circ{C} &lt; \mu &lt; 12.85^\circ{C}\]</span></p>
<ol start="2" type="1">
<li><strong>Intervalo de confianza</strong> para <span class="math inline">\(\mu\)</span> (<span class="math inline">\(N&lt;30\)</span>, <span class="math inline">\(\sigma\)</span> desconocida)<br>
Cuando <span class="math inline">\(N&lt;30\)</span> y <span class="math inline">\(\sigma\)</span> es desconocida, podemos usar el estadístico <span class="math inline">\(t\)</span>-student para encontrar el intervalo de confianza para <span class="math inline">\(\mu\)</span>. Hay un <span class="math inline">\(100*(1-\alpha)\%\)</span> que cualquier estadístico <span class="math inline">\(t\)</span> caiga en el intervalo</li>
</ol>
<p><span class="math display">\[t_{-\alpha/2}&lt;\frac{\bar{x}-\mu}{s}\sqrt{N-1}&lt;t_{\alpha/2}\,,\]</span></p>
<p><span class="math display">\[\bar{x}-t_{\alpha/2}\frac{s}{\sqrt{N-1}} &lt; \mu &lt;
   \bar{x}+t_{\alpha/2}\frac{s}{\sqrt{N-1}}\,.\]</span></p>
<p>Si <span class="math inline">\(\alpha=0.05\)</span>, hay un 95% de probabilidad que cualquier estadístico <span class="math inline">\(t\)</span> caiga en el intervalo <span class="math display">\[t_{-0.025}&lt;\frac{\bar{x}-\mu}{s}\sqrt{N-1}&lt;t_{0.025}\,,\]</span> de lo cual podemos deducir que la verdadera media <span class="math inline">\(\mu\)</span> es de esperar con un 95% de confianza que caiga en el intervalo: <span class="math display">\[\bar{x}-t_{0.025}\frac{s}{\sqrt{N-1}}&lt;\mu&lt;\bar{x}+t_{0.025}\frac{s}{\sqrt{N-1}}\,.\]</span></p>
<p>De forma general, podemos definir el intervalo de confianza como: <span class="math display">\[\mu=\bar{x}\pm t_c\frac{\hat{s}}{\sqrt{N}}\,,\]</span> donde <span class="math inline">\(t_c\)</span> es el valor crítico del estadístico <span class="math inline">\(t\)</span> (límites del intervalo), el cual depende del número de grados de libertad y del nivel de confiabilidad deseado. El intervalo de confianza con el estadístico <span class="math inline">\(z\)</span>, el cual solo es apropiado para tamaños muestrales grandes (<span class="math inline">\(N&gt;30\)</span>) donde la desviación estándar es conocida: <span class="math display">\[\mu=\bar{x}\pm z_c\frac{\sigma}{\sqrt{N}}\,.\]</span> Observamos que la teoría para tamaños muestrales pequeños reemplaza el estadístico <span class="math inline">\(z\)</span> por el <span class="math inline">\(t\)</span> y utiliza una desviación estándar muestral modificada <span class="math display">\[\hat{s}=\sqrt{\frac{N}{N-1}s}\,.\]</span> % % {iferencias entre medias}\ %Supongamos dos muestras de tamaño <span class="math inline">\(N_1\)</span> y <span class="math inline">\(N_2\)</span> extraidas de una población %con distribución normal con desviaciones estándar siguales. Supongamos %que las medias muestrales son <span class="math inline">\(\bar{x_1}\)</span> y <span class="math inline">\(\bar{x_2}\)</span> y las %desviaciones estándar muestrales son <span class="math inline">\(s_1\)</span> y <span class="math inline">\(s_2\)</span>. Para comprobar %la hipótesis nula (<span class="math inline">\(H_0\)</span>) que ambas muestras provienen de la misma población, %es decir <span class="math inline">\(\mu_1=\mu_2\)</span> y <span class="math inline">\(\sigma_1=\sigma_2\)</span> podemos usar la siguiente expresión</p>
<p><span class="math display">\[t=\frac{(\bar{x_1}-\bar{x_2})-(\mu_1-\mu_2)}{\sigma\sqrt{\frac{1}{N_1} + \frac{1}{N_2}}};\]</span></p>
<p>donde <span class="math inline">\(\nu=N_1+N_2-2\)</span>.<br>
</p>
<ol start="3" type="1">
<li><strong>Intervalo de confianza</strong> para la diferencia de medias <span class="math inline">\(\mu_1 - \mu_2\)</span>}<br>
</li>
</ol>
<p>El teorema central del límite (TCL) para la diferencia de medias muestrales de dos poblaciones viene dado por</p>
<p><span class="math display">\[\bar{x}_1-\bar{x}_2\,\sim\,{ N}(\mu_{\bar{x}_1-\bar{x}_2},\sigma_{\bar{x}_1-\bar{x}_2})\,,\]</span><br>
</p>
<p>donde <span class="math display">\[{ Media:}\,\,\mu_{\bar{x}_1-\bar{x}_2}=\mu_{\bar{x}_1}-\mu{\bar{x}_2}=\mu_1-\mu_2\]</span></p>
<p><span class="math display">\[\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,{ Varianza:}\,\,\sigma^2_{\bar{x}_1-\bar{x}_2}=
   \sigma^2{\bar{x}_1}+\sigma^2{\bar{x}_2}=
   \sqrt{  \frac{{\sigma_1}^2}{N_1} + \frac{{\sigma_2}^2}{N_2} }\]</span><br>
</p>
<p> Desviaciones estándar poblacionales (<span class="math inline">\(\sigma_1\)</span> y <span class="math inline">\(\sigma_2\)</span>) conocidas; <span class="math inline">\(\mu_1\)</span> y <span class="math inline">\(\mu_2\)</span> desconocidas} (estadístico z):</p>
<p><span class="math display">\[z=\frac{\bar{x}_1-\bar{x}_2-(\mu_1-\mu_2)}
         {\sqrt{\frac{{\sigma_1}^2}{N_1}+\frac{{\sigma_2}^2}{N_2}}}\]</span><br>
</p>
<p><strong>3.2</strong> Desviaciones estándar poblacionales (<span class="math inline">\(\sigma_1\)</span> y <span class="math inline">\(\sigma_2\)</span>) y medias poblacionales (<span class="math inline">\(\mu_1\)</span> y <span class="math inline">\(\mu_2\)</span>) desconocidas (estadístico t):</p>
<p><span class="math display">\[t=\frac{\bar{x}_1-\bar{x}_2-(\mu_1-\mu_2)}
              {\sqrt{\frac{{s_1}^2}{N_1}+\frac{{s_2}^2}{N_2}}}\]</span><br>
</p>
<p><strong>3.3</strong> Desviaciones estándar poblacionales (<span class="math inline">\(\sigma_1\)</span> y <span class="math inline">\(\sigma_2\)</span>) desconocidas pero iguales (estadístico t):<br>
</p>
<p>Supongamos dos muestras de tamaño <span class="math inline">\(N_1\)</span> y <span class="math inline">\(N_2\)</span> extraídas de dos poblaciones Normales con desviaciones estándar iguales (<span class="math inline">\(\sigma_1=\sigma_2\)</span>). Supongamos que conocemos las medias y desviaciones estándar muestrales <span class="math inline">\(\bar{x}_1\)</span> y <span class="math inline">\(\bar{x}_2\)</span> y <span class="math inline">\(s_1\)</span> y <span class="math inline">\(s_2\)</span>. Para comprobar la hipótesis nula <span class="math inline">\(H_o\)</span> que las muestran proceden de la misma población (<span class="math inline">\(\mu_1=\mu_2\)</span> y <span class="math inline">\(\sigma_1=\sigma_2\)</span>) usamos el estadístico t-score (o pooled t):</p>
<p><span class="math display">\[t=\frac{\bar{x}_1-\bar{x}_2-(\mu_1-\mu_2)}
         {\hat{s}_d\sqrt{\frac{1}{N_1}+\frac{1}{N_2}}}\]</span> \ <span class="math display">\[\hat{s}_d=\sqrt{\frac{(N_1-1)s^2_1 + (N_2-1)s^2_2}{N_1+N_2-2}}\,,\]</span><br>
</p>
<p>donde <span class="math inline">\(\nu=N_1+N_2-2\)</span> es el número de grados de libertad.<br>
</p>
<p><strong>Ejemplo</strong>: Un ingeniero que diseña instrumentos oceanográficos está ineteresado en aumentar el tiempo durante el cuál la pintura “antifouling” evita que los microorganismos se peguen y crezcan sobre el instrumento oceanográfico. Se prueban dos fórmulas de pintura: fórmula 1 estándar y fórmula 2 con un nuevo ingrediente que aumenta el tiempo de acción.</p>
<p>De la experiencia se sabe que la desviación estándar del tiempo de acción de la pintura es de 8 días y ésta variabilidad no se vé afectada por el nuevo ingrediente. Se pintan 35 instrumentos con la fórmula 1 y otros 35 con la fórmula 2. Los tiempos promedios de acción del “antifouling” son de 116 días para la fórmula 1 y 112 días para la fórmula 2. ¿A qué conclusión puede llegar el ingeniero diseñador del instrumento sobre la eficacia del nuevo ingrediente, con un nivel de significancia de 0.01?\</p>
<p><span class="math inline">\(x_1\equiv\)</span> Tiempo de acción “antifouling” fórmula 1<br>
</p>
<p><span class="math inline">\(x_2\equiv\)</span> Tiempo de acción “antifouling” fórmula 2<br>
<span class="math inline">\(x_1\sim\)</span> Desconocida (<span class="math inline">\(\mu_1,\sigma_1=\)</span> 8 días)<br>
<span class="math inline">\(x_2\sim\)</span> Desconocida (<span class="math inline">\(\mu_2,\sigma_2=\)</span> 8 días)<br>
<span class="math inline">\(\bar{x}_1-\bar{x}_2\,\sim\,{ N}(\mu_1-\mu_2,\sigma_1/\sqrt{N_1} + \sigma_2/\sqrt{N_2})\)</span> (TCL)<br>
<span class="math inline">\(\bar{x}_1=116\)</span> días\ <span class="math inline">\(\bar{x}_2=112\)</span> días<br>
<span class="math inline">\(N_1=N_2=35\)</span>&nbsp; <span class="math inline">\(\alpha=0.01\)</span><br>
<span class="math inline">\(H_0:\,\mu_1 - \mu_2 =0\)</span><br>
<span class="math inline">\(H_1:\,\mu_1 - \mu_2 \neq0\)</span><br>
</p>
<p>El intervalo de confianza de la diferencia de medias <span class="math inline">\(\mu_1-\mu_2\)</span></p>
<p><span class="math display">\[\bar{x}_1-\bar{x}_2-z_{\alpha/2} \left(
   \frac{\sigma_1}{\sqrt{N_1}} + \frac{\sigma_2}{\sqrt{N_2}}\right)&lt;
   \mu_1-\mu_2 &lt;
   \bar{x}_1-\bar{x}_2+z_{\alpha/2} \left(
   \frac{\sigma_1}{\sqrt{N_1}} + \frac{\sigma_2}{\sqrt{N_2}}\right)\]</span></p>
<p><span class="math display">\[\bar{x}_1-\bar{x}_2-2.33 (1.9124)&lt;
   \mu_1-\mu_2 &lt;
   \bar{x}_1-\bar{x}_2+2.33 (1.9124)\]</span></p>
<p><span class="math display">\[\bar{x}_1-\bar{x}_2-4.4559&lt;
   \mu_1-\mu_2 &lt;
   \bar{x}_1-\bar{x}_2+4.4559\]</span></p>
<p><span class="math display">\[4-4.4559&lt;
   \mu_1-\mu_2 &lt;
   4+4.4559\,,\]</span><br>
</p>
<p>y el intervalo de confianza es:<br>
</p>
<p><span class="math display">\[-0.4559&lt;\mu_1-\mu_2&lt;8.4559\,\,{ al}\,\,99\%\,(\alpha=0.01)\]</span></p>
<p><span class="math display">\[H_0:\,\mu_1 - \mu_2 = 0\,\,{ al}\,\,99\%\,,\]</span></p>
<p>y aceptamos hipótesis nula <span class="math inline">\(H_0\)</span>.<br>
</p>
<ol start="4" type="1">
<li><strong>Intervalo de confianza para la varianza:</strong></li>
</ol>
<p>Distribución <em>chi-cuadrada</em><br>
</p>
<p>En ocasiones queremos definir un intervalo de confianza para la varianza muestral. Para ello podemos usar el estadístico <em>chi-cuadrado</em>. Definamos</p>
<p><span class="math display">\[\chi^2=(N-1)\frac{s^2}{\sigma^2}\,.\]</span><br>
</p>
<p><strong>Propiedades:</strong></p>
<p>Para definir el intervalo de confianza sabemos que hay un <span class="math inline">\(100*(1-\alpha)\%\)</span> que cualquier estadístico <span class="math inline">\(\chi^2\)</span> caiga en el intervalo</p>
<p><span class="math display">\[\chi^2_{1-\alpha/2}&lt;(N-1)\frac{s^2}{\sigma^2}&lt;\chi^2_{\alpha/2}\,,\]</span> <span class="math display">\[\frac{1}{\chi^2_{1-\alpha/2}}&gt;\frac{\sigma^2}{(N-1)s^2}&gt;\frac{1}{\chi^2_{\alpha/2}}\,,\]</span></p>
<p>y entonces:</p>
<p><span class="math display">\[\frac{(N-1)s^2}{\chi^2_{\alpha/2}}&lt;\sigma^2&lt;\frac{(N-1)s^2}{\chi^2_{1-\alpha/2}}\,.\]</span></p>
<p>Usamos <span class="math inline">\(1-{\alpha/2}\)</span> porque la <span class="math inline">\(\chi^2\)</span> es positiva. El valor <span class="math inline">\(\chi^2_{\alpha/2}\)</span> es mayor que el valor <span class="math inline">\(\chi^2_{1-\alpha/2}\)</span>. Las tablas dan la probabilidad a la derecha del valor.<br>
Para una población normalmente distribuida con desviación estándar <span class="math inline">\(\sigma\)</span>, la función de densidad de probabilidad de la <em>chi-cuadrada</em> es: <span class="math display">\[F_x(\chi)=f_0\chi^{\nu-2}e^{-\frac{1}{2}\chi^2};\,\,\nu=N-1\,.\]</span></p>
<p>Puesto que la distribución <em>chi</em> es asimétrica y positiva, si <span class="math inline">\(\alpha=0.05\)</span> (95% confianza), el intervalo de confianza para la varianza <span class="math inline">\(\sigma^2\)</span> como <span class="math display">\[\frac{(N-1)s^2}{\chi^2_{0.025}}&lt;\sigma^2&lt;\frac{(N-1)s^2}{\chi^2_{0.975}}\,,\]</span></p>
<p>y si leemos en las tablas para <span class="math inline">\(\nu=9\)</span> grados de libertad:</p>
<p><span class="math display">\[\frac{(N-1)s^2}{19.023}&lt;\sigma^2&lt;\frac{(N-1)s^2}{2.700}\,,\]</span></p>
<p><strong>Ejemplo</strong>: Supongamos que tenemos <span class="math inline">\(\nu=9\)</span> grados de libertad de nuestra estimación espectral de la componente meridional de la velocidad de la corriente. Sabemos que la varianza muestral de un pico espectral es <span class="math inline">\(s^2=10\,{ cm}\,{ s}^2\)</span> ?`Cuál es el intervalo de confianza al 95% para la varianza?<br>
</p>
<p>De las tablas estadísticas vemos que para <span class="math inline">\(\nu=N-1=9\)</span> grados de libertad, <span class="math inline">\(\chi^2_{1-\alpha/2}=\chi^2_{0.095}=19.02\)</span> y <span class="math inline">\(\chi^2_{\alpha/2}=\chi^2_{0.025}=2.70\)</span>. Entonces, el intervalo es:</p>
<p><span class="math display">\[\frac{(9)10}{19.023}&lt;\sigma^2&lt;\frac{(9)10}{2.700}\]</span> <span class="math display">\[4.7\,{ cm}^2\,{ s}^{-2}&lt;\sigma^2&lt;33.3\,{ cm}^2\,{ s}^{-2}\]</span></p>
<p>{rados de libertad}\ El número de grados de libertad es el número de muestras independientes N menos el número de parámetros del estadístico que queremos estimar. Por ejemplo en el estadístico <span class="math inline">\(t\)</span> <span class="math display">\[t=\frac{\bar{x}-\mu}{\frac{s}{\sqrt{N-1}}}=\frac{\bar{x}-\mu}{\frac{\hat{s}}{\sqrt{N}}};
\hat{s}=\sqrt{\frac{N}{N-1}s}\,,\]</span> calculamos la media muestral y la desviación estándar <span class="math inline">\(s\)</span> a partir de los datos, pero la verdadera media <span class="math inline">\(\mu\)</span> debe ser estimada, por lo que <span class="math inline">\(\nu=N-1\)</span>. Similarmente en el estadístico <em>chi-cuadrada</em> <span class="math display">\[\chi^2=(N-1)\frac{s^2}{\sigma^2}\,,\]</span> conocemos la varianza muestral <span class="math inline">\(s^2\)</span> y el tamaño muestral <span class="math inline">\(N\)</span>, pero debemos estimar la verdadera varianza, y entonces <span class="math inline">\(\nu=N-1\)</span>.</p>
<p>{stadístico <span class="math inline">\(F\)</span>}\ Otro estadístico útil para tests espectrales es el estadístico <span class="math inline">\(F\)</span>. Si <span class="math inline">\(s^2_1\)</span> y <span class="math inline">\(s^2_2\)</span> son las varianzas de muestras aleatorias independientes de tamaño <span class="math inline">\(N_1\)</span> y <span class="math inline">\(N_2\)</span>, tomadas de dos poblaciones Normales con la misma varianza <span class="math inline">\(\sigma^2\)</span>, entonces <span class="math display">\[F=\frac{s^2_1}{s^2_2}\,,\]</span> es el valor de una variable aleatria cuya distribución es <span class="math inline">\(F\)</span> con los parámetros <span class="math inline">\(\nu_1=N_1-1\)</span> y <span class="math inline">\(\nu_2=N_2-1\)</span>. Este estadístico es muy útil en tests de significancia para los picos de los espectros frecuenciales de potencia. Los dos parámetros son los grados de libertad para las varianzas del cociente; <span class="math inline">\(\nu_1\)</span> para <span class="math inline">\(s^2_1\)</span> y <span class="math inline">\(\nu_2\)</span> para <span class="math inline">\(s^2_2\)</span>.</p>
<p>{ests para hipótesis}\ Para usar los test de significancia estadística debemos de seguir 5 pasos:<br>
(1) Definir el nivel de confianza<br>
(2) Definir la hipótesis nulla <span class="math inline">\(H_0\)</span> y su alternativa <span class="math inline">\(H_1\)</span><br>
(3) Definir el estadístico que usaremos<br>
(4) Definir la región crítica<br>
(5) Evaluar el estadístico y concluir<br>
</p>
<p>Es muy importante definir correctamente la hipótesis nula. Es decir, estar seguro que rechazar la hipótesis nula <span class="math inline">\(H_0\)</span> implica únicamente la existencia de su alternativa <span class="math inline">\(H_1\)</span>. Normalmente la hipótesis nula y su alternativa son mutualmente excluyentes. Ejemplos:<br>
</p>
<p><span class="math inline">\(H_0\)</span>: Las medias de dos muestras son iguales <span class="math inline">\(H_1\)</span>: Las medias de dos muestras no son iguales<br>
</p>
<p><span class="math inline">\(H_0\)</span>: El coeficiente de correlación es cero <span class="math inline">\(H_1\)</span>: El coeficiente de correlación no es cero<br>
</p>
<p>\ En una muestra de 41 inviernos la temperatura media de Enero es <span class="math inline">\(5.55^\circ{C}\)</span> y la desviación es de <span class="math inline">\(0.65^\circ{C}\)</span> ?`Cual es el intervalo de confianza al 95% de que la verdadera temperatura media sea esa? \ (1) Nivel de confianza del 95% \ (2) <span class="math inline">\(H_0\)</span> es que la media verdadera se encuentra en el intervalo <span class="math inline">\(5.55\pm \Delta{T}\)</span> y su alternativa <span class="math inline">\(H_1\)</span> es que se encuentra fuera de este intervalo.<br>
(3) Usamos el estadístico <span class="math inline">\(t\)</span>.<br>
(4) La región crítica es <span class="math inline">\(|t|&lt;t_{0.025}\)</span>, lo cual para <span class="math inline">\(\nu=N-1=40\)</span> es <span class="math inline">\(|t|&lt;2.26\)</span> (leido de tablas estadísticas). Escrito en términos de intervalo de confianza para la media poblacional: <span class="math display">\[\bar{x}-2.0211\frac{s}{\sqrt{N-1}}&lt;\mu&lt;\bar{x}+2.0211\frac{s}{\sqrt{N-1}}  \]</span><br>
(5) Si ponemos en números el intervalo obtenemos <span class="math inline">\(5.06&lt;\mu&lt;6.03\)</span>. Tenemos un 95% de confianza que la verdadera temperatura media se encuentra en ese intervalo.</p>
<p>{eorema de Bayes:}\ Sea <span class="math inline">\(E_i\,,i=1,2,3,...,n\)</span> un conjunto de <span class="math inline">\(n\)</span> eventos que constituyen una partición del espacio muestral <span class="math inline">\(S\)</span></p>
<p><span class="math display">\[\bigcup^n_{i=1}E_i\in S,\]</span></p>
<p>{}cada uno de los cuales tiene probabilidad positiva de ocurrir <span class="math inline">\(P(Ei)&gt;0\)</span> para <span class="math inline">\(i=1,2,....,n\)</span> y son exclusivos entre si <span class="math display">\[E_i\cap E_j=\o \,\,\,\,i\ne j\,.\]</span> Entonces dada la ocurrencia previa de un evento cualquiera <span class="math inline">\(B\)</span>, la probabilidad de que suceda el evento <span class="math inline">\(E_j\)</span> es</p>
<span class="math display">\[\begin{equation}
P(E_j|B)=\frac{P(B|E_j)P(E_j)}{\sum^n_{i=1}P(B|E_i)P(E_i)}\,,
\end{equation}\]</span>
<p>{}donde</p>
<p><span class="math display">\[P(E_j|E_i)=\frac{P(E_i\cap E_j)}{P(E_i)}\,,\]</span></p>
<p>es la probabilidad condicional, es deicr, la probabilidad que ocurra el evento <span class="math inline">\(E_j\)</span> si previamente ha ocurrido el evento <span class="math inline">\(E_i\)</span> y <span class="math inline">\(P(E_i \cap E_j)\)</span> es la probabilidad que ambos eventos ocurran, i.e.&nbsp;la intersección de dos eventos. La intersección es puede escribir</p>
<p><span class="math display">\[P(E_i\cap E_j)=P(E_j|E_i)*P(E_i)=P(E_i|E_j)*P(E_j)\,.\]</span></p>
<p>Si ambos eventos son independientes (no interseccionan) tal que <span class="math inline">\(P(E_i|E_j)=P(E_i)\)</span> obtenemos</p>
<p><span class="math display">\[P(E_i\cap E_j)=P(E_i)*P(E_j)\,,\]</span></p>
<p>es decir, la definición de independencia estadística entre eventos.<br>
</p>
<p>{jemplo teorema de Bayes:} Imaginemos que queremos saber si una muestra de agua contiene diatomeas o no. La probabilidad de que una muestra de agua tomada al azar en la bahía de Todos Santos tenga diatomeas es de <span class="math inline">\(1/100\)</span> (<span class="math inline">\(P(D)=0.01\)</span>). La probabilidad de que si hay diatomeas el test de negativo es cero (<span class="math inline">\(P(+|D)=1\)</span>) y la probabilidad de que el test de un falso positivo es del <span class="math inline">\(5\%\)</span> (<span class="math inline">\(P(+|noD)=0.05\)</span>). Si agarramos una muestra de agua y da positivo, ?`Cual es la probabilidad de que hayan diatomeas? Intuitivamente, sabemos que existe un <span class="math inline">\(5\%\)</span> de probabilidad de que el test me de un falso positivo y por lo tanto, un <span class="math inline">\(95\%\)</span> de que si da positivo tenga diatomeas en la muestra de agua. Veamos que nos dice el teorema de Bayes. Sabemos que <span class="math inline">\(P(D)=0.01\)</span>, <span class="math inline">\(P(+|D)=1\)</span>, y <span class="math inline">\(P(+|noD)=0.05\)</span>. Si usamos el teorema de Bayes:</p>
<p><span class="math display">\[ P(D|+)=\frac{P(+|D)P(D)}{P(+|D)P(D) + P(+|noD)P(noD)}=
          \frac{1*0.01}{1*0.01+0.05*0.99}\sim\frac{1}{6}\,.\]</span></p>
<p>En verdad solamente existe <span class="math inline">\(1\)</span> probabilidad de <span class="math inline">\(6\)</span> (<span class="math inline">\(\sim16\%\)</span>) que si el test es positivo existan diatomeas. De cada <span class="math inline">\(100\)</span> muestras solamente <span class="math inline">\(1\)</span> tiene diatomeas y <span class="math inline">\(5\%*100=0.05*100=5\)</span> muestras de las <span class="math inline">\(100\)</span> dan positivo y no tienen diatomeas. Por ello, sabiendo que la muestra que tiene diatomeas dio positivo, de <span class="math inline">\(6\)</span> positivos, solamente <span class="math inline">\(1\)</span> es cierto.</p>
</section>
</section>
<section id="repaso-de-álgebra-lineal" class="level2">
<h2 class="anchored" data-anchor-id="repaso-de-álgebra-lineal">Repaso de álgebra Lineal</h2>
<p>Una matriz es un elemento matemático compuesto por filas y columnas. Una matriz rectangular de dimensiones <span class="math inline">\(m\times n\)</span> se define como <span class="math display">\[{\textbf A}=\left(\begin{array}{cccc}
  a_{11} &amp; a_{12} &amp; .... &amp; a_{1n}\\
  a_{21} &amp; a_{22} &amp; .... &amp; a_{2n}\\
    .    &amp;   .    &amp;      &amp;   . \\
    .    &amp;   .    &amp;      &amp;   . \\
    .    &amp;   .    &amp;      &amp;   . \\
  a_{m1} &amp; a_{m2} &amp; .... &amp; a_{mn}\\
\end{array}
  \right)
\,.\]</span></p>
<p>Un elemento de la matriz <span class="math inline">\({\textbf A}\)</span> queda definida como <span class="math inline">\({\textbf A}=[a_{ij}],\,\,\,i=1,2,...,m; j=1,2,...,n\,.\)</span> El primer índice <span class="math inline">\(i\)</span> denota filas y el segundo <span class="math inline">\(j\)</span> columnas.</p>
<p>{efinición} Un vector es una matriz con solo una columna (<span class="math inline">\(m\times1\)</span>) <span class="math display">\[{\textbf v}=\left( \begin{array}{c}
v_1 \\ v_2 \\ . \\ . \\ . \\ v_m
       \end{array} \right)
       \,.\]</span></p>
<p>{ectores ortogonales}</p>
<p>Sean dos vectores <span class="math inline">\({\textbf u}=[u_1,u_2,...,u_N]\)</span> y <span class="math inline">\({\textbf v}=[v_1,v_2,...,v_N]\)</span> de longitud <span class="math inline">\(N\)</span>, decimos que son ortogonales si <span class="math display">\[{\textbf u}\cdot{\textbf v}=({\textbf u},{\textbf v})=\sum\limits^N_{i=1} u_i v_i={\textbf u}^T{\textbf v}=0\,.\]</span></p>
<p>{orma, módulo, longitud de un vector}</p>
<p>Sea el vector <span class="math inline">\({\textbf u}=[u_1,u_2,...,u_N]\)</span>, entonces la norma de dicho vector se define como <span class="math display">\[||{\textbf u}||=|{\textbf u}|=\sqrt{u^2_1 + u^2_2 + ... + u^2_N}=({\textbf u},{\textbf u})^{1/2}=({\textbf u}^T{\textbf u})^{1/2}\,.\]</span></p>
<p>En general se puede obtener un {ector unitario} (de módulo <span class="math inline">\(=1\)</span>) dividiendo el vector por su norma: <span class="math display">\[{\textbf u}_I=\frac{{\textbf u}}{||{\textbf u}||}\,.\]</span></p>
<p>{} <strong>Vectores ortonormales</strong></p>
<p>Dos vectores ortogonales <span class="math inline">\({\textbf u}\)</span> y <span class="math inline">\({\textbf v}\)</span> si tienen módulo la unidad, entonces se denominan vectores ortonormales.</p>
<p>{uma de vectores}</p>
<p>La suma de 2 vectores <span class="math inline">\({\textbf u}=[u_1,u_2,...,u_N]\)</span> y <span class="math inline">\({\textbf v}=[v_1,v_2,...,v_N]\)</span> de longitud <span class="math inline">\(N\)</span> se define como <span class="math display">\[{\textbf u} + {\textbf v}= [u_1 + v_1, u_2 + v_2,..., u_N + v_N]=\sum\limits^N_{i=1} u_i + v_i\,.\]</span></p>
<p>{ombinación lineal de vectores}</p>
<p>Un vector <span class="math inline">\({\textbf y}\)</span> se dice que es combinación lineal de un conjunto de vectores <span class="math inline">\({\textbf x}_1, {\textbf x}_2, ...,{\textbf x}_N\)</span> si se puede expresar como la suma de los <span class="math inline">\(N\)</span> vectores multiplicados por <span class="math inline">\(N\)</span> coeficientes escalares <span class="math inline">\(a_1,a_2,...,a_N\)</span>:</p>
<p><span class="math display">\[{\textbf y}=a_1 {\textbf x}_1 + a_2 {\textbf x}_2 +...+a_N {\textbf x}_N=\sum\limits^N_{i=1}a_i {\textbf x}_i\,.\]</span></p>
<p>{ndependencia lineal}</p>
<p>Un conjunto de vectores <span class="math inline">\({\textbf y}_1, {\textbf y}_2, ...,{\textbf y}_N\)</span> se dice que es linealmente independiente si existe una combinación lineal finita de los vectores del conjunto tal que:</p>
<p><span class="math display">\[\sum\limits^N_{i=1} a_i {\textbf y}_i=a_1{\textbf y}_1 + a_2{\textbf y}_2+...+a_N{\textbf y}_N=0\,,\]</span></p>
<p>que se satisface cuando no todos los coeficientes son cero. En caso contarrio, se dice que son linealmente dependientes.</p>
<p>{} <strong>Ortonormalización Gram-Schmidt</strong></p>
<p>Es un método para convertir un conjunto de vectores <span class="math inline">\({\textbf v}\)</span> en vectores ortonormales. De forma general el proceso definido por Gram-Schmidt para ortonormalizar el vector ortogonal <span class="math inline">\({\textbf w}_k\)</span> a partir de un conjunto de vectores ortonormales <span class="math inline">\({\textbf u}_i=[{\textbf u}_1, {\textbf u}_2,...,{\textbf u}_{k-1}]\)</span> se define como:</p>
<p><span class="math display">\[{\textbf w}_k={\textbf v}_k-\sum\limits^{k-1}_{i=1} {\textbf u}_i\cdot{\textbf v}_k{\textbf u}_i\,.\]</span></p>
<p><span class="math display">\[{\textbf v}_k\equiv\,\,{ vectores}\,\,{ originales}\]</span></p>
<p><span class="math display">\[{\textbf u}_i\equiv\,\,{ vectores}\,\,{ ortonormales}\]</span></p>
<p><span class="math display">\[{\textbf w}_k\equiv\,\,{ vectores}\,\,{ ortogonales}\,\,{ a}\,\,{\textbf u}_j\]</span></p>
<p><strong>Ejemplo</strong>:<br>
</p>
<p>Convertir el conjunto de vectores de la base <span class="math inline">\(A\)</span> en una base ortonormal:</p>
<p><span class="math display">\[{\textbf A}=\left(\begin{array}{cccc}
  1 &amp; 2 &amp; 1\\
  0 &amp; 2 &amp; 0\\
  2 &amp; 3 &amp; 1\\
  1 &amp; 1 &amp; 0\\
\end{array}
  \right)
\,.\]</span></p>
<p>Primero normalizamos el primer vector columna <span class="math inline">\({\textbf v}_1\)</span>: <span class="math display">\[{\textbf u}_1=\frac{{\textbf v}_1}{||{\textbf v}_1||}=\left[\frac{1}{\sqrt{6}},0,\frac{2}{\sqrt{6}},\frac{1}{\sqrt{6}}\right]\,.\]</span></p>
<p>(1er vector ortonormal)<br>
</p>
<p>Ahora usamos la fórmula de arriba para encontrar el vector <span class="math inline">\({\textbf w}_2\)</span> ortogonal a <span class="math inline">\({\textbf u}_1\)</span></p>
<p><span class="math display">\[{\textbf w}_2={\textbf v}_2- {\textbf u}_1\cdot{\textbf v}_2{\textbf u}_1=[2,2,3,1]-
   \left[\frac{1}{\sqrt{6}},0,\frac{2}{\sqrt{6}},\frac{1}{\sqrt{6}}\right]\cdot
   [2,2,3,1]\left[\frac{1}{\sqrt{6}},0,\frac{2}{\sqrt{6}},\frac{1}{\sqrt{6}}\right]=\]</span></p>
<p><span class="math display">\[=[2,2,3,1]-\left(\frac{9}{\sqrt{6}}\right)\left[\frac{1}{\sqrt{6}},0,\frac{2}{\sqrt{6}},\frac{1}{\sqrt{6}}\right]=
   [2,2,3,1]-\left[\frac{3}{2},0,3,\frac{3}{2}\right]=\left[ \frac{1}{2},2,0,\frac{-1}{2}\right]\,.\]</span></p>
<p>(vector ortogonal a <span class="math inline">\(u_1\)</span>)</p>
<p>Normalizamos <span class="math inline">\({\textbf w}_2\)</span> para obtener el primer vector ortonormal a <span class="math inline">\({\textbf u}_1\)</span> <span class="math display">\[{\textbf u}_2=\frac{{\textbf w}_2}{||{\textbf w}_2||}=\left[ \frac{\sqrt{2}}{6},\frac{2\sqrt{2}}{3},0,\frac{-\sqrt{2}}{6}\right]\]</span><br>
</p>
<p>(2o vector ortonormal)<br>
</p>
<p>Ahora calculamos <span class="math inline">\({\textbf w}_3\)</span> en términos de <span class="math inline">\({\textbf u}_1\)</span> y <span class="math inline">\({\textbf u}_2\)</span> <span class="math display">\[{\textbf w}_3=  {\textbf v}_3- {\textbf u}_1\cdot{\textbf v}_3{\textbf u}_1 - {\textbf u}_2\cdot{\textbf v}_3{\textbf u}_2=
  \left[ \frac{4}{9},\frac{-2}{9},0,\frac{-4}{9}\right]\,,\]</span> (vector ortogonal a <span class="math inline">\(u_1\)</span> y <span class="math inline">\(u_2\)</span>) \ \ y si normalizamos<br>
</p>
<p><span class="math display">\[{\textbf u}_3=\left[ \frac{2}{3},\frac{-1}{3},0,\frac{-2}{3}\right]\,.\]</span> &nbsp; (3er vector ortonormal)<br>
</p>
<p>La matriz o conjunto de vectores ortonormal es</p>
<p><span class="math display">\[{\textbf A}=\left(\begin{array}{cccc}
  \frac{\sqrt{6}}{6} &amp; \frac{\sqrt{2}}{6} &amp; \frac{2}{3}\\
  0 &amp; \frac{2\sqrt{2}}{3} &amp; \frac{-1}{3}\\
  \frac{\sqrt{6}}{3} &amp; 0 &amp; 0\\
  \frac{\sqrt{6}}{6} &amp; \frac{-\sqrt{2}}{6} &amp; \frac{-2}{3}\\
\end{array}
  \right)
\,\]</span> (base ortonormal)</p>
<p>{} **Aplicación lineal*</p>
<p>Sean dos espacios vectoriales <span class="math inline">\(V\)</span> y <span class="math inline">\(W\)</span>, decimos que una aplicación <span class="math inline">\(f:V \rightarrow W\)</span> es lineal si la `imagen’ de la combincación lineal es la combinación lineal de las imágenes. Es decir,</p>
<p><span class="math display">\[f(\alpha {\textbf u} + \beta {\textbf v} )=\alpha f({\textbf u}) + \beta f({\textbf v})\,.\]</span></p>
<p>La imagen de una aplicación es el resultado de aplicar al vector una aplicación.</p>
<p>Ejemplos:</p>
<ol type="1">
<li>La aplicación <span class="math inline">\(f\)</span>: <span class="math inline">\({\cal R}^3 \rightarrow {\cal R}^2\)</span> definida por <span class="math inline">\(f(x,y,z)=(x+y,y+2z)\)</span> es lineal.</li>
</ol>
<p>Demostración: Definimos <span class="math inline">\({\textbf u}=[u_1,u_2,u_3]\)</span> y <span class="math inline">\({\textbf v}=[v_1,v_2,v_3]\)</span>. Entonces <span class="math inline">\(f\)</span> es lineal si <span class="math display">\[f(\alpha (u_1,u_2,u_3) + \beta(v_1,v_2,v_3))=\alpha f(u_1,u_2,u_3) + \beta f(v_1,v_2,v_3)=\]</span> <span class="math display">\[f(\alpha u_1 + \beta v_1,\alpha u_2 + \beta v_2,\alpha u_3 + \beta v_3 )=\alpha [u_1 + u_2,u_2 + 2u_3] +
\beta [v_1 + v_2,v_2 + 2v_3]\]</span> <span class="math display">\[[\alpha u_1 + \beta v_1 + \alpha u_2 + \beta v_2,\alpha u_2 + \beta v_2 + 2 (\alpha u_3 + \beta v_3)]=
[\alpha u_1 + \alpha u_2 + \beta v_1 + \beta v_2, \alpha u_2 + 2\alpha u_3 + \beta v_2 + 2\beta v_3]\]</span></p>
<ol start="2" type="1">
<li>La aplicación <span class="math inline">\(f\)</span>: <span class="math inline">\({\cal R}^3 \rightarrow {\cal R}^2\)</span> definida por <span class="math inline">\(f(x,y,z)=(x+y+1,y+2z)\)</span> no es lineal.</li>
</ol>
<ol start="3" type="1">
<li>La aplicación <span class="math inline">\({\textbf R}\)</span>: <span class="math inline">\({\cal R}^2 \rightarrow {\cal R}^2\)</span> definida por <span class="math display">\[{\textbf x}'={\textbf R}{\textbf x}\,,\]</span> <span class="math display">\[{\textbf R}=\left( \begin{array}{cc}
cos(\theta) &amp; -sin(\theta)\\
sin(\theta) &amp; cos(\theta)
   \end{array} \right)\,,\]</span> rota el vector un ángulo <span class="math inline">\(\theta\)</span> en sentido contrario a las agujas del reloj. Si queremos cambiar el sentido de la rotación solo debemos tomar el signo de <span class="math inline">\(\theta\)</span> negativo. Aqui <span class="math inline">\({\textbf x}'\)</span> es el vector rotado o la imagen de la aplicación rotación.</li>
</ol>
<p>{atriz Identidad}</p>
<p>Se define la matriz identidad <span class="math inline">\({\textbf I}\)</span> como una matriz diagonal compuesta por unos</p>
<p><span class="math display">\[{\textbf I}=\left(\begin{array}{ccc}
  1 &amp; 0 &amp; 0\\
  0 &amp; 1 &amp; 0\\
  0 &amp; 0 &amp; 1
\end{array}\right)\,.\]</span></p>
<p>El producto matricial de cualquier matriz <span class="math inline">\({\textbf A}\)</span> por la matriz identidad <span class="math inline">\({\textbf I}\)</span> es igual a la matriz original <span class="math display">\[{\textbf A}{\textbf I}={\textbf A}\,.\]</span></p>
<p>{atriz transpuesta}</p>
<p>Sea <span class="math inline">\({\textbf A}\)</span> una matriz <span class="math inline">\(m\times n\)</span> con elementos <span class="math inline">\([a_{ij}]\,\,{ para}\,\,i=1,2,...,m; j=1,2,...,n\)</span>. Se define el elemento de su transpuesta como <span class="math display">\[{\textbf A}^T=[a_{ji}]\,.\]</span></p>
<p>La inversa de la matriz transpuesta es <span class="math display">\[({\textbf A}^T)^{-1}={\textbf A}\,,\]</span> si <span class="math inline">\({\textbf A}\)</span> es ortogonal. \ {emostración}: <span class="math display">\[({\textbf A}^T)^{-1}={\textbf A}\]</span> <span class="math display">\[{\textbf A}^T({\textbf A}^T)^{-1}={\textbf A}^T{\textbf A}={\textbf I}\]</span> <span class="math display">\[{\textbf A}{\textbf A}^T({\textbf A}^T)^{-1}={\textbf A}{\textbf I}\]</span></p>
<p>{atriz Ortogonal}</p>
<p>Una matriz <span class="math inline">\({\textbf A}\)</span> es ortogonal si <span class="math inline">\({\textbf A}{\textbf A}^T={\textbf A}^T{\textbf A}={\textbf I}\,.\)</span></p>
<p>{atriz Diagonal}</p>
<p>Una matriz es diagonal si únicamente contiene algunos elementos diferentes de cero en la diagonal principal (ceros en la matriz triangular superior e inferior).</p>
<p><span class="math display">\[{\textbf D}=\left(\begin{array}{ccc}
  a_{11} &amp; 0 &amp; 0\\
  0 &amp; a_{22} &amp; 0\\
  0 &amp; 0 &amp; a_{33}
\end{array}\right)
\]</span></p>
<p>{atriz Simétrica}</p>
<p>Una matriz simétrica se define como aquella matriz <span class="math inline">\({\textbf A}\)</span> tal que sea igual a su traspuesta <span class="math display">\[{\textbf A}={\textbf A}^T\,\,(a_{ij}=a_{ji})\]</span></p>
<p><span class="math display">\[{\textbf A}=\left(\begin{array}{cccc}
  a_{11} &amp; a_{12} &amp; a_{31}\\
  a_{12} &amp; a_{22} &amp; a_{32}\\
  a_{31} &amp; a_{32} &amp; a_{33}\\
\end{array}\right)
\]</span></p>
<p>{atriz Antisimétrica}</p>
<p>Una matriz antisimétrica se define como aquella matriz <span class="math inline">\({\textbf A}\)</span> tal que sea igual a su traspuesta <span class="math display">\[{\textbf A}=-{\textbf A}^T\,\,(a_{ij}=-a_{ji})\]</span></p>
<p><span class="math display">\[{\textbf A}=\left(\begin{array}{cccc}
  0 &amp; a_{12} &amp; a_{13}\\
  -a_{12} &amp; 0 &amp; a_{23}\\
  -a_{13} &amp; -a_{23} &amp; 0\\
\end{array}\right)
\]</span> \ NOTA: Cualquier matriz <span class="math inline">\({\textbf A}\)</span> se puede descomponer en una parte simétrica y otra antisimétrica:</p>
<p><span class="math display">\[A=\frac{1}{2}({\textbf A}+{\textbf A}^T) + \frac{1}{2}({\textbf A}-{\textbf A}^T)\,.\]</span></p>
<p>{atriz Singular}</p>
<p>Una matriz singular es aquella matriz cuadrada cuyo determinante es igual a cero. Las matrices singulares no tienen matriz inversa. \ \ Ejemplo:</p>
<p><span class="math display">\[{\textbf S}=\left(\begin{array}{cc}
  3 &amp; 2\\
  6 &amp; 4
\end{array}\right)
\]</span></p>
<p>Si nos fijamos las 2 filas de la matriz singular <span class="math inline">\({\textbf S}\)</span> son linealmente dependientes, es decir, podemos recuperar la segunda fila multiplicando la primera fila por 2. Si la matriz tiene columnas o filas linealmente dependientes, el determinante es cero.</p>
<p>Si <span class="math inline">\({\textbf A}\)</span> es ortogonal</p>
<p><span class="math display">\[{\textbf A}^{-1}={\textbf A}^T\]</span> <span class="math display">\[{\textbf A}^{-1}({\textbf A}^T)^{-1}={\textbf I}\]</span> <span class="math display">\[{\textbf A}^{-1}\underbrace{({\textbf A}^T)^{-1}({\textbf A}^T)}_{{\textbf I}}={\textbf A}^T\]</span></p>
<p>Una matriz cualquiera se puede convertir en matriz cuadrada si es multiplicada por su transpuesta.</p>
<p>{eterminante de una matriz}</p>
<p>El determinante de una matriz <span class="math inline">\({\textbf A}\)</span> <span class="math inline">\(3\times 3\)</span> se puede calcular como <span class="math display">\[|{\textbf A}|=\left|\begin{array}{cccc}
  a_{11} &amp; a_{12} &amp; a_{13}\\
  a_{21} &amp; a_{22} &amp; a_{23}\\
  a_{31} &amp; a_{32} &amp; a_{33}\\
\end{array}
  \right|=a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32}-(a_{31}a_{22}a_{13} + a_{32}a_{23}a_{11} +
          a_{33}a_{21}a_{12})\,.\]</span></p>
<p>Para matrices de orden superior se puede utilizar la formula de adjuntos. El determinante de una matriz <span class="math inline">\(n\times n\)</span> es el producto escalar entre cualquier fila o columna con sus adjuntos <span class="math display">\[|{\textbf A}|=a_{i1}C_{i1} + a_{i2}C_{i2}+...+a_{in}C_{in}\,,\]</span> donde los adjuntos <span class="math inline">\(C_{ij}\)</span> son subdeterminantes (de orden <span class="math inline">\(n-1\)</span>, sin contar la columna j y fila i) con el signo adecuado <span class="math display">\[C_{ij}=(-1)^{i+j}|{\textbf M}_{ij}|\,.\]</span></p>
<p>{ango de una matriz} \ \ {efinición 1}: El rango de una matriz se define como el número de filas o columnas de la matriz que son linealmente independientes. \ \ {efinición 2}: El orden de la mayor submatriz cuadrada no nula</p>
<p>Así para calcular el rango debemos de hacer cero todos los elementos posibles de la matriz hasta obtener una submatriz no nula que nos indicará el rango de la matriz.</p>
<p><span class="math display">\[\left[\begin{array}{cccc}
  1 &amp; 2 &amp; 1\\
  -2 &amp; -3 &amp; 1\\
  3 &amp; 5 &amp; 0\\
\end{array}\right]\underbrace{\rightarrow}_{2r_1+r_2}
\left[\begin{array}{cccc}
1 &amp; 2 &amp; 1\\
  0 &amp; 1 &amp; 3\\
  3 &amp; 5 &amp; 0\\
\end{array}\right]\underbrace{\rightarrow}_{-3r_1+r_3}
\left[\begin{array}{cccc}
1 &amp; 2 &amp; 1\\
  0 &amp; 1 &amp; 3\\
  0 &amp; -1 &amp; -3\\
\end{array}\right]\underbrace{\rightarrow}_{r_2+r_3}
\left[\begin{array}{cccc}
1 &amp; 2 &amp; 1\\
  0 &amp; 1 &amp; 3\\
  0 &amp; 0 &amp; 0\\
\end{array}\right]\underbrace{\rightarrow}_{-2r_2+r_1}
\left[\begin{array}{cccc}
1 &amp; 0 &amp; -5\\
  0 &amp; 1 &amp; 3\\
  0 &amp; -1 &amp; -3\\
\end{array}\right]\,,\]</span> y entonces el rango de la matriz es 2. El rango lo podíamos haber encontrado simplimente viendo que sustrayendo la segunda fila de la matriz a la primera obtenemos la tercera fila; solamente hay 2 vectores linealmente independientes.</p>
<p>{atriz inversa}</p>
<p>La matriz cuadrada <span class="math inline">\({\textbf A}\)</span> es invertible si existe una matriz <span class="math inline">\({\textbf A}^{-1}\)</span> tal que <span class="math display">\[{\textbf A}^{-1}{\textbf A}={\textbf I}\,\,\,{ and}\,\,\,{\textbf A}{\textbf A}^{-1}={\textbf I}\,.\]</span></p>
<p>{roposición} Si <span class="math inline">\({\textbf A}\)</span> es invertible entonces la única solución de <span class="math inline">\({\textbf A}{\textbf x}={\textbf b}\)</span> es <span class="math display">\[{\textbf x}={\textbf A}^{-1}{\textbf b}\,.\]</span></p>
<p>{roposición} Si <span class="math inline">\({\textbf A}\)</span> es una matriz cuadrada de dimensión <span class="math inline">\(2\times 2\)</span> y <span class="math inline">\(|{\textbf A}|=a_{11}a_{22} - a_{12}a_{21}\)</span> no es cero, entonces <span class="math display">\[{\textbf A}^{-1}=\left(\begin{array}{cc}
  a_{11} &amp; a_{12} \\
  a_{21} &amp; a_{22} \\
\end{array}
  \right)^{-1} =
  \frac{1}{|{\textbf A}|}
  \left(\begin{array}{cc}
  a_{22} &amp; -a_{12} \\
  -a_{21} &amp; a_{11} \\
\end{array}
  \right)
\,.\]</span></p>
<p>{álculo de matriz inversa con método Gauss-Jordan}</p>
<p>El método de Gauss-Jordan usa la siguiente identidad matemática <span class="math inline">\([{\textbf A}|{\textbf I}]\rightarrow [{\textbf I}|{\textbf A}^{-1}]\)</span>. \ {jemplo:} Calcula la matriz inversa de <span class="math display">\[\left(\begin{array}{cc}
  2 &amp; 3 \\
  4 &amp; 7 \\
\end{array}
  \right)\]</span></p>
<p><span class="math display">\[[{\textbf A}  {\textbf I}]=
  \left(
        \begin{array}{ccccc}
  2 &amp; 3 &amp; &amp; 1 &amp; 0\\
  4 &amp; 7 &amp; &amp; 0 &amp; 1\\
        \end{array}
  \right)
  \underbrace{\rightarrow}_{-2r_1+r_2}
  \left(
        \begin{array}{ccccc}
  2 &amp; 3 &amp; &amp; 1 &amp; 0\\
  0 &amp; 1 &amp; &amp; -2 &amp; 1\\
        \end{array}
  \right)
  \underbrace{\rightarrow}_{-3r_2+r_1}
  \left(
        \begin{array}{ccccc}
  2 &amp; 0 &amp; &amp; 7 &amp; -3\\
  0 &amp; 1 &amp; &amp; -2 &amp; 1\\
        \end{array}
  \right)\rightarrow\]</span> <span class="math display">\[\underbrace{\rightarrow}_{r_1/2}
  \left(
        \begin{array}{ccccc}
  1 &amp; 0 &amp; &amp; 7/2 &amp; -3/2\\
  0 &amp; 1 &amp; &amp; -2 &amp; 1\\
        \end{array}
  \right)
  \,.
  \]</span></p>
<p>Entonces</p>
<p><span class="math display">\[{\textbf A}^{-1}=\left(
        \begin{array}{ccccc}
  7/2 &amp; -3/2\\
  -2 &amp; 1\\
        \end{array}
  \right)\,.\]</span></p>
<p> <span class="math display">\[{\textbf A}=\left[\begin{array}{cccc}
  1 &amp; -1 &amp; 2\\
  2 &amp; 0 &amp; 3\\
  0 &amp; 1 &amp; -1\\
\end{array}\right]\rightarrow
{\textbf A}^{-1}=\left[\begin{array}{cccc}
  3 &amp; -1 &amp; 3\\
  -2 &amp; 1 &amp; -1\\
  -2 &amp; 1 &amp; -2\\
\end{array}\right]\,.\]</span></p>
<p>{actorización LU}</p>
<p>Es la descomposición de una matriz en una matriz triangular inferior <span class="math inline">\({\textbf L}\)</span> y una matriz triangular superior <span class="math inline">\({\textbf U}\)</span>, es decir <span class="math display">\[{\textbf A}={\textbf L}{\textbf U}\,,\]</span> donde <span class="math display">\[{\textbf L}=\left(\begin{array}{cccc}
  a_{12} &amp; 0\\
  a_{21} &amp; a_{22}\\
\end{array}\right)\,\]</span> es una matriz triangular inferior (L; lower) y <span class="math display">\[
{\textbf U}=\left(\begin{array}{cccc}
  a_{12} &amp; a_{21}\\
  0 &amp; a_{12}\\
\end{array}\right)\,,\]</span> es una matriz triangular superior (U; superior).\ Esta factorización se suele usar para resolver un sistema de ecuaciones lineal <span class="math inline">\({\textbf A}{\textbf x}={\textbf b}\)</span> y no es única. Es decir, pueden existir más de una factorización. Si sustituimos la definición de arriba <span class="math display">\[{\textbf A}{\textbf x}=({\textbf L}{\textbf U}){\textbf x}={\textbf b}\,,\]</span> lo que implica que <span class="math display">\[{\textbf L}({\textbf U}{\textbf x})={\textbf b}\,.\]</span> \ Si definimos <span class="math inline">\({\textbf U}{\textbf x}={\textbf z}\)</span>, entonces tenemos que <span class="math display">\[{\textbf L}{\textbf z}={\textbf b}\,.\]</span> Como <span class="math inline">\({\textbf L}\)</span> es una matriz triangular inferior, podemos resolver para <span class="math inline">\({\textbf z}\)</span> utilizando sustitución hacia delante. Luego, como <span class="math inline">\({\textbf U}\)</span> es una matriz triangular superior, resolvemos <span class="math inline">\({\textbf U}{\textbf x}={\textbf z}\)</span> por sustitución en reversa.</p>
<p>{jemplo:}</p>
<p>Encuentre la descomposición LU de la matriz <span class="math display">\[\left(\begin{array}{ccccc}
  2 &amp; 5\\
-3 &amp; -4\\
        \end{array}\right)\,.
\]</span></p>
<p>Si multiplicamos la primera fila por <span class="math inline">\(L_{21}=3/2\)</span> y le sumamos la segunda fila hacemos cero el elemento <span class="math inline">\(a_{21}=-3\)</span> <span class="math display">\[{\textbf U}=\left(\begin{array}{ccccc}
  2 &amp; 5\\
0 &amp; 7/2\\
        \end{array}\right)\,.
\]</span> Esta matriz ya es una matriz triangular superior, es decir, la matriz <span class="math inline">\({\textbf U}\)</span>. Para encontrar la matriz triangular inferior solo debemos de conocer el valor del elemento <span class="math inline">\(L_{21}\)</span>. Ese elemento es el multiplicador con signo opuesto usado en la eliminación de Gauss-Jordan. Es decir, <span class="math inline">\(L_{21}=-3/2\)</span>. La matriz <span class="math inline">\({\textbf L}\)</span> es <span class="math display">\[{\textbf L}=\left(\begin{array}{ccccc}
  1 &amp; 0\\
-3/2 &amp; 1\\
        \end{array}\right)\,.\]</span></p>
<p>Vamos a comprobar <span class="math display">\[\left(\begin{array}{ccccc}
  1 &amp; 0\\
-3/2 &amp; 1\\
        \end{array}\right)
    \left(\begin{array}{ccccc}
  2 &amp; 5\\
0 &amp; 7/2\\
        \end{array}\right)
    =
    \left(\begin{array}{ccccc}
  2 &amp; 5\\
-3 &amp; -4\\
        \end{array}\right)\,.\]</span></p>
<p>{jemplo:}</p>
<p>Resuelva el siguiente sistema de ecuaciones con la descomposición LU <span class="math display">\[2x_1+3x_2+4x_3=6\]</span> <span class="math display">\[4x_1+5x_2+10x_3=16\]</span> <span class="math display">\[4x_1+8x_2+2x_3=2\]</span></p>
<p>La matriz de coeficientes es <span class="math display">\[{\textbf A}=\left(\begin{array}{ccccc}
  2 &amp; 3 &amp; 4\\
  4 &amp; 5 &amp; 10\\
  4 &amp; 8 &amp; 2 \\
        \end{array}\right)\,.
\]</span></p>
<p>Si factorizamos <span class="math display">\[{\textbf L}=\left(\begin{array}{ccccc}
  1 &amp; 0 &amp; 0\\
  2 &amp; 1 &amp; 0\\
  2 &amp; -2 &amp; 1 \\
        \end{array}\right)\,,
\]</span></p>
<p>y</p>
<p><span class="math display">\[{\textbf U}=\left(\begin{array}{ccccc}
  2 &amp; 3 &amp; 4\\
  0 &amp; -1 &amp; 2\\
  0 &amp; 0 &amp; 0 \\
        \end{array}\right)\,.\]</span></p>
<p>Si utilizamos la identidad <span class="math inline">\({\textbf L}{\textbf z}={\textbf b}\)</span> obtenemos <span class="math display">\[\left(\begin{array}{ccccc}
  1 &amp; 0 &amp; 0\\
  2 &amp; 1 &amp; 0\\
  2 &amp; -2 &amp; 1 \\
        \end{array}\right)
    \left(\begin{array}{c}
  z_1 \\
  z_2 \\
  z_3 \\
        \end{array}\right)=
    \left(\begin{array}{c}
  6 \\
  16 \\
  2 \\
        \end{array}\right)\,.\]</span></p>
<p>Si resolvemos para <span class="math inline">\({\textbf z}\)</span> <span class="math display">\[z_1=6\]</span> <span class="math display">\[z_2=16-2z_1=4\]</span> <span class="math display">\[z_3=2+2z_2-2z_1=-2\]</span></p>
<p>Así que <span class="math display">\[{\textbf z}=\left(\begin{array}{c}
  6 \\
  4 \\
  -2 \\
        \end{array}\right)\,.\]</span></p>
<p>Si utilizamos ahora la definición <span class="math inline">\({\textbf U}{\textbf x}={\textbf z}\,,\)</span> <span class="math display">\[\left(\begin{array}{ccccc}
  2 &amp; 3 &amp; 4\\
  0 &amp; -1 &amp; 2\\
  0 &amp; 0 &amp; 0 \\
        \end{array}\right)
    \left(\begin{array}{c}
  x_1 \\
  x_2 \\
  x_3 \\
        \end{array}\right)=
    \left(\begin{array}{c}
  6 \\
  4 \\
  -2 \\
        \end{array}\right)\,,\]</span> y obtenemos <span class="math display">\[x_3=1\]</span> <span class="math display">\[x_2=\frac{4-2x_3}{-1}=-2\]</span> <span class="math display">\[x_1=\frac{6-4x_3-3x_2}{2}=4\]</span></p>
<p>Por lo tanto la solución del sistema de ecuaciones es <span class="math display">\[{\textbf x}=\left(\begin{array}{c}
  4 \\
  -2\\
  1 \\
        \end{array}\right)\,.\]</span></p>
<p>{alores propios y vectores propios}</p>
<p>Sea <span class="math inline">\({\textbf A}\)</span> una matriz cuadrada, un número real <span class="math inline">\(\lambda\)</span> se dice que es un valor propio de <span class="math inline">\({\textbf A}\)</span> si existe un vector, diferente del vector cero, <span class="math inline">\({\textbf x}\)</span> tal que <span class="math display">\[{\textbf A}{\textbf x}=\lambda{\textbf x}\,.\]</span></p>
<p>Es decir, <span class="math inline">\({\textbf x}\)</span> es un vector que al transformarlo mediante la multiplicación por <span class="math inline">\({\textbf A}\)</span> el vector resultante mantiene la misma dirección; solamente se modifica su longitud (magnitud) y/o sentido. El valor propio <span class="math inline">\(\lambda\)</span> nos informa si el vector propio <span class="math inline">\({\textbf x}\)</span> se acorta o alarga o cambia de signo cuando es multiplicado por <span class="math inline">\({\textbf A}\)</span>.</p>
<p>{efinición} El número <span class="math inline">\(\lambda\)</span> es un valor propio si y solo si <span class="math display">\[|{\textbf A}-\lambda{\textbf I}|=0\,.\]</span></p>
<p>{jemplo:}</p>
<p>Calcula los valores propios y vectores propios de la matriz <span class="math display">\[{\textbf A}=\left(\begin{array}{ccccc}
  1 &amp; 2\\
  2 &amp; 4\\
        \end{array}\right)\,.\]</span></p>
<p>Sabemos <span class="math display">\[|{\textbf A}-\lambda{\textbf I}|=\left|\begin{array}{ccccc}
  1-\lambda &amp; 2\\
  2 &amp; 4-\lambda\\
        \end{array}\right|=\lambda^2-5\lambda=0\,.\]</span></p>
<p>El polinomio de arriba se llama polinomio característico y es igual a cero cuando <span class="math inline">\(\lambda\)</span> es un valor propio. Resolviendo obtenemos dos soluciones <span class="math inline">\(\lambda=0\)</span> y <span class="math inline">\(\lambda=5\)</span>. Ahora para encontrar los vectores propios debemos resolver el sistema <span class="math inline">\(({\textbf A}-\lambda{\textbf I}){\textbf x}=0\)</span> separadamente para las dos <span class="math inline">\(\lambda\)</span>: <span class="math display">\[({\textbf A}-0{\textbf I}){\textbf x}=
\left(\begin{array}{ccccc}
  1 &amp; 2\\
  2 &amp; 4\\
      \end{array}\right)
      \left(\begin{array}{c}
  x_1 \\
  x_2 \\
      \end{array}\right)=
      \left(\begin{array}{c}
  0 \\
  0 \\
      \end{array}\right)\,\,\,\rightarrow\,\,\,{\textbf x}=\left(\begin{array}{c}
  2 \\
  -1 \\
      \end{array}\right)\,\,\,{ para}\,\,\,\lambda_1=0\,,\]</span></p>
<p>y</p>
<p><span class="math display">\[({\textbf A}-5{\textbf I}){\textbf x}=
\left(\begin{array}{ccccc}
  -4 &amp; 2\\
  2 &amp; -1\\
      \end{array}\right)
      \left(\begin{array}{c}
  x_1 \\
  x_2 \\
      \end{array}\right)=
      \left(\begin{array}{c}
  0 \\
  0 \\
      \end{array}\right)\,\,\,\rightarrow\,\,\,{\textbf x}=\left(\begin{array}{c}
  1 \\
  2 \\
      \end{array}\right)\,\,\,{ para}\,\,\,\lambda_2=5\,.\]</span></p>
<p> Calcule los valores y vectores propios de la matriz de rotación</p>
<p><span class="math display">\[{\textbf R}=
\left(\begin{array}{ccccc}
  cos\theta &amp; -sen\theta\\
  sen\theta &amp; cos\theta\\
      \end{array}\right)\,.\]</span></p>
<p>{ultiplicación de matrices}</p>
<p>El producto matricial de dos matrices <span class="math inline">\({\textbf A}\)</span> (<span class="math inline">\(m\times n\)</span>) y <span class="math inline">\({\textbf B}\)</span> (<span class="math inline">\(n\times p\)</span>) se define como <span class="math display">\[{\textbf C}={\textbf A}{\textbf B}\,,\]</span> donde <span class="math inline">\({\textbf C}\)</span> es una matriz <span class="math inline">\(m\times p\)</span>, con el elementi <span class="math inline">\((i,j)\)</span> definido por <span class="math display">\[c_{ij}=\sum^n_{k=1} a_{ik} b_{kj}\,,\]</span> para todo <span class="math inline">\(i=1,2,...,m; j=1,2,...,p\)</span>.</p>
<p>{ultiplicación de matrices}</p>
<p>Sea <span class="math inline">\({\textbf A}\)</span> una matriz <span class="math inline">\(m\times n\)</span>, y <span class="math inline">\({\textbf v}\)</span> un vector <span class="math inline">\(n\times 1\)</span>, entonces el elemento del producto <span class="math display">\[{\textbf z}={\textbf A}{\textbf v}\]</span> viene dado por <span class="math display">\[z_{i}=\sum^n_{k=1} a_{ik} v_{k}\,,\]</span> para todo <span class="math inline">\(i=1,2,....,m\)</span>. Similarmente, si <span class="math inline">\({\textbf u}\)</span> es un vector <span class="math inline">\(m\times 1\)</span>, entonces el elemento del producto <span class="math display">\[{\textbf z}^T={\textbf u}^T{\textbf A}\]</span> viene dado por <span class="math display">\[z_{i}=\sum^n_{k=1} a_{ki} u_{k}\,,\]</span> para todo <span class="math inline">\(i=1,2,....,n\)</span>. Finalmente, el escalar que resulta del producto <span class="math inline">\(\alpha={\textbf u}^T{\textbf A}{\textbf v}\,,\)</span> viene dado por <span class="math display">\[\alpha=\sum^m_{j=1}\sum^n_{k=1}a_{jk} u_j v_k\,.\]</span></p>
<p>{roposición:}</p>
<p>Sea <span class="math inline">\({\textbf C}={\textbf A}{\textbf B}\)</span> una matriz <span class="math inline">\(m\times p\)</span>, entonces <span class="math display">\[{\textbf C}^T={\textbf B}^T {\textbf A}^T\,.\]</span></p>
<p>{roposición} Sea <span class="math inline">\({\textbf A}\)</span> y <span class="math inline">\({\textbf B}\)</span> matrices cuadradas <span class="math inline">\(n\times n\)</span> invertibles. Sea el producto matricial <span class="math inline">\({\textbf C}={\textbf A}{\textbf B}\)</span>, entonces <span class="math display">\[{\textbf C}^{-1}={\textbf B}^{-1} {\textbf A}^{-1}\,.\]</span></p>
<p>{erivada de matrices}</p>
<p>{roposición:}</p>
<p>Sea <span class="math display">\[{\textbf y}={\textbf A}{\textbf x}\,,\]</span> donde <span class="math inline">\({\textbf y}\)</span> es <span class="math inline">\(m\times 1\)</span>, <span class="math inline">\({\textbf x}\)</span> es <span class="math inline">\(n\times 1\)</span>, <span class="math inline">\({\textbf A}\)</span> es <span class="math inline">\(m\times n\)</span>, y <span class="math inline">\({\textbf A}\)</span> no depende de <span class="math inline">\({\textbf x}\)</span>, entonces la derivada de <span class="math inline">\({\textbf y}\)</span> es <span class="math display">\[\frac{\partial{\textbf y}}{\partial{\textbf x}}={\textbf A}\,.\]</span></p>
<p>{roposición:}</p>
<p>Sea <span class="math display">\[{\textbf y}={\textbf A}{\textbf x}\,,\]</span> donde <span class="math inline">\({\textbf y}\)</span> es <span class="math inline">\(m\times 1\)</span>, <span class="math inline">\({\textbf x}\)</span> es <span class="math inline">\(n\times 1\)</span>, <span class="math inline">\({\textbf A}\)</span> es <span class="math inline">\(m\times n\)</span>, y <span class="math inline">\({\textbf A}\)</span> no depende de <span class="math inline">\({\textbf x}\)</span>. Supongamos que <span class="math inline">\({\textbf x}\)</span> es una función del vector <span class="math inline">\({\textbf z}\)</span>, mientras que <span class="math inline">\({\textbf A}\)</span> es independiente de <span class="math inline">\({\textbf z}\)</span>. Entonces <span class="math display">\[\frac{\partial{\textbf y}}{\partial{\textbf z}}={\textbf A}\frac{\partial{\textbf x}}{\partial{\textbf z}}\,.\]</span></p>
<p>{roposición:}</p>
<p>Sea el escalar <span class="math inline">\(\alpha\)</span> definido como <span class="math display">\[\alpha={\textbf x}^T{\textbf A}{\textbf x}\,,\]</span> donde <span class="math inline">\({\textbf x}\)</span> es <span class="math inline">\(n\times 1\)</span>, <span class="math inline">\({\textbf A}\)</span> es <span class="math inline">\(n\times n\)</span>, y <span class="math inline">\({\textbf A}\)</span> es independiente de <span class="math inline">\({\textbf x}\)</span>, entonces <span class="math display">\[\frac{\partial{\alpha}}{\partial{{\textbf x}}}={\textbf x}^T({\textbf A}+{\textbf A}^T)={\textbf x}^T{\textbf A}^T + {\textbf x}^T{\textbf A}={\textbf x}({\textbf A}^T+{\textbf A})\]</span></p>
<p>{roposición:}</p>
<p>Sea el escalar <span class="math inline">\(\alpha\)</span> definido como <span class="math display">\[\alpha={\textbf y}^T{\textbf A}{\textbf x}\,,\]</span> donde <span class="math inline">\({\textbf y}\)</span> es <span class="math inline">\(m\times 1\)</span>, <span class="math inline">\({\textbf x}\)</span> es <span class="math inline">\(n\times 1\)</span>, y <span class="math inline">\({\textbf A}\)</span> es <span class="math inline">\(m\times n\)</span>, y <span class="math inline">\({\textbf A}\)</span> es independiente de <span class="math inline">\({\textbf x}\)</span> y <span class="math inline">\({\textbf y}\)</span>, entonces <span class="math display">\[\frac{\partial{\alpha}}{\partial{{\textbf x}}}={\textbf y}^T{\textbf A}\]</span> y <span class="math display">\[\frac{\partial{\alpha}}{\partial{{\textbf y}}}={\textbf x}^T{\textbf A}\,.\]</span></p>
<p>{efinición} Sea <span class="math inline">\({\textbf A}\)</span> una matriz <span class="math inline">\(m\times n\)</span> cuyos elementos son funciones de un escalar <span class="math inline">\(\alpha\)</span>. Entonces la derivada de <span class="math inline">\({\textbf A}\)</span> con respecto <span class="math inline">\(\alpha\)</span> es una matriz <span class="math inline">\(m\times n\)</span> compuesta por las derivadas de elemento por elemento: <span class="math display">\[\frac{\partial{\textbf A}}{\partial{\alpha}}=\left(\begin{array}{cccc}
  \frac{\partial{a_{11}}}{\partial{\alpha}} &amp; \frac{\partial{a_{12}}}{\partial{\alpha}} &amp; .... &amp;
   \frac{\partial{a_{1n}}}{\partial{\alpha}}\\
  \frac{\partial{a_{21}}}{\partial{\alpha}} &amp; \frac{\partial{a_{22}}}{\partial{\alpha}} &amp; .... &amp;
   \frac{\partial{a_{2n}}}{\partial{\alpha}}\\
    .    &amp;   .    &amp;      &amp;   . \\
    .    &amp;   .    &amp;      &amp;   . \\
    .    &amp;   .    &amp;      &amp;   . \\
  \frac{\partial{a_{m1}}}{\partial{\alpha}} &amp; \frac{\partial{a_{m2}}}{\partial{\alpha}} &amp; .... &amp;
   \frac{\partial{a_{mn}}}{\partial{\alpha}}\\
\end{array}
  \right)
\,.\]</span></p>
</section>
<section id="cuadrados-mínimos-y-regresión" class="level2">
<h2 class="anchored" data-anchor-id="cuadrados-mínimos-y-regresión">Cuadrados mínimos y regresión</h2>
<p>En esta sección se van a introducir algunos modelos lineales estadísticos o modelos de regresión. Aqui se incluyen ajustes lineales por cuadrados mínimos, coeficientes de correlación, regresión múltiple, etc.</p>
<section id="métodos-de-cuadrados-mínimos" class="level3">
<h3 class="anchored" data-anchor-id="métodos-de-cuadrados-mínimos">Métodos de cuadrados mínimos</h3>
<p>Estos métodos son utilizados para ajustar un modelo dependiente de un conjunto compuesto por <span class="math inline">\(k\)</span> variables independientes <span class="math inline">\(x_k;i=1,2,...,k\)</span>.</p>
<p>{A) Mínimos cuadrados lineales} \ Empezamos aplicando el método en términos de estimación lineal. Nos referimos a lineal en cuanto a los coeficientes <span class="math inline">\(b_0, b_1,...,b_k\)</span>, es decir, <span class="math inline">\(y=b_0 + b_1x_1 + \epsilon\)</span> es lineal, pero <span class="math inline">\(y=b_0+sin(b_1x_1)\)</span> no lo es. \</p>
<ol type="1">
<li>{juste de una recta a un conjunto de datos} \ Queremos usar los mejores coeficientes <span class="math inline">\(b_0\)</span> y <span class="math inline">\(b_1\)</span> en el sentido que se reduzca la desviación estándar de la recta ajustada versus los datos. Sea <span class="math inline">\(i=1,2,...,N\)</span> observaciones, entonces <span class="math display">\[y_i=\hat{y_i} + \epsilon\,,\]</span> donde <span class="math display">\[\hat{y_i}=b_0+b_1 x_i\]</span> es el estimador estadístico y <span class="math inline">\(\epsilon\)</span> es el residuo (medida de la diferencia de la recta ajustada versus conjunto de puntos). Para encontrar <span class="math inline">\(b_0, \, b_1\)</span> debemos de minimizar la suma de los errores cuadrados (SEC), donde SEC es la varianza total que no es explicada por nuestro modelo de regresión lineal <span class="math display">\[SEC=\sum^N_{i=1} \epsilon^2_i=\sum^N_{i=1} (y_i - \hat{y_i})^2=\sum^N_{i=1} [y_i - (b_0+b_1 x_i)]^2\,.\]</span> \</li>
</ol>
<p>La suma de los cuadrados totales se define como <span class="math display">\[SCT=\sum^N_{i=1} (y_i - \bar{y})^2\,,\]</span> y la suma de los cuadrados residuales <span class="math display">\[SCR=\sum^N_{i=1} (\hat{y_i} - \bar{y})^2\,.\]</span> \ La SCT es proporcional a la varianza de los datos y SCR es proporcional a la cantidad de varianza explicada por nuestra regresión. La varianza total (o SCT) se puede descomponer en función de SCR y SEC como: <span class="math display">\[SCT=\sum^N_{i=1}(y_i-\bar{y})^2 = \sum^N_{i=1}(\hat{y_i} + \epsilon_i - \bar{y})^2
=\sum^N_{i=1}((\hat{y_i}- \bar{y}) + \epsilon_i)^2=\sum^N_{i=1}(\hat{y_i}- \bar{y})^2 +\]</span> <span class="math display">\[+\sum^N_{i=1}\epsilon_i^2 + 2\sum^N_{i=1}(\hat{y_i}- \bar{y})\epsilon_i\,,\]</span> y por lo tanto <span class="math display">\[SCT=SCR + SEC\,\]</span> \ ya que por las ecuaciones normales <span class="math inline">\(2\sum^N_{i=1}(\hat{y_i}- \bar{y})\epsilon_i=0\)</span>.<br>
</p>
<p>El problema de mínimos cuadrados consiste en minimizar la SEC con respecto los coeficientes, cuyas condiciones son <span class="math display">\[\frac{\partial{SEC}}{\partial{b_0}}=0; \frac{\partial{SEC}}{\partial{b_1}}=0\,.\]</span><br>
</p>
<p>Substituyendo obtenemos para <span class="math inline">\(b_0\)</span> <span class="math display">\[\frac{\partial{SEC}}{\partial{b_0}} =\frac{\partial}{\partial{b_0}}
  \left\{
    \sum^N_{i=1} [y_i - (b_0+b_1 x_i)]^2
  \right\}=
  -2\sum^N_{i=1} [y_i - (b_0+b_1 x_i)]=\]</span> <span class="math display">\[=-2\left(\sum^N_{i=1}y_i -N b_0 - b_1\sum^N_{i=1} x_i  \right)=0\,,\]</span> y para <span class="math inline">\(b_1\)</span> <span class="math display">\[\frac{\partial{SEC}}{\partial{b_1}} =\frac{\partial{}}{\partial{b_1}}
  \left\{
    \sum^N_{i=1} [y_i - (b_0+b_1 x_i)]^2
  \right\}=
  -2\sum^N_{i=1} x_i [y_i - (b_0+b_1 x_i)]=\]</span> <span class="math display">\[=-2\left(\sum^N_{i=1}x_i y_i - b_0\sum^N_{i=1}x_i - b_1\sum^N_{i=1} x^2_i \right)=0\,.\]</span><br>
</p>
<p>Si resolvemos para <span class="math inline">\(b_0\)</span></p>
<p><span class="math display">\[b_0=\bar{y}-b_1\bar{x}\,,\]</span></p>
<p>y sustituimos en la segunda ecuación, obtenemos:</p>
<p><span class="math display">\[b_1=\frac{N \sum\limits^N_{i=1} x_i y_i - \sum\limits^N_{i=1}x_i \sum\limits^N_{i=1}y_i}
           {N\sum\limits^N_{i=1}x^2_i-\left(\sum\limits^N_{i=1}x_i \right)^2}=
      \frac{\sum\limits^N_{i=1} x_i y_i - \frac{1}{N}\sum\limits^N_{i=1}x_i \sum\limits^N_{i=1}y_i}
           {\sum\limits^N_{i=1}x^2_i-\frac{1}{N}\left(\sum\limits^N_{i=1}x_i \right)^2}=\]</span> <span class="math display">\[=\frac{(N-1)C_{xy}}{(N-1)s^2_x}=\frac{\sum\limits^N_{i=1} (x_i-\bar{x})(y_i-\bar{y})}
                                      {\sum\limits^N_{i=1} (x_i-\bar{x})^2}=\frac{&lt;x'y'&gt;}{&lt;x'^2&gt;}\,,\]</span> donde <span class="math inline">\(C_{xy}=&lt;x'y'&gt;\)</span> es la covarianza entre <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span>, y <span class="math inline">\(s^2_x=&lt;x'^2&gt;\)</span> es la varianza de <span class="math inline">\(x\)</span>. Así, una vez hemos calculado las medias de las variables <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span>, podemos encontrar el coeficiente <span class="math inline">\(b_1\)</span> y, a partir de este, calcular el segundo coeficiente <span class="math inline">\(b_0\)</span>.<br>
<br>
Si sustituimos <span class="math inline">\(b_0=\bar{y}-b_1\bar{x}\)</span> en la ecuación de la regresión lineal <span class="math inline">\(\hat{y_i}=b_0+b_1 x_i\)</span></p>
<p>obtenemos</p>
<p><span class="math display">\[\hat{y_i}=\bar{y} +b_1(x_i-\bar{x})\,,\]</span> o</p>
<p><span class="math display">\[\hat{y_i}=\bar{y} +b_1{x'_i}\,,\]</span> o finalmente</p>
<p><span class="math display">\[\hat{y'_i}=b_1{x'_i}\,,\]</span></p>
<p>que nos informa que cuando <span class="math inline">\(x'_i=0\,(x_i=\bar{x})\)</span> entonces <span class="math inline">\(\hat{y'_i}=0 \,(\hat{y}=\bar{y})\)</span>, es decir, la recta pasa por el punto <span class="math inline">\((\bar{x},\bar{y})\)</span>, de tal forma, que puesto que <span class="math inline">\(\partial{SEC}/{\partial{b_0}}=0\)</span> minimiza la suma del error, <span class="math inline">\(\sum \epsilon_i=0\)</span>, los puntos estan dispersos respecto a la recta ajustada de tal forma que los residuos positivos (<span class="math inline">\(\epsilon&gt;0\)</span>) siempre se cancelan con los residuos negativos (<span class="math inline">\(\epsilon&lt;0\)</span>). El parámetro <span class="math inline">\(b_0\)</span> se interpreta como la intersección (corte con el eje <span class="math inline">\(y\)</span>) y <span class="math inline">\(b_1\)</span> es la pendiente de la recta ajustada.<br>
<br>
El cociente</p>
<p><span class="math display">\[100(SCR/SCT)\,,\]</span></p>
<p>es el porcentaje de varianza explicada por nuestra regresión lineal (varianza explicada/varianza total) y nos informa de la bondad del ajuste denominado <em>coeficiente de correlación</em>, <span class="math inline">\(r^2\)</span>. Si la regresión se ajusta perfectamente a todos los datos, todos los residuos son cero y por lo tanto <span class="math inline">\(SEC=0\)</span> y <span class="math inline">\(SCR/SCT=r^2=1\)</span>. A medida que el ajuste empeora el coeficiente <span class="math inline">\(r^2\)</span> disminuye hasta un mínimo posible de <span class="math inline">\(r^2=0\)</span>.</p>
<p>{rror estándar de la estimación}</p>
<p>Una medida de la magnitud absoluta de la bondad del ajuste es el error estándar del estimado, <span class="math inline">\(s_{\epsilon}\)</span>, definido como</p>
<p><span class="math display">\[s_{\epsilon}=[SEC/(N-2)]^{1/2}=\left[ \frac{1}{N-2}\sum\limits^N_{i=1} (y-\hat{y})^2\right]^{1/2}\,.\]</span></p>
<p>El número de grados de libertad, <span class="math inline">\(N-2\)</span>, se debe a que necesitamos estimar dos parámetros para encontrar realizar la regresión lineal. Si <span class="math inline">\({\epsilon}\)</span> es una variable aleatoria que sigue una distribución Normal de media cero y desviación estándar <span class="math inline">\(s_{\epsilon}\)</span>, entonces, el <span class="math inline">\(68.3\%\)</span> de las observaciones caen dentro del intervalo <span class="math inline">\(\pm 1s_{\epsilon}\)</span> unidades de la recta ajustada, <span class="math inline">\(95.4\%\)</span> caerá dentro del intervalo <span class="math inline">\(\pm 2 s_{\epsilon}\)</span> unidades de la recta, y <span class="math inline">\(99.7\%\)</span> caerán en el intervalo <span class="math inline">\(\pm 3 s_{\epsilon}\)</span> unidades de la recta. <span class="math inline">\(s_{\epsilon}\)</span> es la desviación estándar de <span class="math inline">\(y\)</span> alrededor de su media, i.e., la recta ajustada <span class="math inline">\(b_0 + b_1 x\)</span>.</p>
<p><strong>Generalización de mínimos cuadrados en notación matricial</strong></p>
<p>Supongamos el modelo dependiente de <span class="math inline">\(k\)</span> variables independientes <span class="math inline">\(X_k\)</span> <span class="math display">\[Y=b_0 + b_1 X_1 + b_2 X_2 + .... + b_k X_k + \epsilon\,,\]</span> y supongamos que hacemos N observaciones independientes <span class="math inline">\(y_1, y_2,....,y_N\)</span> de <span class="math inline">\(Y\)</span>. Por lo tanto podemos escribir el modelo como <span class="math display">\[y_i=b_0 + b_1 x_{i1} + b_2 x_{i2} + ... + b_k x_{ik} + \epsilon_i\,,\]</span> donde <span class="math inline">\(x_{ij}\)</span> es la observación <span class="math inline">\(i\)</span> del de la variable independiente <span class="math inline">\(j\)</span>. Es decir, <span class="math display">\[N;\,\,{ observaciones}\]</span> <span class="math display">\[k;\,\,{ variables\,\,independientes}\]</span> <span class="math display">\[k+1;\,\,{ coeficientes}\]</span></p>
<p>Si escribimos en notación matricial <span class="math display">\[{\textbf Y}=\left(\begin{array}{c}
  y_1\\
  y_2\\
  ...\\
  ...\\
  ...\\
  y_N
      \end{array}\right)\,\,\,{\textbf X}=\left(\begin{array}{cccc}
  1 &amp; x_{11} &amp; ... &amp; x_{1k}\\
  1 &amp; x_{21} &amp; ... &amp; x_{2k}\\
  ... &amp; ... &amp; ... &amp; ...\\
  ... &amp; ... &amp; ... &amp; ...\\
  1 &amp; x_{N1} &amp; ... &amp; x_{Nk}\\
      \end{array}\right)\]</span> <span class="math display">\[{\textbf B}=\left(\begin{array}{c}
  b_0\\
  b_1\\
  ...\\
  ...\\
  ...\\
  b_k
      \end{array}\right)\,\,\,{\textbf E}=\left(\begin{array}{c}
  \epsilon_1\\
  \epsilon_2\\
  ...\\
  ...\\
  ...\\
  \epsilon_N
      \end{array}\right)\,.\]</span> \ \ De esta forma, podemos escribir nuestro modelo en notación matricial como <span class="math display">\[{\textbf Y}={\textbf X}{\textbf B} + {\textbf E}\,.\]</span> \ \ Si restringimos el modelo a una variable independiente (<span class="math inline">\(k=1\)</span>), i.e.&nbsp;dos coeficientes <span class="math inline">\((b_0, b_1)\)</span>, entonces <span class="math display">\[{\textbf B}=\left(\begin{array}{c}
  b_0\\
  b_1\\
      \end{array}\right)\,,\]</span> es la matriz de coeficientes, e <span class="math display">\[{\textbf X}=\left(\begin{array}{cc}
  1&amp; x_{11}\\
  1&amp; x_{21}\\
  ... &amp; ...\\
  ... &amp; ...\\
   ... &amp; ...\\
1 &amp; x_{N1}
\end{array}\right)\,,\]</span> es la matriz de variables independientes. Si utilizamos la definición de los residuos como <span class="math display">\[{\textbf E}={\textbf Y}-{\textbf X}{\textbf B}\,,\]</span> podemos encontrar la suma de los residuos al cuadrado como <span class="math display">\[SEC=\sum^N_{i=1} \epsilon^2_i=\sum^N_{i=1} \epsilon_i \epsilon_i=
{\textbf E}^T{\textbf E}=({\textbf Y}-{\textbf X}{\textbf B})^T({\textbf Y}-{\textbf X}{\textbf B})=\]</span> <span class="math display">\[{\textbf Y}^T{\textbf Y} - {\textbf Y}^T{\textbf X}{\textbf B}-{\textbf B}^T{\textbf X}^T{\textbf Y}+{\textbf B}^T
{\textbf X}^T{\textbf X}{\textbf B}\,.\]</span></p>
<p>Si queremos minimizar esa suma de errores entonces <span class="math display">\[\frac{\partial{SEC}}{\partial{\textbf B}}=0\,.\]</span> Para calcular las derivadas de <span class="math inline">\({SEC}\)</span> respecto de los coeficientes <span class="math inline">\({\textbf b}\)</span> vamos a asumir las siguientes consideraraciones \ \ (i) <span class="math inline">\({\textbf Y}^T{\textbf X}{\textbf B}={\textbf B}^T{\textbf X}^T{\textbf Y}\)</span> ya que son matrices elemento (<span class="math inline">\(1\times1\)</span>) y siempre son simétricas. Por lo tanto <span class="math inline">\(-{\textbf Y}^T{\textbf X}{\textbf B}-{\textbf B}^T{\textbf X}^T{\textbf Y}=-2{\textbf B}^T{\textbf X}^T{\textbf Y}\)</span>, y su derivada <span class="math display">\[-2{\textbf X}^T{\textbf Y}\,.\]</span> \ \ (ii) <span class="math inline">\(\partial{{\textbf B}^T{\textbf X}^T{\textbf X}{\textbf B}}/\partial{\textbf B}=2{\textbf X}^T{\textbf X}{\textbf B}\)</span>. Para demostrar esto tomemos el caso para el ajuste lineal (2 parámetros). Definamos los elementos de <span class="math inline">\({\textbf X}^T{\textbf X}\)</span> como <span class="math inline">\(c_{ij}\,\,i,j=1,2\)</span> y <span class="math inline">\(c_{12}=c_{21}\)</span> por ser simétrica. Entonces <span class="math display">\[{\textbf B}^T{\textbf X}^T{\textbf X}{\textbf B}=c_{11}b_0^2+c_{22}b_1^2 + 2c_{12} b_0 b_1\,,\]</span> y su derivada respecto <span class="math inline">\(b_0\)</span> es <span class="math display">\[2c_{11}b_0 + 2c_{12}b_1\]</span> y respecto a <span class="math inline">\(b_1\)</span> es <span class="math display">\[2c_{12}b_0 + 2c_{22}b_1\,.\]</span> Si acomodamos las derivadas en un vector columna <span class="math inline">\(2\times1\)</span> obtenemos <span class="math display">\[\left(\begin{array}{c}
  2c_{11}b_0 + 2c_{12}b_1 \\
  2c_{12}b_0 + 2c_{22}b_1 \\
        \end{array}\right)=2{\textbf X}^T{\textbf X}{\textbf B}\,.\]</span> y por lo tanto <span class="math display">\[\frac{\partial{S}}{\partial{\textbf B}}=-2{\textbf X}^T{\textbf Y}+2{\textbf X}^T{\textbf X}{\textbf
B}\,,\]</span> y las ecuaciones normales quedan <span class="math display">\[-2{\textbf X}^T{\textbf Y}+2{\textbf X}^T{\textbf X}{\textbf
B}=0\,.\]</span></p>
<p>Finalmente, el problema de mínimos cuadrados es <span class="math display">\[({\textbf X}^T{\textbf X}){\textbf B}={\textbf X}^T{\textbf Y}\,.\]</span> Y resolviendo para <span class="math inline">\({\textbf B}\)</span> la forma general del método de regresión por mínimos cuadrados es <span class="math display">\[{\textbf B}=({\textbf X}^T{\textbf X})^{-1}{\textbf X}^T{\textbf Y}\,,\]</span> o equivalentemente <span class="math display">\[{\textbf B}^T={\textbf Y}^T{\textbf X}({\textbf X}^T{\textbf X})^{-1}\,.\]</span></p>
<p> \ Algunas de las consideraciones de un modelo de regresión múltiple son:</p>
<p> \ La esperanza del vector de coeficientes <span class="math inline">\(k\times 1\)</span>, <span class="math inline">\(b\)</span>, se obtiene a partir de las consideraciones 1,4, y 5. La consideración 5 implica que el estimador de los coeficientes <span class="math inline">\(b=({\textbf X}^T{\textbf X})^{-1}{\textbf X}^T { y}\)</span> se puede escribir cómo</p>
<p><span class="math display">\[b=({\textbf X}^T{\textbf X})^{-1}{\textbf X}^T({\textbf X} \beta + \epsilon)={\textbf I}\beta +
    ({\textbf X}^T{\textbf X})^{-1}{\textbf X}^T \epsilon\,.\]</span> \</p>
<p>De las consideraciones 1, y 4 obtenemos <span class="math display">\[E[b]=E[\beta + ({\textbf X}^T{\textbf X})^{-1}{\textbf X}^T\epsilon]=
  \beta + ({\textbf X}^T{\textbf X})^{-1}{\textbf X}^T \underbrace{E[\epsilon]}_{0}=\beta\,,\]</span> lo que implica que <span class="math inline">\(b\)</span> es insesgado.</p>
<p> \ Usando el resultado de arriba, bajo las consideraciones 1-6, la matriz de varianza de los estimadores de los coeficientes <span class="math inline">\(b\)</span> (o momento centrado de orden 2) viene dada por <span class="math display">\[{ var(b)}=E[(b-\beta)(b-\beta)^T]=E[({\textbf X}^T{\textbf X})^{-1}{\textbf X}^T\epsilon \epsilon^T{\textbf X}
    ({\textbf X}^T{\textbf X})^{-1}]=\]</span> <span class="math display">\[=({\textbf X}^T{\textbf X})^{-1}{\textbf X}^T E[\epsilon \epsilon^T]{\textbf X}({\textbf X}^T{\textbf X})^{-1}=
    ({\textbf X}^T{\textbf X})^{-1}{\textbf X}^T(\sigma^2{\textbf I}){\textbf X}({\textbf X}^T{\textbf X})^{-1}=
    \sigma^2({\textbf X}^T{\textbf X})^{-1}\,.\]</span> \ \ Los elementos de la diagonal principal de esta matriz son las varianzas asociadas a los estimadores de los coeficientes <span class="math inline">\(b\)</span>, y los elementos fuera de la diagonal principal representan la covarianza entre esos estimadores. \ Si asumimos que las variables del problema han sido estandarizadas, es decir la media ha sido extraida de las variables <span class="math inline">\(x_{ij}\)</span> e <span class="math inline">\(y_i\)</span> y hemos dividido por las desviaciones estándar <span class="math display">\[{\textbf X}^T{\textbf X}=\left(\begin{array}{cccc}
   \sum\limits_{i=1}^N{x_{i1}x_{i1}} &amp; \sum\limits_{i=1}^N{x_{i1}x_{i2}} &amp; ... &amp; \sum\limits_{i=1}^N{x_{i1}x_{ik}}\\
   \sum\limits_{i=1}^N{x_{i2}x_{i1}} &amp; \sum\limits_{i=1}^N{x_{i2}x_{i2}} &amp; ... &amp; \sum\limits_{i=1}^N{x_{i2}x_{ik}}\\
                . &amp; . &amp; . \\
        . &amp; . &amp; . \\
        . &amp; . &amp; . \\
   \sum\limits_{i=1}^N{x_{ik}x_{i1}} &amp; \sum\limits_{i=1}^N{x_{ik}x_{i2}} &amp; ... &amp; \sum\limits_{i=1}^N{x_{ik}x_{ik}}\\
        \end{array}\right)\,,\]</span> es una matriz <span class="math inline">\(k\times k\)</span> y en notación índice se puede escribir como <span class="math display">\[[{\textbf X}^T{\textbf X}]_{nm}=(N-1)\rho_{x_{in} x_{im}}\,\,;(i=1,2,...,N)\,,\]</span> que se puede interpretar como la matriz de correlaciones entre las variables independientes. La matriz <span class="math display">\[{\textbf X}^T{\textbf Y}=\left(\begin{array}{c}
  \sum\limits_{i=1}^N y_i x_{i1}\\
  \sum\limits_{i=1}^N y_i x_{i2}\\
                   . \\
           . \\
           . \\
  \sum\limits_{i=1}^N y_i x_{ik}\\
        \end{array}\right)\,,\]</span> es una matriz <span class="math inline">\(k\times 1\)</span> y en notación índice es <span class="math display">\[[{\textbf Y}^T{\textbf X}]_n=(N-1)\rho_{y_i x_{in}}\,\,;(i=1,2,...,N)\,.\]</span> Esta matriz se puede interpretar como la matriz de correlación entre las variables independientes y dependientes. \ El modelo multivariado en notación índice es <span class="math display">\[(N-1)\rho_{x_{in} x_{im}}b_m=(N-1)\rho_{y_i x_{in}}\]</span> o <span class="math display">\[\rho_{x_{in} x_{im}}b_m=\rho_{y_i x_{in}}\,;m=n=1,2,....,k; i=1,2,...,N\]</span> y para una única observación <span class="math inline">\(i\)</span> dada obtenemos <span class="math display">\[\rho_{x_{n} x_{m}}b_m=\rho_{y x_{n}}\,\]</span> Ahora supongamos, por simplicidad, que solo tenemos dos variables independientes. Entonces: <span class="math display">\[\rho_{x_{1} x_{1}}b_1+\rho_{x_{1} x_{2}}b_2=\rho_{y x_{1}}\]</span> <span class="math display">\[\rho_{x_{2} x_{1}}b_1+\rho_{x_{2} x_{2}}b_2=\rho_{y x_{2}}\,,\]</span> y puesto que <span class="math inline">\(\rho_{x_{1} x_{1}}=\rho_{x_{2} x_{1}}=1\)</span>, y <span class="math inline">\(\rho_{x_{1} x_{2}}=\rho_{x_{2} x_{1}}\)</span>, podemos escribir el sistema como: <span class="math display">\[\left(\begin{array}{cc}
  1 &amp; \rho_{x_{1} x_{2}} \\
  \rho_{x_{1} x_{2}} &amp; 1 \\
        \end{array}\right)
  \left(\begin{array}{c}
  b_1 \\
  b_2 \\
        \end{array}\right)=
     \left(\begin{array}{c}
  \rho_{y x_{1}} \\
  \rho_{y x_{2}} \\
        \end{array}\right)\,.\]</span> De forma que los coeficientes quedan como <span class="math display">\[\left(\begin{array}{c}
  b_1 \\
  b_2 \\
        \end{array}\right)=\left(\begin{array}{cc}
  1 &amp; \rho_{x_{1} x_{2}} \\
  \rho_{x_{1} x_{2}} &amp; 1 \\
        \end{array}\right)^{-1}
     \left(\begin{array}{c}
  \rho_{y x_{1}} \\
  \rho_{y x_{2}} \\
        \end{array}\right)=\frac{1}{1-\rho^2_{x_{1} x_{2}}}
    \left(\begin{array}{cc}
  1 &amp; -\rho_{x_{1} x_{2}} \\
  -\rho_{x_{1} x_{2}} &amp; 1 \\
        \end{array}\right)\left(\begin{array}{c}
  \rho_{y x_{1}} \\
  \rho_{y x_{2}} \\
        \end{array}\right)\,.\]</span> <span class="math display">\[b_1=\frac{1}{1-\rho^2_{x_{1} x_{2}}}({\rho_{y x_{1}} - \rho_{x_{1} x_{2}} \rho_{y x_{2}}})\]</span> <span class="math display">\[b_2=\frac{1}{1-\rho^2_{x_{1} x_{2}}}({\rho_{y x_{2}} - \rho_{x_{1} x_{2}} \rho_{y x_{1}}})\,.\]</span></p>
<p>Finalmente puntualizar que el problema de mínimos cuadrados se puede resolver utilizando la descomposición <span class="math inline">\(LU\)</span>. Para ello solo es necesario un cambio de variable en la ecuación <span class="math display">\[{\textbf X}^T{\textbf X}{\textbf B}={\textbf X}^T{\textbf Y}\]</span> para obtener un sistema de ecuaciones tipo <span class="math inline">\({\textbf A}{\textbf x}={\textbf b}\)</span>, donde ahora <span class="math inline">\({\textbf A}={\textbf X}^T{\textbf X}\)</span>, <span class="math inline">\({\textbf x}={\textbf B}\)</span>, y <span class="math inline">\({\textbf b}={\textbf X}^T{\textbf Y}\)</span>.</p>
<p>{2) Mínimos cuadrados con restricciones}</p>
<p>{NOTA: Multiplicadores de Lagrange} \ Dada la función <span class="math inline">\(f(x)=f(x_1,x_2,...,x_N)\)</span> que depende de <span class="math inline">\(N\)</span> variables y <span class="math inline">\(p\)</span> restricciones <span class="math inline">\(g_1(x)=d_1, g_2(x)=d_2,....,g_p(x)=d_p\)</span> entonces el teorema de Lagrange nos dice que para minimizar la función <span class="math inline">\(f(x)\)</span> bajo esas <span class="math inline">\(p\)</span> restricciones debemos resolver el sistema de ecuaciones <span class="math display">\[\frac{\partial}{\partial{x_i}}\left[ f(x) +\sum\limits_{j=1}^p \lambda_j g_j(x)\right]=0\,\,\,; i=1,2,...,N\]</span> <span class="math display">\[g_j(x)=d_j\,\,\,; j=1,2,...,p\]</span></p>
<p>Vamos a usar los multiplicadores de Lagrange para resolver el problema de mínimos cuadrados <span class="math display">\[{\textbf Y}={\textbf X}{\textbf B}\,,\]</span> pero incluyendo <span class="math inline">\(p\)</span> restricciones de la forma <span class="math display">\[{\textbf G}{\textbf B}={\textbf d}\,.\]</span></p>
<p>Queremos minimizar la función: <span class="math display">\[{\textbf \cal L}=({\textbf Y}-{\textbf X}{\textbf B})^T({\textbf Y}-{\textbf X}{\textbf B}) +{\textbf \lambda}^T({\textbf G}{\textbf B}-{\textbf d})\,.\]</span> Aqui hemos introducido <span class="math inline">\(p\)</span> incógnitas pero también tenemos <span class="math inline">\(p\)</span> nuevas ecuaciones <span class="math inline">\({\textbf G}{\textbf B}={\textbf d}\)</span>. Derivando <span class="math inline">\({\textbf \cal L}\)</span> e igualando a cero <span class="math display">\[\frac{\partial{\textbf \cal L}}{\partial{\textbf B}}=-2{\textbf X}^T{\textbf Y}+2{\textbf X}^T{\textbf X}{\textbf B} + {\textbf G}^T{\textbf \lambda}=0\,,\]</span> lo cual tiene la solución <span class="math display">\[{\textbf B}=( {\textbf X}^T {\textbf X} )^{-1} ({\textbf X}^T {\textbf Y} -\frac{1}{2}{\textbf G}^T{\textbf \lambda})\,,\]</span> que para <span class="math inline">\({\textbf \lambda}=0\)</span> se reduce a la expresión de los mínimos cuadrados sin restricciones <span class="math display">\[{\textbf B}=( {\textbf X}^T {\textbf X} )^{-1} ({\textbf X}^T {\textbf Y})\,.\]</span></p>
<p>Si sutituimos en la ecuación de las restricciones <span class="math display">\[{\textbf G}( {\textbf X}^T {\textbf X} )^{-1} ({\textbf X}^T {\textbf Y} -\frac{1}{2}{\textbf G}^T{\textbf \lambda})={\textbf d}\,,\]</span> y resolvemos para <span class="math inline">\({\textbf \lambda}\)</span>, obtenemos <span class="math display">\[\frac{1}{2}{\textbf \lambda}=\left[ {\textbf G}({\textbf X}^T {\textbf X})^{-1}{\textbf G}^T\right]^{-1} \left[ {\textbf G}({\textbf X}^T {\textbf X})^{-1}{\textbf X}^T{\textbf Y}-{\textbf d}\right]\,.\]</span></p>
<p>Finalmente, si sustituimos en la solución para la matriz de coeficientes <span class="math inline">\({\textbf B}\)</span> obtenemos <span class="math display">\[{\textbf B}=({\textbf X}^T {\textbf X})^{-1}\left( {\textbf X}^T {\textbf Y} - {\textbf G}^T\left[{\textbf G}({\textbf X}^T{\textbf X})^{-1}{\textbf G}^T \right]^{-1}
           \left[{\textbf G}({\textbf X}^T{\textbf X})^{-1}{\textbf X}^T {\textbf Y}-{\textbf d}\right] \right)\,.\]</span></p>
<p>Un ejemplo sería ajustar una recta a un conjunto de observaciones pero imponiendo que la recta pase por algún punto determinado del espacio <span class="math inline">\(xy\)</span>.</p>
<p>{3) Mínimos cuadrados pesados} \ En ocasiones debido a las incertidumbres asociadas a la medidas de las variables independientes es conveniente asignarles un peso diferencial en el problema de mínimos cuadrados. Supongamos que unas variables independientes son conocidas con mayor precisión que otras. Entonces el modelo de mínimos cuadrados pesados es: <span class="math display">\[SEC=({\textbf Y}-{\textbf X}{\textbf B})^T{\textbf W}_{\epsilon}({\textbf Y}-{\textbf X}{\textbf B})\,,\]</span> donde <span class="math inline">\({\textbf W}_{\epsilon}\)</span> es una matriz diagonal con los elementos <span class="math inline">\(\sigma_j^{-2}\)</span>, el inverso de la varianza de cada variable independiente. \ Sin embargo, en general, los errores en los datos estan correlacionados, y <span class="math inline">\({\textbf W}_{\epsilon}\)</span> no es diagonal. Una elección razonable para <span class="math inline">\({\textbf W}_{\epsilon}\)</span> es el inverso de la matriz de covarianzas. Si asumimos que <span class="math inline">\({\textbf Y}\)</span> se compone de un valor medio mas un error o fluctuación respecto la media <span class="math display">\[{\textbf Y}=\bar{\textbf Y}+{\textbf Y}'\,,\]</span> entonces la matriz de covarianzas es <span class="math inline">\(&lt;{\textbf Y}'{\textbf Y}'^T&gt;\)</span> y <span class="math display">\[{\textbf W}_{\epsilon}=&lt;{\textbf Y}'{\textbf Y}'^T&gt;^{-1}\,.\]</span></p>
<p>{jemplo (1):} Emery and Thompson (sección 3.12.4). \ En la tabla adjunta se muestran 5 observaciones de la variable independiente (<span class="math inline">\(x_i\)</span>) y dependiente (<span class="math inline">\(y_i\)</span>). Se pide ajustar una recta al conjunto de datos y calcular las medidas de error (varianza <span class="math inline">\(s^2\)</span> y coeficiente de correlación al cuadrado <span class="math inline">\(r^2\)</span>) asociadas al ajuste lineal. <!--
your comment goes here
\begin{center}
`` \includegraphics[width=1\textwidth]{regresion_example} ``
\end{center}
--> % %insert table</p>
<p>Los coeficientes del modelo lineal se pueden calcular con las expresiones: <span class="math display">\[\hat{b}_1=\frac{\left[N \sum\limits^N_{i=1} x_i y_i - \sum\limits^N_{i=1}x_i \sum\limits^N_{i=1}y_i\right]}
           {N\sum\limits^N_{i=1}x^2_i-\left(\sum\limits^N_{i=1}x_i \right)^2}=\]</span> <span class="math display">\[\frac{[(5)(7)-(0)(5)]}{[(5)(10)-10^2]}=0.7\]</span></p>
<p><span class="math display">\[b_0=\bar{y}-b_1\bar{x}=5/5-(0.7)(0)=1\]</span></p>
<p>En notación matricial obtenemos el mismo resultado:</p>
<p><span class="math display">\[{\textbf Y}=\left(\begin{array}{c}
  0 \\ 0 \\ 1 \\ 1 \\ 3\\
        \end{array}\right)\]</span></p>
<p><span class="math display">\[{\textbf X}=\left(\begin{array}{cc}
  1 &amp; -2 \\
  1 &amp; -1 \\
  1 &amp; 0 \\
  1 &amp; 1 \\
  1 &amp; 2\\
        \end{array}\right)\]</span></p>
<p><span class="math display">\[{\textbf X}^T{\textbf X}= \left(\begin{array}{ccccc}
   5 &amp;  0 \\
  0 &amp; 10 \\
        \end{array}\right)\]</span></p>
<p><span class="math display">\[{\textbf X}^T{\textbf Y}= \left(\begin{array}{ccccc}
   5  \\
  7  \\
        \end{array}\right)\]</span></p>
<p><span class="math display">\[({\textbf X}^T{\textbf X})^{-1}= \left(\begin{array}{ccccc}
   1/5 &amp;  0 \\
   0 &amp; 1/10 \\
        \end{array}\right)\]</span></p>
<p><span class="math display">\[ ({\textbf X}^T{\textbf X})^{-1}{\textbf X}^T{\textbf Y}=
    \left(\begin{array}{ccccc}
   1/5 &amp;  0 \\
   0 &amp; 1/10 \\
        \end{array}\right)\left(\begin{array}{ccccc}
   5  \\
  7  \\
        \end{array}\right)=
    \left(\begin{array}{ccccc}
   1  \\
  0.7  \\
        \end{array}\right)\,.\]</span></p>
<p>{} Como ya vimos, para calcular la varianza de nuestro ajuste usamos</p>
<p><span class="math display">\[s^2=\frac{1}{N-2}\sum\limits^N_{i=1}(y_i-\hat{y_i})^2=\frac{1}{N-2}SEC\,,\]</span></p>
<p>donde <span class="math inline">\(SEC\)</span> es la suma de los errores cuadrados y <span class="math inline">\(N-2\)</span> resulta debido a que en la regresión lineal se requiere la estimación de dos parámetros. En notación matricial <span class="math display">\[SEC={\textbf Y}^T{\textbf Y} - {\textbf Y}^T{\textbf X}{\textbf B}-{\textbf B}^T{\textbf X}^T{\textbf Y}+{\textbf B}^T
{\textbf X}^T{\textbf X}{\textbf B}=
{\textbf Y}^T{\textbf Y}-2{\textbf B}^T{\textbf X}^T{\textbf Y}+{\textbf B}^T{\textbf X}^T{\textbf X}{\textbf B}=\]</span></p>
<p><span class="math display">\[={\textbf Y}^T{\textbf Y}-2{\textbf B}^T{\textbf X}^T{\textbf Y}+{\textbf B}^T{\textbf X}^T{\textbf Y}=
{\textbf Y}^T{\textbf Y}-{\textbf B}^T{\textbf X}^T{\textbf Y}\,,\]</span></p>
<p>donde hemos usado la identidad <span class="math inline">\({\textbf X}^T{\textbf X}{\textbf B}={\textbf X}^T{\textbf Y}\)</span>.</p>
<p>Si sustituimos las matrices de nuestro ejemplo, obtenemos</p>
<p><span class="math display">\[SEC=\left(\begin{array}{ccccc}
  0 &amp; 0 &amp; 1 &amp; 1 &amp; 3\\
        \end{array}\right)
      \left(\begin{array}{c}
  0 \\ 0 \\ 1 \\ 1 \\ 3\\
        \end{array}\right)-
    \left(\begin{array}{ccccc}
  1 &amp; 0.7\\
        \end{array}\right)
    \left(\begin{array}{ccccc}
   1 &amp;  1 &amp; 1 &amp; 1 &amp; 1\\
  -2 &amp; -1 &amp; 0 &amp; 1 &amp; 2 \\
        \end{array}\right)
    \left(\begin{array}{c}
  0 \\ 0 \\ 1 \\ 1 \\ 3
        \end{array}\right)=\]</span> <span class="math display">\[=11-\left(\begin{array}{ccccc}
  1 &amp; 0.7\\
        \end{array}\right)
    \left(\begin{array}{c}
        5 \\ 7\\
        \end{array}\right)=11-9.9=1.1\]</span></p>
<p>{} <span class="math inline">\(SEC\)</span> puede ser calculado directamente con la expresión: <span class="math display">\[SEC=\sum \limits^N_{i=1}(\hat{y_i}-y_i)^2=
   (-0.4)^2 + (-0.3)^2+(0)^2 + (0.7)^2 + (0.6)^2=1.1\]</span></p>
<p>Y entonces la desviación estándar de nuestro ajuste lineal es <span class="math display">\[s=\sqrt{\frac{1}{N-2}SEC}=\sqrt{\frac{1}{5-2}1.1}=\sqrt{1.1/3}\simeq0.366\,.\]</span></p>
<p>{} El coeficiente de correlación se puede escribir como</p>
<p><span class="math display">\[r^2=\frac{SCR}{SCT}=\frac{\sum\limits^N_{i=1}(\hat{y_i}-\bar{y})^2}{\sum\limits^N_{i=1}
({y_i}-\bar{y})^2}=\frac{4.9}{6}\simeq0.8167\]</span></p>
<ol start="4" type="1">
<li><strong>Ajuste de curvas con mínimos cuadrados</strong> &nbsp;</li>
</ol>
<p>{} En general, podemos escribir nuestro modelo lineal como <span class="math display">\[Y=b_0 + b_1x + b_2 x^2 + ... + b_k x^k + \epsilon\,.\]</span> \ \ El procedimiento es el mismo que para el caso de la línea recta, pero ahora la matriz <span class="math inline">\({\textbf X}\)</span> tiene una columna mas. Es decir, para <span class="math inline">\(k=2\)</span> y para <span class="math inline">\(N\)</span> observaciones independientes las ecuaciones independientes son <span class="math display">\[y_1=b_0 + b_1x_1 + b_2 x_1^2 + \epsilon_1\]</span> <span class="math display">\[y_2=b_0 + b_1x_2 + b_2 x_2^2 + \epsilon_2\]</span> <span class="math display">\[...\]</span> <span class="math display">\[...\]</span> <span class="math display">\[...\]</span> <span class="math display">\[y_N=b_0 + b_1x_N + b_2 x_N^2 + \epsilon_N\,\]</span> y puden resolverse matricialmente para <span class="math inline">\({\textbf B}\)</span> como <span class="math display">\[{\textbf B}=({\textbf X}^T{\textbf X})^{-1}{\textbf X}^T{\textbf Y}\,,\]</span> donde <span class="math inline">\({\textbf X}\)</span> tiene una columna mas que el caso del ajuste de una recta, i.e.&nbsp;<span class="math inline">\(k+1\)</span> columnas.</p>
<p>{} <strong>Ejemplo (2)</strong>: Ajustes por cuadrados mínimos con restricciones.<br>
</p>
<p>{} Supongamos que queremos ajustar dos polinomios <span class="math inline">\(f(x)\)</span> y <span class="math inline">\(g(x)\)</span> de orden d-1 a dos conjunto de datos contínuos de <span class="math inline">\(M\)</span> y <span class="math inline">\(N\)</span> observaciones, respectivamente, <span class="math inline">\(x_1, x_2, ....,x_M\leq a\)</span> y <span class="math inline">\(x_{M+1}, x_{M+2}, ....,x_N&gt;a\)</span>, tal que, queremos minimizar:</p>
<p><span class="math display">\[\sum\limits^M_{i=1}(f(x_i)-y_i)^2 + \sum\limits^N_{i=M+1}(g(x_i)-y_i)^2\,,\]</span></p>
<p>sujeto a las restricciones:</p>
<p><span class="math display">\[f(a)=g(a)\,\,y\,\,f'(a)=g'(a)\,.\]</span><br>
</p>
<p>Primero debemos de construir las matrices del problema lineal de cuadrados mínimos:</p>
<p><span class="math display">\[ {\textbf X}=\left(\begin{array}{cccccccc}
  1 &amp; x_1   &amp; .  .  . &amp; x_1^{d-1}  &amp; 0 &amp; 0 &amp; .  .  . &amp; 0 \\
  . &amp; .         &amp;  &amp; .                  &amp; . &amp; . &amp;  &amp; . \\
  . &amp; .         &amp;  &amp; .                  &amp; . &amp; . &amp;  &amp; . \\
  . &amp; .         &amp;  &amp; .                  &amp; . &amp; . &amp;  &amp; . \\
  1 &amp; x_M &amp; .  .  . &amp; x_M^{d-1} &amp; 0 &amp; 0 &amp; .  .  . &amp; 0 \\
  0 &amp; 0       &amp; .  .  . &amp; 0                 &amp; 1 &amp; x_{M+1} &amp; .  .  . &amp; x^{d-1}_{M+1} \\
  . &amp; .         &amp;&amp; .                  &amp; . &amp; . &amp;  &amp; . \\
  . &amp; .         &amp; &amp; .                  &amp; . &amp; . &amp;  &amp; . \\
  . &amp; .         &amp;  &amp; .                  &amp; . &amp; . &amp;  &amp; . \\
  0 &amp; 0       &amp; .  .  . &amp; 0                &amp; 1  &amp; x_N &amp; .  .  . &amp; x_N^{d-1} \\
  \end{array}\right)
  \]</span></p>
<p><span class="math display">\[{\textbf Y}=\left(\begin{array}{c}
    y_1 \\ y_2 \\ . . . \\ y_M \\ y_{M+1}\\ y_{M+2}\\...\\y_N
      \end{array}\right)\,,\]</span></p>
<p><span class="math display">\[{\textbf G}=\left(\begin{array}{cccccccc}
        1 &amp; a &amp; .  .  .&amp;a^{d-1}&amp;-1&amp;-a&amp; .  .  .&amp;-a^{d-1}\\
        0 &amp; 1 &amp; .  .  .&amp;(d-1)a^{d-2}&amp;0&amp;-1&amp; .  .  .&amp;-(d-1)a^{d-2}\\
          \end{array}\right)\,,
          {\textbf d}=\left(\begin{array}{c}
            0\\
            0\\
              \end{array}\right)\,.\]</span></p>
<p>Segundo debemos de calcular los coeficientes del ajuste <span class="math inline">\({\textbf B}\)</span> con la expresión para mínimos cuadrados con restricciones.</p>
<p><span class="math display">\[{\textbf B}=({\textbf X}^T {\textbf X})^{-1}\left( {\textbf X}^T {\textbf Y} - {\textbf G}^T\left[{\textbf G}({\textbf X}^T{\textbf X})^{-1}{\textbf G}^T \right]^{-1}
           \left[{\textbf G}({\textbf X}^T{\textbf X})^{-1}{\textbf X}^T {\textbf Y}-{\textbf d}\right] \right)\,.\]</span></p>
<p>Apliquemos el caso particular del ajuste de dos rectas</p>
<p><span class="math inline">\(f(x)=b_1 + b_2 x\)</span> y <span class="math inline">\(g(x)=b_3 + b_4 x\)</span>:<br>
</p>
<p>donde <span class="math inline">\(h\)</span> es el valor donde ambas rectas interseccionan. Por ello exigimos que ambas rectas tomen el mismo valor y sus derivadas sean iguales en <span class="math inline">\(x=h\)</span>. Se resuleve el sistema de ecuaciones:</p>
<p><span class="math display">\[{\textbf X}\,{\textbf B}={\textbf Y}\]</span> <span class="math display">\[({\it 1})b_1+({\it x_1})b_2 + ({\it 0})b_3 + ({\it 0})b_4=y_1\]</span> <span class="math display">\[({\it 1})b_1+({\it x_2})b_2 + ({\it 0})b_3 + ({\it 0})b_4=y_2\]</span> <span class="math display">\[({\it 1})b_1+({\it x_3})b_2 + ({\it 0})b_3 + ({\it 0})b_4=y_3\]</span> <span class="math display">\[({\it 1})b_1+({\it x_4})b_2 + ({\it 0})b_3 + ({\it 0})b_4=y_4\]</span> <span class="math display">\[({\it 0})b_1+({\it 0})b_2 + ({\it 1})b_3 + ({\it x_5})b_4=y_5\]</span> <span class="math display">\[({\it 0})b_1+({\it 0})b_2 + ({\it 1})b_3 + ({\it x_6})b_4=y_6\]</span> <span class="math display">\[({\it 0})b_1+({\it 0})b_2 + ({\it 1})b_3 + ({\it x_7})b_4=y_7\]</span> <span class="math display">\[({\it 0})b_1+({\it 0})b_2 + ({\it 1})b_3 + ({\it x_8})b_4=y_8\,,\]</span> &nbsp;</p>
<p>bajo el sistema de ecuaciones de restricciones:</p>
<p><span class="math display">\[{\textbf G}\,{\textbf B}={\textbf d}\]</span> <span class="math display">\[({\it 1})b_1+({\it h})b_2 + (-{\it 1})b_3 + (-{\it 1})b_4=0\]</span> <span class="math display">\[({\it 1})b_1+({\it 0})b_2 + (-{\it 1})b_3 + ({\it 0})b_4=0\]</span><br>
</p>
<p>En notación matricial:<br>
</p>
<p><span class="math display">\[{\textbf X}=\left(\begin{array}{cccccccc}
  1 &amp; x_1  &amp; 0 &amp; 0 \\
  1 &amp; x_2   &amp; 0 &amp; 0 \\
  1 &amp; x_3   &amp; 0 &amp; 0\\
  1 &amp; x_4   &amp; 0 &amp; 0 \\
  0 &amp; 0  &amp; 1 &amp; x_5 \\
  0 &amp; 0   &amp; 1 &amp; x_6 \\
  0 &amp; 0   &amp; 1 &amp; x_7\\
  0 &amp; 0   &amp; 1 &amp; x_8 \\
  \end{array}\right)\,,
{\textbf Y}=\left(\begin{array}{c}
    y_1 \\ y_2 \\ y_3 \\ y_4 \\ y_5\\ y_6\\y_7\\y_8
      \end{array}\right)\,,
{\textbf G}=\left(\begin{array}{cccccccc}
        1 &amp; h &amp; -1&amp;-h\\
        0 &amp; 1&amp; 0&amp;-1\\
          \end{array}\right)\,,
{\textbf d}=\left(\begin{array}{c}
            0\\
            0\\
              \end{array}\right)\,.
  \]</span></p>
<p><br>
</p>
<section id="cuadrados-mínimos-no-lineales" class="level4">
<h4 class="anchored" data-anchor-id="cuadrados-mínimos-no-lineales">Cuadrados Mínimos no-lineales<br>
</h4>
<p>Los mínimos cuadrados no-lineales se aplican cuando queremos ajustar un conjunto de observaciones a un modelo que no es lineal en cuanto a los coeficientes.</p>
<ol type="1">
<li>Linealización del problema no lineal En ocasiones se puede linealizar el problema y resolverlo utilizando la solución de mínimos cuadrados lineales. Por ejemplo, supongamos que queremos ajustar un conjunto de <span class="math inline">\(N\)</span> observaciones a una función no-lineal exponencial: <span class="math display">\[\hat{y}=a e^{bx}\,.\]</span></li>
</ol>
<p>Este ejemplo se puede linealizar: <span class="math display">\[\hat{y}=a e^{bx}\rightarrow ln(\hat{y})=ln(a) + bx\,,\]</span> y resolver el problema lineal: <span class="math display">\[{\cal Y}={\cal B}_1 + {\cal B}_2 {\cal X}\,,\]</span> donde hemos hecho el cambio de variables: <span class="math display">\[{\cal Y}=ln(\hat{y})\,,{\cal B}_1=ln(a)\,,{\cal B}_2=b\,\,\,{ y}\,{\cal X}=x\,.\]</span></p>
<p>Resolvemos para <span class="math inline">\({\cal B}_1\)</span> y <span class="math inline">\({\cal B}_2\)</span>:</p>
<p><span class="math display">\[{\cal B}=\left(\begin{array}{c}
            {\cal B}_1\\
            {\cal B}_2\\
              \end{array}\right)=({\cal X}^T {\cal X})^{-1}\,{\cal X}^T{\cal Y}
\,,\]</span> y calculamos los coeficientes originales <span class="math inline">\(a\)</span> y <span class="math inline">\(b\)</span> con el cambio de variables.</p>
<ol start="2" type="1">
<li>Resolución numérica del problema no lineal<br>
Cuando no es posible linealizar la función no-lineal que queremos ajustar, debemos minimizar la suma de los errores cuadráticos medios y resolver numéricamente. La <span class="math inline">\(SEC\)</span> del modelo exponencial</li>
</ol>
<p><span class="math display">\[SEC=\sum\limits^N_{i=1}(y_i-\hat{y_i})^2=\sum\limits^N_{i=1}
              {\underbrace{\left(y_i - a e^{bx_i}\right)}_{\epsilon_i}}^2\,\]</span></p>
<p>es minimizada derivando con respecto los coeficientes:</p>
<p><span class="math display">\[\frac{\partial{SEC}}{\partial{a}}=
      2\sum\limits^N_{i=1}\left(y_i - a e^{bx_i}\right)\frac{\partial{\epsilon_i}}{\partial{a}}=0\]</span></p>
<p><span class="math display">\[\frac{\partial{SEC}}{\partial{b}}=
2\sum\limits^N_{i=1}\left(y_i - a e^{bx_i}\right)\frac{\partial{\epsilon_i}}{\partial{b}}=0\,.\]</span></p>
<p>Debemos resolver el sistema de ecuaciones de arriba, lo cual se puede hacer con descomposiciones algebráicas tipo <span class="math inline">\({\textbf L}{\textbf U}\)</span>, minimizando <span class="math inline">\(SEC\)</span> con la función de Matlab <em>fminsearch.m</em> (o implementando un método numérico manual), o directamente utilizando la función de mínimos cuadrados no-lineales de Matlab <em>nlinfit.m</em>.<br>
Otro ejemplo no-lineal es ajustar la siguiente función trigonométrica: <span class="math display">\[\hat{y}=\phi_1 e^{\phi_2 x} cos(\phi_3 x + \phi_4)\,,\]</span> donde la suma de los errores cuadráticos es:</p>
<p><span class="math display">\[SEC=\sum\limits^N_{i=1}(y_i-\hat{y_i})^2=\sum\limits^N_{i=1}
              {\underbrace{\left(y_i - \phi_1 e^{\phi_2 x_i} cos(\phi_3 x_i + \phi_4)\right)}_{\epsilon_i}}^2\,,\]</span></p>
<p>y sus derivadas respecto los coeficientes <span class="math inline">\(\phi_1\)</span>, <span class="math inline">\(\phi_2\)</span>, <span class="math inline">\(\phi_3\)</span>, y <span class="math inline">\(\phi_4\)</span> son:</p>
<p><span class="math display">\[\frac{\partial{SEC}}{\partial{\phi_1}}=
     2\sum\limits^N_{i=1}\left(y_i - \phi_1 e^{\phi_2 x_i} cos(\phi_3 x_i + \phi_4)\right)
     \frac{\partial{\epsilon_i}}{\partial{\phi_1}}=0\]</span></p>
<p><span class="math display">\[\frac{\partial{SEC}}{\partial{\phi_2}}=2\sum\limits^N_{i=1}\left(y_i - \phi_1 e^{\phi_2 x_i} cos(\phi_3 x_i + \phi_4)\right)
\frac{\partial{\epsilon_i}}{\partial{\phi_2}}=0\]</span></p>
<p><span class="math display">\[\frac{\partial{SEC}}{\partial{\phi_3}}=
    2\sum\limits^N_{i=1}\left(y_i - \phi_1 e^{\phi_2 x_i} cos(\phi_3 x_i + \phi_4)\right)
    \frac{\partial{\epsilon_i}}{\partial{\phi_3}}=0\]</span></p>
<p><span class="math display">\[\frac{\partial{SEC}}{\partial{\phi_4}}=
    2\sum\limits^N_{i=1}\left(y_i - \phi_1 e^{\phi_2 x_i} cos(\phi_3 x_i + \phi_4)\right)
    \frac{\partial{\epsilon_i}}{\partial{\phi_4}}=0\]</span></p>
<p><strong>Relación entre regresión y correlación</strong><br>
</p>
<p>El coeficiente de correlación, <span class="math inline">\(r\)</span>, nos informa de que tanto dos (o mas) variables covarian en el espacio-tiempo. Para dos variables aleatorias <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span>, el coeficiente de correlación es</p>
<p><span class="math display">\[r=\rho_{xy}=\frac{\frac{1}{N-1}\sum\limits^N_{i=1}(x_i-\bar{x})(y_i-\bar{y})}
     {\sqrt{\frac{1}{N-1}\sum\limits^N_{i=1}(x_i-\bar{x})}
     \sqrt{\frac{1}{N-1}\sum\limits^N_{i=1}(y_i-\bar{y})}}=\frac{C_{xy}}{s_x s_y}\,,\]</span></p>
<p>donde <span class="math inline">\(C_{xy}\)</span> es la covarianza de <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span>, y <span class="math inline">\(s_x\)</span> y <span class="math inline">\(s_y\)</span> son las correspondientesdesviaciones estándar.<br>
<br>
<strong>Propiedades del coeficiente de correlación</strong><br>
</p>
<ol type="1">
<li><span class="math inline">\(r\)</span> es adimensional.<br>
</li>
<li>la magnitud de <span class="math inline">\(r\)</span> se encuantra acotada entre <span class="math inline">\(-1\)</span> y <span class="math inline">\(1\)</span>, ya que es una normalización de la covarianza por el producto de la desviación estándar de las dos variables aleatorias.<br>
</li>
</ol>
<p>Si <span class="math inline">\(r=\pm1\)</span> entonces el ajuste es perfecto. Para <span class="math inline">\(r=0\)</span> los puntos estan dispersos aleatoriamente y no existe relación alguna entre las variables. Normalmente encontramos el estadístico <span class="math inline">\(r^2\)</span> en lugar de <span class="math inline">\(r\)</span>. <span class="math inline">\(r^2\)</span> se puede reescribir como</p>
<p><span class="math display">\[r^2=SCR/SCT=\frac{SCT-SEC}{SCT}=1-\frac{SEC}{SCT}=\frac{C^2_{xy}}{(s_x s_y)^2}\,,\]</span></p>
<p>lo que nos informa del porcentaje de la varianza explicada (<span class="math inline">\(r^2={ varianza\,explicada}/{ varianza\,total}\)</span>) como vimos anteriormente. Un valor de <span class="math inline">\(r=0.75\)</span> significa que la regresión lineal de <span class="math inline">\(y\)</span> sobre <span class="math inline">\(x\)</span>, es decir <span class="math inline">\(\hat{y}\)</span>, explica <span class="math inline">\(100*r^2=56.25\%\)</span> de la varianza total de la muestra.<br>
Finalmente puntualizar que también podemos calcular el coeficiente de correlación utilizando los estimados de los coeficientes de regresión. Para el caso de una línea recta sabemos</p>
<p><span class="math display">\[\hat{b}_1=\frac{C_{xy}}{s^2_x}\,.\]</span></p>
<p>Si sustituimos en la definición de <span class="math inline">\(r\)</span> obtenemos</p>
<p><span class="math display">\[r=\hat{b}_1\frac{s^2_x}{s_x s_y}=\hat{b}_1\frac{s_x}{s_y}\,.\]</span></p>
<p><strong>Coeficiente de correlación ajustado</strong><br>
</p>
<p>Con el fin de considerar los grados de libertad del modelo de regresión lineal, el coeficiente de correlación debe ser ajustado en función del número de variables independientes <span class="math inline">\(k\)</span>. Para ello hay que considerar la verdadera varianza de los errores</p>
<p><span class="math display">\[Var(SEC)=\frac{SEC}{N-k-1}\,,\]</span></p>
<p>y de la variable dependiente</p>
<p><span class="math display">\[Var(SCT)=\frac{SCT}{N-1}\,.\]</span></p>
<p>Puesto que los grados de libertad no son iguales (<span class="math inline">\(N-1\)</span> vs <span class="math inline">\(N-k-1\)</span>), el coeficiente de correlación ajustado al cuadrado (<span class="math inline">\(\tilde{r}^2\)</span>) se define como</p>
<p><span class="math display">\[\tilde{r}^2=1-\frac{Var(SEC)}{Var(SCT)}=1-\frac{SEC/N-k-1}{SCT/N-1}=
1-\frac{N-1}{N-k-1}(1-r^2)=\]</span> <span class="math display">\[=1-(1-r^2)\frac{N-1}{N-k-1}\,,\]</span></p>
<p>donde para <span class="math inline">\(k=0\)</span> obtenemos la definición clásica del coeficiente de correlación.<br>
<br>
<strong>Intervalo de confianza para el coeficiente de correlación</strong><br>
Podemos calcular intervalos de confianza para el coeficiente de correlación <span class="math inline">\(r\)</span> por medio de la denominada transformación Z de Fisher. Básicamente transforma <span class="math inline">\(r\)</span> en una variable normal estándar <span class="math inline">\(Z\)</span></p>
<p><span class="math display">\[Z=\frac{1}{2}\frac{ln(1+r)}{ln(1-r)}\,,\]</span></p>
<p>con desviación estándar</p>
<p><span class="math display">\[\sigma(Z)=\frac{1}{\sqrt{(N-3)}}\,,\]</span></p>
<p>y media</p>
<p><span class="math display">\[\mu(Z)=\frac{1}{2}\frac{ln(1+\rho_0)}{ln(1-\rho_0)}\,,\]</span></p>
<p>donde <span class="math inline">\(\mu(Z)\)</span> es la media esperada (media poblacional) del estadístico <span class="math inline">\(Z\)</span>.</p>
<p>El intervalo de confianza se escribe entonces como</p>
<p><span class="math display">\[Z-Z_{\alpha/2}&lt;Z&lt;Z+Z_{\alpha/2}\]</span></p>
<p>*Ejemplo**:<br>
Supongamos <span class="math inline">\(N=21\)</span> y <span class="math inline">\(r=0.8\)</span>. Encuentra el intervalo de confianza al <span class="math inline">\(95\%\)</span> para el coeficiente de correlación poblacional <span class="math inline">\(\rho_0\)</span>.</p>
<p><span class="math display">\[Z=\frac{1}{2}\frac{ln(1+0.8)}{ln(1-0.8)}=1.0986\]</span></p>
<p>Puesto que <span class="math inline">\(Z\)</span> esta Normalmente distribuida, entonces todos los valores deben de caer dentro de 1.96 desviaciones estándar de <span class="math inline">\(Z\)</span>. Entonces, al <span class="math inline">\(95\%\)</span>, la verdadera media <span class="math inline">\(\mu_Z\)</span> esta contenida en <span class="math display">\[Z-1.96\sigma(Z) &lt; \mu(Z) &lt; Z + 1.96\sigma(Z)\]</span> <span class="math display">\[Z-1.96\frac{1}{\sqrt{(21-3)}} &lt; \mu(Z) &lt; Z + 1.96\frac{1}{\sqrt{(21-3)}}\]</span> <span class="math display">\[0.6366&lt;\mu(Z)&lt;1.5606\]</span></p>
<p>Los límites encontrados para <span class="math inline">\(\mu(Z)\)</span> los podemos transformar en términos de la verdadera correlación <span class="math display">\[\mu(Z)=0.6366=\frac{1}{2} \left\{ \frac{1+\rho}{1-\rho} \right\} \rightarrow  \rho=0.56\,,\]</span> donde hemos usado <span class="math display">\[e^{2\mu}=\frac{1+\rho}{1-\rho}\]</span> <span class="math display">\[e^{2\mu}-1=\rho(1+e^{2\mu})\]</span> <span class="math display">\[\rho=\frac{(e^{2\mu(Z)}-1)}{(e^{2\mu(Z)}+1)}\,.\]</span></p>
<p>Podemos afirmar con un 95% de confianza que la verdadera correlación <span class="math inline">\(\rho\)</span> esta en el intervalo <span class="math inline">\(0.56&lt;\rho,0.92\)</span>, dado un tamaño muestral <span class="math inline">\(N=21\)</span> y una correlación muestral <span class="math inline">\(r=0.8\)</span>.</p>
<p>{erdaderos grados de libertad} \ Ya hemos definido anteriormente los grados de libertad se define como el número de muestras independientes <span class="math inline">\(N\)</span> menos el número de parámetros que se quieren estimar. Esta definición es un tanto incorrecta puesto que debemos de también asegurar que las <span class="math inline">\(N\)</span> muestras son efectivamente independientes, es decir, no estan autocorreladas en el espacio-tiempo. Para considerar esto <span class="math inline">\(N\)</span> debe reesecribirse como</p>
<p><span class="math display">\[N^{*}=\frac{N}{\left[ \sum\limits^{\infty}_{\tau=-\infty}
C_{xx}(\tau)C_{yy}(\tau) + C_{xy}(\tau)C_{yx}(\tau)
\right]/\left[ C_{xx}(0)C_{yy}(0)\right]}=\]</span></p>
<p><span class="math display">\[\frac{N}{\left[\sum\limits^{\infty}_{\tau=-\infty}
               \rho_{xx}(\tau)\rho_{yy}(\tau) +
           \rho_{xy}(\tau)\rho_{yx}(\tau)\right]}\,.\]</span></p>
<p>En general, series de datos suelen estar correlacionados en el espacio-tiempo y <span class="math inline">\(N^{*}&lt;&lt;N\)</span>. Cuanto mayores las escalas de correlación espaciales-temporales, menores los <span class="math inline">\(N^*\)</span>. Esto nos hace pensar que es muy importante la selección de las escalas espaciales-temporales sobre las que queremos calcular un estadístico. Para extraer las escalas de interés podemos usar métodos espectrales y filtros. El proceso de filtrado se encarga de eliminar aquellas escalas que esperamos no contribuyen a la verdadera correlación pero pueden adicionar correlación artificial debido a errores instrumentales y de muestreo.</p>
</section>
</section>
</section>
<section id="propagación-de-errores" class="level2">
<h2 class="anchored" data-anchor-id="propagación-de-errores">Propagación de errores</h2>
<p>*Regla 1** Si <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span> tienen errores aleatorios independientes <span class="math inline">\(\delta{x}\)</span> y <span class="math inline">\(\delta{y}\)</span>,nentonces el error en la suma <span class="math inline">\(z=x+y\)</span> es</p>
<p><span class="math display">\[\delta{z}=\sqrt{\delta{x}^2 + \delta{y}^2}\,.\]</span></p>
<p>*Regla 2**</p>
<p>Si <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span> tienen errores aleatorios independientes <span class="math inline">\(\delta{x}\)</span> y <span class="math inline">\(\delta{y}\)</span>, entonces el error en la multiplicación <span class="math inline">\(z=xy\)</span> es</p>
<p><span class="math display">\[\frac{\delta{z}}{z}=\sqrt{\left(\frac{\delta{x}^2}{x}\right) + \left(\frac{\delta{y}^2}{y}\right)}\,.\]</span></p>
<p>*Regla 3**<br>
</p>
<p>Si <span class="math inline">\(z=f(x)\)</span>, donde <span class="math inline">\(f()\)</span> es una función dada, entonces</p>
<p><span class="math display">\[\delta{z}=|f'(x)|\delta{x}\,.\]</span></p>
<p>*Formula general para propagación del error**</p>
<p>Sea <span class="math inline">\(x_1, x_2,....,x_N\)</span> medidas con incertidumbres <span class="math inline">\(\delta{x_1},\delta{x_2},....,\delta{x_3}\)</span>. Supongamos que queremos determinar <span class="math inline">\(q\)</span>, el cual es una función de <span class="math inline">\(x_1,x_2,...,x_N\)</span>: <span class="math display">\[q=f(x_1, x_2,...,x_N)\,.\]</span> El error asociado a <span class="math inline">\(q\)</span> es entonces</p>
<p><span class="math display">\[\delta{q}=\sqrt{\left( \frac{\partial{q}}{\partial{x_1}}\delta{x_1}\right)^2 + ... +
                  \left( \frac{\partial{q}}{\partial{x_N}}\delta{x_N}\right)^2}\]</span> Si <span class="math inline">\(q=x_1+x_2\)</span> entonces obtenemos la regla 1:</p>
<p><span class="math display">\[\frac{\partial{q}}{\partial{x_1}}=1\,,\]</span> <span class="math display">\[\frac{\partial{q}}{\partial{x_2}}=1\,,\]</span></p>
<p><span class="math display">\[\delta{q}=\sqrt{\delta{x_1}^2 + \delta{x_2}^2}\,.\]</span></p>
<p>Si <span class="math inline">\(q=x_1x_2\)</span> entonces obtenemos la regla 2:</p>
<p><span class="math display">\[\frac{\partial{q}}{\partial{x_1}}=x_2\,,\]</span> <span class="math display">\[\frac{\partial{q}}{\partial{x_2}}=x_1\,,\]</span></p>
<p><span class="math display">\[\delta{q}=\sqrt{x^2_2\delta{x_1}^2 + x^2_1\delta{x_2}^2}=\sqrt{q^2\left[ \left(\frac{\partial{x_1}}{x_1}\right)^2 + \left(\frac{\partial{x_2}}{x_2}\right)^2\right]}\,.\]</span></p>
<p><span class="math display">\[\frac{\delta{q}}{q}=\sqrt{\left(\frac{\delta{x_1}^2}{x_1}\right) + \left(\frac{\delta{x_2}^2}{x_2}\right)}\,.\]</span></p>
<p>Demstración:*</p>
<p>Queremos calcular la desviación estándar de la función</p>
<p><span class="math display">\[q=q(x_1, x_2,....,x_N)\,,\]</span></p>
<p>que depende de <span class="math inline">\(N\)</span> variables independientes <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, ….,<span class="math inline">\(x_N\)</span>. El desarrollo de Taylor de la función <span class="math inline">\(q\)</span> alrededor de la media <span class="math inline">\(\bar{q}\)</span> se puede escribir: <span class="math display">\[q - \bar{q}=\left(x_1 - \bar{x}_1\right)\frac{\partial{q}}{\partial{x_1}} +
                          \left(x_2 - \bar{x}_2\right)\frac{\partial{q}}{\partial{x_2}}+  \,.\,.\,.\,
                          + \left(x_N - \bar{x}_N\right)\frac{\partial{q}}{\partial{x_N}}\,.\]</span></p>
<p>La varianza de la función <span class="math inline">\(q\)</span> es:</p>
<p><span class="math display">\[s^2_q=\frac{1}{N-1}\sum\limits^N_{i=1}\left( q_i-\bar{q} \right)^2=
               \frac{1}{N-1}\left[
               \left(x_1 - \bar{x}_1\right)\frac{\partial{q}}{\partial{x_1}} +
               \left(x_2 - \bar{x}_2\right)\frac{\partial{q}}{\partial{x_2}} +
              \,.\,.\,.\, +
               \left(x_N - \bar{x}_N\right)\frac{\partial{q}}{\partial{x_N}}
               \right]^2=\]</span></p>
<p><span class="math display">\[=\frac{1}{N-1}\left[
\left(x_1 - \bar{x}_1\right)^2\left(\frac{\partial{q}}{\partial{x_1}}\right)^2 +
\left(x_2 - \bar{x}_2\right)^2\left(\frac{\partial{q}}{\partial{x_2}}\right)^2 +
2\left(x_1 - \bar{x}_1\right)\left(x_2 - \bar{x}_2\right)
\frac{\partial{q}}{\partial{x_1}}\frac{\partial{q}}{\partial{x_2}}+\,.\,.\,.\right]=\]</span></p>
<p><span class="math display">\[=s^2_{x_1}\left(\frac{\partial{q}}{\partial{x_1}}\right)^2 +
   s^2_{x_2}\left(\frac{\partial{q}}{\partial{x_2}}\right)^2+
   2 s_{{x_1}{x_2}}
   \frac{\partial{q}}{\partial{x_1}}\frac{\partial{q}}{\partial{x_2}}+\,.\,.\,.\]</span></p>
<p>y finalmente la expresión general para la propagación del error es:</p>
<p><span class="math display">\[s_{q}=\sqrt{s^2_{x_1}\left(\frac{\partial{q}}{\partial{x_1}}\right)^2 +
               s^2_{x_2}\left(\frac{\partial{q}}{\partial{x_2}}\right)^2+
                2 s_{{x_1}{x_2}}
                \frac{\partial{q}}{\partial{x_1}}\frac{\partial{q}}{\partial{x_2}}+\,.\,.\,.}\]</span></p>
<p><strong>Ejemplo:</strong></p>
<p>La ecuación para el cálculo de la salinidad a partir de la conductividad (C) y temperatura (T) es (Unesco EOS-80)</p>
<p><span class="math display">\[S=a_0+a_1 R_T^{1/2}+a_2 R_T+a_3 R_T^{3/2}+a_4 R_T^{2}+a_5 R_T^{5/2}+\Delta{S}\,,\]</span></p>
<p>donde <span class="math display">\[R_T=\frac{R}{R_p r_t}\,\,\,,\,\,R=\frac{C(S,T,0)}{C(35,15,0)}\,,\]</span></p>
<p><span class="math inline">\(C(35,15,0)\)</span> es la conductividad de un agua de salinidad práctica 35 a los <span class="math inline">\(15\,^\circ{ C}\)</span>, <span class="math display">\[r_t=c_0 + c_1 T + c_2 T^2 + c_3T^3 + c_4T^4\,,\]</span></p>
<p><span class="math display">\[R_p=1+\frac{P(e_1+e_2P+e_3P^2)}{(1+d_1T+d_2T^2+(d_3 + d_4T)R)}\,,\]</span> y <span class="math display">\[\Delta{S}=\frac{T-15}{1+k(T-15)}(b_0+b_1 R_T^{1/2}+b_2 R_T+b_3 R_T^{3/2}+b_4 R_T^{2}+b_5 R_T^{5/2})\,,\]</span></p>
<p>con los coeficientes <span class="math inline">\(a_i\,,b_i\,,c_i\,,d_i\,,\,\,y\,e_i\)</span>&nbsp;</p>
<p><span class="math inline">\(a_0=0.0080\,\,\,\,\,\,\,\,\,\,\,\,\,\,b_0=0.0005\,\,\,\,\,\,\,\,\,\,\,\,\,\,c_0=0.6766097\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,d_1=3.426\,e^{-2}\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,e_1=2.070\,e^{-5}\)</span> &nbsp;<span class="math inline">\(a_1=-0.1692\,\,\,\,\,\,\,\,\,\,b_1=-0.0056\,\,\,\,\,\,\,\,\,c_1=2.00564\,e^{-2}\,\,\,\,\,\,\,\,\,\,\,d_2=4.464\,e^{-4}\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,e_2=-6.370\,e^{-10}\)</span> &nbsp;</p>
<p><span class="math inline">\(a_2=25.3851\,\,\,\,\,\,\,\,\,\,\,\,b=-0.0066\,\,\,\,\,\,\,\,\,\,\,c_2=1.104259\,e^{-4}\,\,\,\,\,\,\,\,\,d_3=4.215\,e^{-1}\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,e_3=3.989\,e^{-15}\)</span><br>
</p>
<p><span class="math inline">\(a_3=14.0941\,\,\,\,\,\,\,\,\,\,\,\,b=-0.0375\,\,\,\,\,\,\,\,\,\,\,c_3=-6.9698\,e^{-7}\,\,\,\,\,\,\,\,\,\,\,d_4=-3.107\,e^{-3}\)</span> &nbsp; <span class="math inline">\(a_4=-7.0261\,\,\,\,\,\,\,\,\,\,b_4=0.0636\,\,\,\,\,\,\,\,\,\,\,\,\,\,c_4=1.0031\,e^{-9}\)</span><br>
</p>
<p><span class="math inline">\(a_5=2.7081\,\,\,\,\,\,\,\,\,\,\,\,\,\,b_5=-0.0144\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\)</span><br>
</p>
<p><span class="math inline">\(\sum a_i=35.0000\,\,\,\,\sum b_i=0.0000\)</span><br>
</p>
<p><span class="math inline">\(k=0.0162\)</span><br>
</p>
<p>Si el error de precisión del termistor y de la celda de conductividad del CTD es <span class="math inline">\(\delta{T}=0.001\,^\circ{ C}\)</span> y <span class="math inline">\(\delta{C}=0.001\,{ S}{ m}^{-1}\)</span>, respectivamente, calcule la incertidumbre asociada al cálculo de la salinidad <span class="math inline">\(S\)</span> a partir de la formula general de propagación del error para <span class="math inline">\(C=5\,{ S}\,{ m}^{-1}\)</span>, <span class="math inline">\(T=28\,^\circ\,{ C}\)</span>, y <span class="math inline">\(P=50\,{ dbar}\)</span>. Suponga que <span class="math inline">\(\delta{P}=0\)</span>, es decir, el altímetro no tiene errores de precisión.<br>
</p>
<p><strong>Resultado</strong>:</p>
<p>El error estándar asociado a la salinidad es</p>
<p><span class="math display">\[\delta{S}=\sqrt{\left( \frac{\partial{S}}{\partial{T}}\delta{T} \right)^2 + \left( \frac{\partial{S}}{\partial{C}}\delta{C}\right)^2}\,,\]</span></p>
<p>donde</p>
<p><span class="math display">\[\frac{\delta{S}}{\delta{T}}=
      \left[(1+k(T-15))^{-1} - (k(T-15))(1+k(T-15))^{-2}\right]\]</span> <span class="math display">\[\left (b_0+b_1 R_T^{1/2}+b_2 R_T+b_3 R_T^{3/2}+b_4 R_T^{2}+b_5 R_T^{5/2}\right)\,,\]</span> y</p>
<p><span class="math display">\[\frac{\delta{S}}{\delta{C}}=
                 \frac{1}{2} a_1 \frac{R_T^{-1/2}}{C(35,T,0)} +
         a_2 \frac{1}{C(35,T,0)} +
             \frac{3}{2}a_3 \frac{R_T^{1/2}}{C(35,T,0)}+ \]</span></p>
<p><span class="math display">\[+2 a_4 \frac{R_T}{C(35,T,0)} +\frac{5}{2} a_5 \frac{R_T^{3/2}}{C(35,T,0)}+\frac{T-15}{(1+k(T-15))}\]</span></p>
<p><span class="math display">\[\left(b_1 \frac{R_T^{-1/2}}{C(35,T,0)} +
         b_2 \frac{1}{C(35,T,0)} +
             \frac{3}{2}b_3 \frac{R_T^{1/2}}{C(35,T,0)}+
               2 b_4 \frac{R_T}{C(35,T,0)} +
     +\frac{5}{2} b_5 \frac{R_T^{3/2}}{C(35,T,0)}
     \right)\,.\]</span></p>
<p>Por lo tanto, dada una medida de conductividad <span class="math inline">\(C\)</span> y temperatura <span class="math inline">\(T\)</span> podemos calcular <span class="math inline">\({\delta{S}}/{\delta{T}}\)</span> y <span class="math inline">\({\delta{S}}/{\delta{C}}\)</span> y obtener la desviación estándar asociado al cálculo de la salinidad <span class="math inline">\(\delta{S}\)</span>.</p>
<p><strong>Ejercicio</strong>:</p>
<p>Calcule el error estándar asociado a la densidad referenciada a la superfície que se obtiene al usar el algoritmo del estado del agua de mar (Unesco EOS-80) <span class="math display">\[\rho(S,T,0)=\rho_w + \left(b_0+b_1 T+b_2 T^2+b_3 T^{3}+b_4 T^{4}\right) S +\]</span></p>
<p><span class="math display">\[+\left(c_0+c_1 T+c_2 T^2\right) S^{3/2} + d_0 S^2\,,\]</span></p>
<p>con los coeficientes <span class="math inline">\(b_i\)</span>, <span class="math inline">\(c_i\)</span>, y <span class="math inline">\(d_0\)</span><br>
<span class="math inline">\(b_0=8.24493\,e^{-1}\,\,\,\,\,\,\,\,\,\,\,\,\,\,c_0=-5.72466\,e^{-3}\)</span> &nbsp;<span class="math inline">\(b_1=-4.0899\,e^{-3}\,\,\,\,\,\,\,\,\,\,\,\,c_1=1.0227\,e^{-4}\)</span><br>
<span class="math inline">\(b_2=7.6438\,e^{-5}\,\,\,\,\,\,\,\,\,\,\,\,c_2=-1.6546\,e^{-6}\)</span><br>
<span class="math inline">\(b_3=-8.2467\,e^{-7}\,\,\,\,\,\,\,\,\,\,\,\,\)</span> &nbsp; <span class="math inline">\(b_4=5.3875\,e^{-9}\,\,\,\,\,\,\,\,\,\,\,\,d_0=4.8314\,e^{-4}\)</span>,,<br>
</p>
<p>y la densidad de referencia para agua pura (<span class="math inline">\(\rho_w\)</span>) definida como:</p>
<p><span class="math display">\[\rho_w=a_0+a_1 T+a_2 T^2+a_3 T^{3}+a_4 T^{4}+a_5 T^{5}\,,\]</span></p>
<p>donde los coeficientes <span class="math inline">\(a_i\)</span> son:<br>
<span class="math inline">\(a_0=999.842594\)</span> &nbsp;<span class="math inline">\(a_1=6.793952\,e^{-2}\)</span> &nbsp;<span class="math inline">\(a_2=-9.095290\,e^{-3}\)</span> &nbsp;<span class="math inline">\(a_3=1.001685\,e^{-4}\)</span> &nbsp;<span class="math inline">\(a_4=-1.120083\,e^{-6}\)</span>&nbsp;<span class="math inline">\(a_5=6.536332\,e^{-9}\)</span>.</p>
</section>
<section id="métodos-de-interpolación" class="level2">
<h2 class="anchored" data-anchor-id="métodos-de-interpolación">Métodos de Interpolación</h2>
<p>Interpolación es el procedimiento para el cual obtenemos valores de propiedades en posiciones o tiempos que nunca fueron muestreados a partir de observaciones existentes en otras localizaciones o tiempos. En oceanografía necesitamos interpolar (i) para rellenar huecos cuando el instrumento dejo de medir, (ii) para obtener mapas espaciales de contornos (2D), (iii) para calcular alguna propiedad derivada en un punto concreto</p>
<section id="interpolación-lineal" class="level3">
<h3 class="anchored" data-anchor-id="interpolación-lineal">Interpolación Lineal</h3>
<p>La interpolación mas sencilla, pero no por ello peor, es la interpolación lineal. Esta interpolación se basa en ajustar una línea recta entre los puntos conocidos, e interpolar cualquier punto intermedio como un punto a lo largo de la recta. Este tipo de interpolación puede ser usado para rellenar huecos en nuestras series temporales. La interpolación lineal de unas serie y(t) se puede escribir como <span class="math display">\[y(t_i)=y(t_0)+\frac{y(t_1)-y(t_0)}{t_1 - t_0}(t_i-t_0)\,.\]</span></p>
</section>
<section id="interpolación-polinómica" class="level3">
<h3 class="anchored" data-anchor-id="interpolación-polinómica">Interpolación polinómica</h3>
<p>En el caso que queramos interpolar entre mas de dos puntos simultaneamente, debemos de usar polinomios de orden superior a la recta (orden 1). Es decir, <span class="math display">\[y(x)=a_0 + a_1 x + a_2 x^2 + ... + a_{n-1}x^{n-1} + a_n x_n\,.\]</span> Si tomamos <span class="math inline">\(N\)</span> observaciones (<span class="math inline">\(y_i(x)\,\,;i=0,2,...,N-1\)</span>) obtenemos un sistema de <span class="math inline">\(N\)</span> ecuaciones</p>
<p><span class="math display">\[\left(\begin{array}{ccccc}
  1 &amp; x_0 &amp; ... &amp; x_0^{N-1} &amp; x_0^N\\
  1 &amp; x_1 &amp; ... &amp; x_1^{N-1} &amp; x_1^N\\
  . &amp; . &amp; . &amp; . &amp; .\\
  . &amp; . &amp; . &amp; . &amp; .\\
  . &amp; . &amp; . &amp; . &amp; .\\
  1 &amp; x_N &amp; ... &amp; x_N^{N-1} &amp; x_N^N\\
     \end{array}\right)
    \left(\begin{array}{c}
  a_0 \\
  a_1 \\
  . \\
  . \\
  . \\
  a_N \\
     \end{array}\right)=
     \left(\begin{array}{c}
  y_0 \\
  y_1 \\
  . \\
  . \\
  . \\
  y_N \\
     \end{array}\right)\,,\]</span> lo cual se puede resolver con eliminación Gauss-Jordan. Este método es muy lento y por ello se usa el método de Lagrange.</p>
<p>{ étodo de Lagrange}</p>
<p>De nuevo asumimos <span class="math display">\[p(x)=a_0 + a_1 x + a_2 x^2 + ... + a_{n-1}x^{n-1} + a_n x_n=\sum\limits^{N}_{k=0} a_k x^k
=\sum^{N+1}_{i=1} y_i {\cal L}_i(x)\,,\]</span> donde <span class="math display">\[{\cal L}_i(x)=\prod^{N+1}_{\substack{k=1\\ k\ne i}} \frac{x-x_k}{x_i-x_k}\,,\]</span> son los denominados polinomios de Lagrange, y <span class="math inline">\(\prod\)</span> es el operador producto. Puesto que este operador cuando <span class="math inline">\(k\ne i\)</span> no incluye el producto, a pesar que varíe de <span class="math inline">\(1\)</span> a <span class="math inline">\(N+1\)</span>, obtendremos un polinomio de orden <span class="math inline">\(N\)</span>.</p>
<p>Esta suma de polinomios de Lagrange es el polinomio de menor grado que interpola un conjunto de datos, es decir, <span class="math display">\[p(x_j)=\sum^{N+1}_{i=1} y_i {\cal L}_i(x_j)=y_j\,.\]</span> {emostración:}</p>
<p>Los polinomios de Lagrange para <span class="math inline">\(i\ne j\)</span> son iguales a cero, y para <span class="math inline">\(i=j\)</span> son iguales a 1. Veamos esto:</p>
<p><span class="math display">\[{ (1)}\,\,\,\,\,{ Para}\,\,\,{i \ne j}:\,\,\,\,\,{\cal L}_i(x_j)=\prod^{N+1}_{\substack{k=1\\ k\ne i}} \frac{x_j-x_k}{x_i-x_k}=\]</span> <span class="math display">\[=\frac{x_j-x_1}{x_i-x_1} \frac{x_j-x_2}{x_i-x_2} ...\frac{x_j-x_j}{x_i-x_j}....\frac{x_j-x_{N+1}}{x_i-x_{N+1}}\]</span></p>
<p><span class="math display">\[{ (2)}\,\,\,\,\,{ Para}\,\,\,{i = j}:\,\,\,\,\,{\cal L}_i(x_j)=\prod^{N+1}_{\substack{k=1\\ k\ne i}} \frac{x_j-x_k}{x_j-x_k}=1\]</span></p>
<p>De modo que <span class="math display">\[{\cal L}_i(x_j)=\delta_{ij}=\begin{cases}
\begin{array}{c}
   1 \,\,\,\,\text{si}\,\,\,\,i= j\\
   0 \,\,\,\,\text{si}\,\,\,\,i\ne j\\
\end{array}
\end{cases}\,.\]</span></p>
<p>Finalmente podemos concluir entonces que <span class="math display">\[p(x_j)=\sum^{N+1}_{i=1} y_i {\cal L}_i(x_j)=\sum^{N+1}_{i=1} y_i \delta_{ij}=y_j\,.\]</span> es un polinomio de grado no mayor a <span class="math inline">\(N\)</span> y que <span class="math inline">\(p(x_j)=y_j\)</span>.</p>
<p>El polinomio se puede reescribir en terminos de la función <span class="math inline">\(Q_i\)</span> como <span class="math display">\[p(x)=\sum^{N+1}_{i=1} y_i[Q_i(x)/Q_i(x_i)]\,,\]</span> donde <span class="math display">\[Q_i(x)=(x-x_1)(x-x_2)....(x-x_{i-1})(x-x_{i+1})...(x-x_{N-1})\,,\]</span> es el producto de todas las diferencias excepto la posición <span class="math inline">\(i\)</span> (i.e., <span class="math inline">\(x-x_{i}\)</span>). Si expandemos <span class="math inline">\(p(x)\)</span> <span class="math display">\[p(x)=y_1\frac{(x-x_2)(x-x_3)...(x-x_{N+1})}{(x_1-x_2)(x_1-x_3)...(x_1-x_{N+1})} +
       y_2\frac{(x-x_1)(x-x_3)...(x-x_{N+1})}{(x_2-x_1)(x_2-x_3)...(x_2-x_{N+1})} +\]</span> <span class="math display">\[+...+y_{N+1}\frac{(x-x_1)(x-x_2)...(x-x_{N})}{(x_{N+1}-x_1)(x_{N+1}-x_2)...(x_{N+1}-x_{N})}\,.\]</span></p>
<p>{ jemplo:}</p>
<p>Considere los puntos (0,2), (1,2), (2,0) y (3,0) para los cuales queremos ajustar un polinomio de orden 3 <span class="math display">\[y(x)=2\frac{(x-1)(x-2)(x-3)}{(0-1)(0-2)(0-3)} + 2\frac{(x-0)(x-2)(x-3)}{(1-0)(1-2)(1-3)} + 0
+ 0=\]</span> <span class="math display">\[=\frac{2}{3}x^3 -3x^2 +\frac{7}{3}x +2\,.\]</span> <!--
your comment goes here
\begin{center}
%\includegraphics[width=0.7\textwidth]{Lagrange_interpolation.pdf}
\end{center}
--></p>
</section>
<section id="spline-cúbico" class="level3">
<h3 class="anchored" data-anchor-id="spline-cúbico">Spline cúbico</h3>
<p>La interpolación por spline cúbicos es un método de ajuste de polinomios de orden 3 por segmentos. Veamos este método con un ejemplo.</p>
<p>Supongamos que queremos interpolar los siguientes datos <span class="math inline">\((x_i,y_i)=[(2,-1), (3,2), (5,-7)]\)</span> con una spline cúbica. Primero definimos un polinomio cúbico para cada intervalo: <span class="math display">\[s(x)=a_1x^3 + b_1 x^2 +c_1x +d_1\,\,\,\,\,{ si}\,\,\,\,\,x\in[2,3]\]</span> <span class="math display">\[s(x)=a_2x^3 + b_2 x^2 +c_2x +d_2\,\,\,\,\,{ si}\,\,\,\,\,x\in[3,5]\]</span></p>
<p>A continuación debemos de asegurar que los polinomios pasan por los puntos del problema: <span class="math display">\[s(2)=8a_1 + 4b_1  +2c_1 +d_1=-1\]</span> <span class="math display">\[s(3)=27a_1 + 9b_1  +3c_1 +d_1=2\]</span> <span class="math display">\[s(3)=27a_2 + 9b_2  +3c_2 +d_2=2\]</span> <span class="math display">\[s(5)=125a_2 + 25b_2  +5c_2 +d_2=-7\]</span></p>
<p>Ahora calculamos la primera y segundas derivadas para cada intervalo <span class="math display">\[s'(x)=3a_1x^2+2b_1x+c_1\,\,\,\,\,{ si}\,\,\,\,\,x\in[2,3]\]</span> <span class="math display">\[s'(x)=3a_2x^2+2b_2x+c_2\,\,\,\,\,{ si}\,\,\,\,\,x\in[3,5]\]</span> <span class="math display">\[s''(x)=6a_1x+2b_1\,\,\,\,\,{ si}\,\,\,\,\,x\in[2,3]\]</span> <span class="math display">\[s''(x)=6a_2x+2b_2\,\,\,\,\,{ si}\,\,\,\,\,x\in[3,5]\]</span> y aseguramos que sean contínuas. Para ello debemos de igualar las derivadas entre intervalos de manera que no hayan discontinuidades <span class="math display">\[3a_1(3)^2+2b_1(3)+c_1=3a_2(3)^2+2b_2(3)+c_2\rightarrow 27a_1+6b_1+c_1=27a_2+6b_2+c_2\]</span> <span class="math display">\[6a_1(3)+2b_1=6a_2(3)+2b_2\rightarrow 18a_1+2_b1=18a_2+2_b2\]</span></p>
<p>En este momento tenemos 6 equaciones y 8 incógnitas. Debemos por lo tanto encontrar dos ecuaciones mas. Para ello vamos a forzar que en los extremos la segunda derivada sea nula, es decir, no haya curvatura <span class="math display">\[s''(x_0)=s''(2)=0\rightarrow 6a_1(2)+2b_1=0 \rightarrow 12a_1 + 2b_1=0\]</span> <span class="math display">\[s''(x_N)=s''(5)=0\rightarrow 6a_2(5)+2b_2=0 \rightarrow 30a_2+2b_2=0\]</span></p>
<p>Ahora ya tenemos un sistema determinado, es decir, 8 ecuaciones y 8 incógnitas <span class="math display">\[8a_1 + 4b_1  +2c_1 +d_1=-1\]</span> <span class="math display">\[27a_1 + 9b_1  +3c_1 +d_1=2\]</span> <span class="math display">\[27a_2 + 9b_2  +3c_2 +d_2=2\]</span> <span class="math display">\[125a_2 + 25b_2  +5c_2 +d_2=-7\]</span> <span class="math display">\[27a_1+6b_1+c_1=27a_2+6b_2+c_2\]</span> <span class="math display">\[18a_1+2b_1=18a_2+2b_2\]</span> <span class="math display">\[12a_1 + 2b_1=0\]</span> <span class="math display">\[30a_2+2b_2=0\]</span></p>
<p>Lo cual en notación matricial se puede escribir</p>
<p><span class="math display">\[\left(\begin{array}{cccccccc}
  8 &amp; 4 &amp; 2 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
  27 &amp; 9 &amp; 3 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
  0 &amp; 0 &amp; 0 &amp; 0 &amp; 27 &amp; 9 &amp; 3 &amp; 1\\
  0 &amp; 0 &amp; 0 &amp; 0 &amp; 125 &amp; 25 &amp; 5 &amp; 1\\
  27 &amp; 6 &amp; 1 &amp; 0 &amp; -27 &amp; -6 &amp; -1 &amp; 0\\
  18 &amp; 2 &amp; 0 &amp; 0 &amp; -18 &amp; -2 &amp; 0 &amp; 0\\
  12 &amp; 2 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
  0 &amp; 0 &amp; 0 &amp; 0 &amp; 30 &amp; 2 &amp; 0 &amp; 0\\
    \end{array}\right)\left(\begin{array}{c}
  a_1 \\
  b_1 \\
  c_1 \\
  d_1 \\
  a_2 \\
  b_2 \\
  c_2 \\
  d_2 \\
    \end{array}\right)=
    \left(\begin{array}{c}
  -1 \\
  2 \\
  2 \\
  -7 \\
  0 \\
  0 \\
  0 \\
  0 \\
    \end{array}\right)\]</span></p>
<p>Este sistema se puede resolver facilmente si la matriz de datos es invertible. Los coeficientes resultantes son <span class="math display">\[a_1=-1.25\,\,\,\,\,b_1=7.5\,\,\,\,c_1=-10.75\,\,\,\,d_1=0.5\]</span> <span class="math display">\[a_1=0.625\,\,\,\,\,b_1=-9.375\,\,\,\,c_1=39.875\,\,\,\,d_1=-50.125\,,\]</span> y los polinomios de nuestra spline cúbica es <span class="math display">\[s(x)=-1.25x^3+7.5x^2-10.75x+0.5\,\,\,\,\,{ si}\,\,\,\,\,x\in[2,3]\]</span> <span class="math display">\[s(x)=0.625x^3+-9.375x^2-39.875x+50.125\,\,\,\,\,{ si}\,\,\,\,\,x\in[3,5]\]</span></p>
<!--
your comment goes here
\begin{center}
%\centering
%\includegraphics[width=10cm,angle=0]{cubic_spline.pdf}
\end{center}
-->
<!--
your comment goes here
\begin{center}
%\centering
%\includegraphics[width=15cm,angle=0]{poly_Lagrange_Spline.pdf}
\end{center}
-->
</section>
</section>
<section id="interpolación-objetiva" class="level2">
<h2 class="anchored" data-anchor-id="interpolación-objetiva">Interpolación Objetiva</h2>
<p>Un mapa objetivo se obtiene como una regresión múltiple (donde el error cuadrático medio es mínimo) de un conjunto de observaciones discretas. Se utiliza en oceanografía para obtener mapas continuos (mallas regulares) a partir de datos discretos distribuidos irregularmente en el espacio. Las variables representadas en el mapa objetivo pueden modificarse de una realización a otra, con lo que deben de considerarse las anomalías de las variables en lugar de las variables en si mismas. Se debe de definir un ensemble (promedio), climatología, o candidato y extraerlo a cada variable para obtener anomalías. Este proceso de elección de la media es una parte delicada de la interpolación objetiva. En general, para el océano, la media es desconocida ya que solamente tenemos pocas realizaciones de nuestro muestreo. Una forma de operar es estimar la media ajustando un polinomio de bajo orden a nuestros datos discretos, extraer esta a los datos y proceder con la interpolación objetiva. La media se añade de nuevo despues de calcular el mapa objetivo de las fluctuaciones. \</p>
<p>Típicamente se debe asumir dos condiciones: \</p>
<ol type="1">
<li>El error (o ruido) asociado a la interpolación no esta correlacionado con nuestra señal (variable a interpolar) <span class="math display">\[&lt;\phi_i\epsilon_i&gt;=0\]</span> \</li>
<li>El error no esta correlacionado de una estación a la otra <span class="math display">\[&lt;\epsilon_i \epsilon_j&gt;=
\begin{cases}
\begin{array}{c}
   &lt;\epsilon^2&gt; \,\,\,\,\text{si} \,\,\,\,i=j\\
   0    \,\,\,\,        \text{si} \,\,\,\,i\ne j\\
\end{array}
\end{cases}\]</span></li>
</ol>
<p>Supongamos entonces fluctuaciones respecto un estado climatológico o media <span class="math display">\[\phi_i'=\phi_i-\bar{\phi}\,; i=1,2,...,N\]</span> Ahora vamos a intentar aproximar el valor de <span class="math inline">\(\phi'\)</span> en un punto de una malla, <span class="math inline">\(\phi_g\)</span>, en términos de una combinación lineal de los valores en estaciones vecinas <span class="math inline">\(\phi_i\)</span> (señal). Entonces el problema de mínimos cuadrados es <span class="math display">\[\phi'_g=\sum\limits^N_{i=1}b_i\phi_i'\]</span> donde <span class="math inline">\(\phi'_g\)</span> son anonmalías en la malla regular y <span class="math inline">\(\phi_i'\)</span> son anomalías en las estaciones. Los mejores coeficientes son aquellos que minimizan el error cuadrático medio, es decir,</p>
<p><span class="math display">\[SEC=\sum\limits^N_{i=1}\left(\phi'_g-\sum\limits_{i=1}^N b_i\phi'_i \right)^2
     =\sum\limits^N_{i=1}\left(\phi'_g-\sum\limits_{i=1}^N b_i\phi'_i \right)\left(\phi'_g-\sum\limits_{i=1}^N b_i\phi'_i \right)=\]</span> <span class="math display">\[\sum\limits^N_{i=1}\left[\phi'_g \phi'_g -2\sum\limits_{i=1}^N b_i\phi'_g\phi'_i
      +\sum\limits^N_{i=1}\sum\limits^N_{j=1} b_i b_j \phi'_i \phi'_j \right]=\]</span> <span class="math display">\[=\sum\limits^N_{i=1}\phi'_g \phi'_g -2 \sum\limits^N_{i=1} b_i \sum\limits^N_{i=1} \phi'_g\phi'_i + \sum\limits^N_{i=1}\sum\limits^N_{j=1}
b_i b_j \sum\limits^N_{i=1} \phi'_i \phi'_j\,.\]</span></p>
<p>El error normalizado se puede escribir como</p>
<p><span class="math display">\[\epsilon=\frac{SEC}{\sum\limits^N_{i=1}\phi'_g \phi'_g}=1-2 \sum\limits^N_{i=1} b_i \frac{\sum\limits^N_{i=1} \phi'_g\phi'_i}{\sum\limits^N_{i=1}\phi'_g \phi'_g} + \sum\limits^N_{i=1}\sum\limits^N_{j=1} b_i b_j \frac{\sum\limits^N_{i=1} \phi'_i \phi'_j}{\sum\limits^N_{i=1}\phi'_g \phi'_g}=\]</span> <span class="math display">\[=1-2 \sum\limits^N_{i=1} b_i r_{gi} + \sum\limits^N_{i=1}\sum\limits^N_{j=1} b_i b_j r_{ij}\,,\]</span></p>
<p>donde</p>
<p><span class="math display">\[r_{gi}=\frac{\sum\limits^N_{i=1} \phi'_g\phi'_i}{\sum\limits^N_{i=1}\phi'_g \phi'_g}\,\,\,\,\,{ y}\,\,\,\,\,r_{ij}=\frac{\sum\limits^N_{i=1} \phi'_i \phi'_j}{\sum\limits^N_{i=1}\phi'_g \phi'_g}\,,\]</span></p>
<p>son matrices de correlación entre el punto de malla y las estaciones, y entre las estaciones, respectivamente.</p>
<p>Si derivamos <span class="math inline">\(\epsilon\)</span> respecto los coeficientes, obtenemos la condición de minimización:</p>
<p><span class="math display">\[\frac{\partial{\epsilon}}{\partial{b_i}}=-2 r_{gi} + 2\sum\limits^N_{j=1}b_j r_{ij}=0\,\,\,\,\,\,\,\,\,i=1,2,...,N\,.\]</span> <span class="math display">\[r_{gi}=\sum\limits^N_{j=1}b_j r_{ij}\,,\]</span> y los coeficientes en notación índice son</p>
<p><span class="math display">\[b_j=(r_{ij})^{-1}r_{gi}\,\]</span></p>
<p>Finalmente, el valor de la medida en el punto de malla es</p>
<p><span class="math display">\[\phi'_g=r_{gi}(r_{ij})^{-1}\phi_i'\]</span></p>
<p>o en notación matricial para un único punto de malla</p>
<p><span class="math display">\[\phi'_g=r_{gs}(r_{ss})^{-1}\phi'\,,\]</span></p>
<p>donde <span class="math inline">\(\phi'_g\)</span> es el valor de la anomalía en un punto de malla (matriz elemento, <span class="math inline">\(1\times 1\)</span>), <span class="math inline">\(r_{gs}\)</span> es un vector fila compuesto por las correlaciones entre el punto de malla y las estaciones de medida, <span class="math inline">\(r_{ss}\)</span> es una matriz de correlaciones entre todas las estaciones, y <span class="math inline">\(\phi'\)</span> es un vector columna con las anomalías en las estaciones.</p>
<p>En el caso de que tengamos <span class="math inline">\(N\)</span> puntos de malla y <span class="math inline">\(k\)</span> estaciones de medida, las ecuaciones básicas de la interpolación objetiva se pueden escribir en notación matricial como</p>
<p><span class="math display">\[\left(\begin{array}{c}
  \phi'_{g_1} \\
  \phi'_{g_2} \\
  . \\
  . \\
  . \\
  \phi'_{g_N} \\
     \end{array}\right)=
\left(\begin{array}{ccccc}
  r_{g_11} &amp; r_{g_12} &amp;... &amp;  r_{g_1k}\\
  r_{g_21} &amp; r_{g_22} &amp;... &amp;  r_{g_2k}\\
  . &amp; . &amp; . &amp; . \\
  . &amp; . &amp; . &amp; . \\
  . &amp; . &amp; . &amp; . \\
  r_{g_N1} &amp; r_{g_N2} &amp; ... &amp; r_{g_Nk}\\
     \end{array}\right)
\left(\begin{array}{ccccc}
  r_{11} &amp; r_{12} &amp;... &amp;  r_{1k}\\
  r_{21} &amp; r_{22} &amp;... &amp;  r_{2k}\\
  . &amp; . &amp; . &amp; . \\
  . &amp; . &amp; . &amp; . \\
  . &amp; . &amp; . &amp; . \\
  r_{k1} &amp; r_{k2} &amp; ... &amp; r_{kk}\\
     \end{array}\right)^{-1}       \left(\begin{array}{c}
  \phi'_1 \\
  \phi'_2 \\
  . \\
  . \\
  . \\
  \phi'_k \\
     \end{array}\right)
     \,.\]</span> <span class="math display">\[\left( N \times 1 \right)\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
  \left( N \times k \right)\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
  \,\,\,\,\,\,\,\,\left( k \times k \right)\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
  \left( k \times 1 \right)\,\,\,\,\,\]</span> \</p>
<p>El error normalizado <span class="math inline">\(\epsilon\)</span> asociado al mapa interpolado es <span class="math display">\[\epsilon=1-2 \sum\limits^N_{j=1} b_j r_{gi} + \sum\limits^N_{i=1}\sum\limits^N_{j=1} b_i b_j r_{ij}=
           1-2 \sum\limits^N_{j=1} b_j \sum\limits^N_{j=1} b_j r_{ij} + \sum\limits^N_{i=1}\sum\limits^N_{j=1} b_i b_j r_{ij}=\]</span> <span class="math display">\[=1-2\sum\limits^N_{i=1}\sum\limits^N_{j=1} b_i b_j r_{ij} + \sum\limits^N_{i=1}\sum\limits^N_{j=1} b_i b_j r_{ij} =1-\sum\limits^N_{i=1}\sum\limits^N_{j=1} b_i b_j r_{ij}=\]</span> <span class="math display">\[= 1- \sum\limits^N_{j=1} b_j \sum\limits^N_{j=1} b_j r_{ij}  =1-\sum\limits^N_{i=1}r_{gi}b_i\,,\]</span></p>
<p>o en notación matricial para un único punto de malla <span class="math display">\[\epsilon=1-r_{gs}(r_{ss})^{-1}r^T_{gs}\,.\]</span></p>
<p>En el caso de <span class="math inline">\(N\)</span> puntos de malla y <span class="math inline">\(k\)</span> estaciones <span class="math display">\[\left(\begin{array}{ccc}
  \epsilon_{1} \\
  \epsilon_{2} \\
  . \\
  . \\
  . \\
  \epsilon_{N} \\
     \end{array}\right)=\text{Diag}\left[
     {\textbf I}-
\left(\begin{array}{ccccc}
  r_{g_11} &amp; r_{g_12} &amp;... &amp;  r_{g_1k}\\
  r_{g_21} &amp; r_{g_22} &amp;... &amp;  r_{g_2k}\\
  . &amp; . &amp; . &amp; . \\
  . &amp; . &amp; . &amp; . \\
  . &amp; . &amp; . &amp; . \\
  r_{g_N1} &amp; r_{g_N2} &amp; ... &amp; r_{g_Nk}\\
     \end{array}\right)
\left(\begin{array}{ccccc}
  r_{11} &amp; r_{12} &amp;... &amp;  r_{1k}\\
  r_{21} &amp; r_{22} &amp;... &amp;  r_{2k}\\
  . &amp; . &amp; . &amp; . \\
  . &amp; . &amp; . &amp; . \\
  . &amp; . &amp; . &amp; . \\
  r_{k1} &amp; r_{k2} &amp; ... &amp; r_{kk}\\
     \end{array}\right)^{-1}
\left(\begin{array}{ccccc}
  r_{g_11} &amp; r_{g_21} &amp;... &amp;  r_{g_N1}\\
  r_{g_12} &amp; r_{g_22} &amp;... &amp;  r_{g_N2}\\
  . &amp; . &amp; . &amp; . \\
  . &amp; . &amp; . &amp; . \\
  . &amp; . &amp; . &amp; . \\
  r_{g_1k} &amp; r_{g_2k} &amp; ... &amp; r_{g_Nk}\\
     \end{array}\right) \right]
     \,.\]</span> <span class="math display">\[\left( N \times 1 \right)\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
  \left( N \times k \right)\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
  \,\,\,\,\,\,\,\,\left( k \times k \right)\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
  \left( k \times N \right)\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\]</span></p>
<p>Este error es tal vez la característica mas importante de la interpolación objetiva. En general solo se muestra el mapa interpolado en las regiones donde <span class="math inline">\(\epsilon\)</span> emenor que un cierto valor. El error solo depende en las localizaciones de las estaciones y no en un valor particular de la medida. Es por ello que esta técnica puede ser usada para el diseño de experimentos. Es decir, nos sirve para saber cual debe ser la distribución óptima de las estaciones de medida para obtener el error mínimo en el mapa.<br>
</p>
<p><strong>Nota</strong>: Si un punto de observación, <span class="math inline">\(i=k\)</span>, coincide con un punto de malla, entonces <span class="math inline">\(r_{gk}=r_{kk}=1\)</span>, y esperamos que el método de regresión nos de <span class="math inline">\(b_k=1\)</span> y todos los demas pesos sean igual a cero. En este caso el valor interpolado en el punto de malla es igual al valor medido en la estación <span class="math inline">\(\phi'*g = r*{gk} (r\_{kk})\^{-1} \phi'\_k =1(1)\^{-1}\phi'\_k=\phi'\_k\)</span> . En este caso el error es cero, <span class="math inline">\(\epsilon=1-r_{gk}(r_{kk})^{-1}r^T_{gk}=1-1(1)^{-1}1=0\)</span>, ya que hemos asumido que los datos son perfectos. Si los puntos de las estaciones estan decorrelacionados con el punto de malla en cuestión (i.e., muy lejos de las estaciones), entonces <span class="math inline">\(b_i=0\)</span> y <span class="math inline">\(\epsilon=1\)</span>, y recuperamos la media o climatología</p>
<p><span class="math display">\[\phi'_g=\sum\limits^N_{i=1}b_i\phi'_i=0\,\,\rightarrow\,\,\phi'_g=\phi_g-\bar{\phi}=0\,\,\,\,\,\text{and}\,\,\,\,\,\phi_g=\bar{\phi}\]</span></p>
<p><strong>Error observacional</strong>:</p>
<p>Vamos asumir ahora que las mediciones en las estaciones no son perfectas, es decir,</p>
<p><span class="math display">\[\phi'_i=E[\phi'_i] + \delta_i\,.\]</span></p>
<p>Igual que anteriormente asumimos que el error de instrumentación <span class="math inline">\(\delta_i\)</span> no esta correlacionado con la señal verdadera <span class="math display">\[&lt;E[\phi_i]\delta_i&gt;=0\,,\]</span> y que el error instrumental entre estaciones tampoco esta correlacionado</p>
<p><span class="math display">\[&lt;\epsilon_i \epsilon_j&gt;=
\begin{cases}
\begin{array}{c}
   &lt;\delta^2&gt; \,\,\,\,\text{si}\,\,\,\,i=j\\
        0     \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\text{si}\,\,\,\,i\ne j\\
\end{array}
\end{cases}\]</span></p>
<p>En este caso obtenemos</p>
<p><span class="math display">\[\epsilon=1-2 \sum\limits^N_{i=1} b_i r_{gi} + \sum\limits^N_{i=1}\sum\limits^N_{j=1} b_i b_j r_{ij} + \eta\sum\limits_{i=1}^N b_i^2\,,\]</span></p>
<p>donde <span class="math inline">\(\eta\)</span> es el cociente entre la varianza del error (ruido) y la varianza de las medidas, es decir el cociente ruido-señal <span class="math display">\[\eta=\frac{&lt;\delta^2&gt;}{&lt;\phi'_g\phi'_g&gt;}\,.\]</span></p>
<p>La minimización del error nos da la condición <span class="math display">\[r_{gi}=\sum\limits^N_{j=1}b_j r_{ij}+\eta b_i\,,\]</span> y los coeficientes en notación índice son <span class="math display">\[b_j=(r_{ij} + \eta I_{ij})^{-1}r_{gi}\,,\]</span> donde <span class="math inline">\(I_{ij}\)</span> es la matriz identidad. De esta expresión se deduce que cuando <span class="math inline">\(\eta\)</span> es grande (mucho ruido en la medida) entonces los coeficientes son mas pequeños respecto al caso de observaciones perfectas. Consecuentemente nuestro mapa interpolado se acerca mas a la climatología ya que la anomalía es menor para coeficientes mas pequeños</p>
<p><span class="math display">\[\downarrow \sum\limits^N_{i=1}b_i\phi'_i \,\,\,\,\,\rightarrow \,\,\,\,\,\downarrow\phi'_g\,\,\,\,\,\,\text{y}\,\,\,\,\,\phi_g\rightarrow\bar{\phi}\,.\]</span></p>
<p>Incluyendo errores observacionales, la interpolación objetiva tendera a la climatología o media y las nuevas observaciones serán incluidas pero con menos pesos. También podemos añadir diferentes errores ruido-señal en la diagonal principal para darle menos peso a las estaciones de medida que tienen mas incertidumbre asociada. Es conveniente entonces añadir errores observacionales al esquema de interpolación objetiva. Otra razón para ello es el caso en que existan dos estaciones que coincidan exactamente con un punto de malla. En este caso, si no hemos añadido error observacional la matriz de correlaciones entre las estaciones se convierte singular y el esquema de interpolación objetiva no se puede resolver. Por ejemplo para la interpolación en un único punto de malla a partir de dos estaciones de medida, la matriz de correlaciones sería singular</p>
<p><span class="math display">\[r_{ss}=
       \left(\begin{array}{cc}
  1 &amp; 1 \\
  1 &amp; 1
     \end{array}\right)\,.\]</span></p>
<p>La anomalía interpolada en notación matricial en un punto de malla es ahora <span class="math display">\[\phi'_g=r_{gs}(r_{ss}+\eta I)^{-1}\phi'\,,\]</span></p>
<p>y el error asociado al mapa interpolado en notación matricial en un único punto de malla es</p>
<p><span class="math display">\[\epsilon=1-r_{gs}(r_{ss}+\eta I)^{-1}r^T_{gs}\,.\]</span></p>
<p>Para el caso de <span class="math inline">\(N\)</span> puntos de malla y <span class="math inline">\(k\)</span> estaciones obtenemos el mismo sistema que para el caso de medidas perfectas pero añadiendo el error ruido-señal en la diagonal principal de la matriz de correlaciones entre estaciones</p>
<p><span class="math display">\[\left(\begin{array}{ccc}
  \epsilon_{1} \\
  \epsilon_{2} \\
  . \\
  . \\
  . \\
  \epsilon_{N} \\
     \end{array}\right)=\text{Diag}\left[
     {\textbf I}-
\left(\begin{array}{ccccc}
  r_{g_11} &amp; r_{g_12} &amp;... &amp;  r_{g_1k}\\
  r_{g_21} &amp; r_{g_22} &amp;... &amp;  r_{g_2k}\\
  . &amp; . &amp; . &amp; . \\
  . &amp; . &amp; . &amp; . \\
  . &amp; . &amp; . &amp; . \\
  r_{g_N1} &amp; r_{g_N2} &amp; ... &amp; r_{g_Nk}\\
     \end{array}\right)
\left(\begin{array}{ccccc}
  r_{11}+\eta &amp; r_{12} &amp;... &amp;  r_{1k}\\
  r_{21} &amp; r_{22}+\eta &amp;... &amp;  r_{2k}\\
  . &amp; . &amp; . &amp; . \\
  . &amp; . &amp; . &amp; . \\
  . &amp; . &amp; . &amp; . \\
  r_{k1} &amp; r_{k2} &amp; ... &amp; r_{kk}+\eta\\
     \end{array}\right)^{-1}
\left(\begin{array}{ccccc}
  r_{g_11} &amp; r_{g_21} &amp;... &amp;  r_{g_N1}\\
  r_{g_12} &amp; r_{g_22} &amp;... &amp;  r_{g_N2}\\
  . &amp; . &amp; . &amp; . \\
  . &amp; . &amp; . &amp; . \\
  . &amp; . &amp; . &amp; . \\
  r_{g_1k} &amp; r_{g_2k} &amp; ... &amp; r_{g_Nk}\\
     \end{array}\right) \right]
     \,.\]</span></p>
<p><span class="math display">\[\left( N \times 1 \right)\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
  \left( N \times k \right)\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
  \,\,\,\,\,\,\,\,\left( k \times k \right)\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
  \left( k \times N \right)\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\]</span></p>
</section>
<section id="funciones-empíricas-ortogonales-feos" class="level2">
<h2 class="anchored" data-anchor-id="funciones-empíricas-ortogonales-feos">Funciones Empíricas Ortogonales (FEOs)</h2>
<section id="interpretación-de-los-sistemas-propios" class="level3">
<h3 class="anchored" data-anchor-id="interpretación-de-los-sistemas-propios">Interpretación de los sistemas `propios’</h3>
<p>Antes de entrar en la teoría para las FEOs, vamos a ver que los vectores propios son equivalentes a modos de oscilación de sistemas físicos. Imaginemos oscilaciones verticales de bolas en una cuerda. Las bolas tienen masa <span class="math inline">\(m\)</span> y estan separadas por cuerdas elásticas de longitud <span class="math inline">\({\cal d}\)</span> en el equilibrio. Supongamos que los desplazamientos <span class="math inline">\(y_n\)</span> son tan pequeños que la tensión de la cuerda <span class="math inline">\({\textbf T}\)</span> se puede considerar constante. El ángulo de cada cuerda es <span class="math inline">\(\theta\)</span> como se ilustra en la figura. Entonces, la ecuación del movimiento para la bola <span class="math inline">\(n\)</span> es</p>
<p><span class="math display">\[m\frac{d^2y_n}{dt^2}=-T sen\theta_{n-1} - T sen\theta_{n}\,.\]</span></p>
<p>Bajo la suposición de que los desplazamientos son pequeños, el <span class="math inline">\(sen\theta_n=tan\theta_n\)</span>, es decir, <span class="math inline">\(sen\theta_{n-1}=tan\theta_{n-1}=y_n-y_{n-1}/d\)</span> y <span class="math inline">\(sen\theta_n=tan\theta_n=(y_n-y_{n+1})/d\)</span>. Entonces la ecuación queda <span class="math display">\[m\frac{d^2y_n}{dt^2}=-T\frac{y_n-y_{n-1}}{d} - T\frac{y_n-y_{n+1}}{d}\,,\]</span> y si reagrupamos <span class="math display">\[m\frac{d^2y_n}{dt^2}=\frac{T}{d}(y_{n-1}-2y_n+y_{n+1})\,.\]</span> Vamos ahora a substituir una solución oscilatoria del tipo <span class="math display">\[y_n=Y_n e^{i\omega t}\]</span> en la ecuación del movimiento <span class="math display">\[-m\omega^2 Y_n e^{i\omega t}=\frac{T}{d}(Y_{n-1} e^{i\omega t}-2Y_n e^{i\omega t}+Y_{n+1} e^{i\omega t})\]</span> <span class="math display">\[\frac{-m\omega^2 d}{T} Y_n e^{i\omega t}=\frac{T}{d}(Y_{n-1} e^{i\omega t}-2Y_n e^{i\omega t}+Y_{n+1} e^{i\omega t})\,.\]</span> Si definimos <span class="math display">\[\lambda=\frac{m\omega^2 d}{T}\,,\]</span> el sistema de ecuaciones a resolver es <span class="math display">\[-\lambda Y_n=(Y_{n-1}-2Y_n +Y_{n+1})\]</span> <span class="math display">\[Y_n(2-\lambda)-Y_{n-1}-Y_{n+1}=0\,,\]</span> con las condiciones de frontera <span class="math inline">\(Y_0=Y_{n+1}=0\)</span> en las paredes.</p>
<p>Vamos a suponer ahora el caso de dos bolas. Para la primera bola <span class="math inline">\(n=1\)</span> <span class="math display">\[Y_1(2-\lambda)-Y_0-Y_2=0\,,\]</span> y para la bola <span class="math inline">\(n=2\)</span> <span class="math display">\[Y_2(2-\lambda)-Y_1-Y_3=0\,.\]</span> Si aplicamos las condiciones de frontera <span class="math inline">\(Y_0=Y_3=0\)</span>, nos queda uns sistema “propio”, donde <span class="math inline">\(\lambda\)</span> son los autovalores.</p>
<p><span class="math display">\[\left|\begin{array}{cc}
  2-\lambda &amp; -1 \\
   -1 &amp; 2-\lambda\\
        \end{array}\right|=0\,.\]</span></p>
<p>El polinomio característico es</p>
<p><span class="math display">\[\lambda^2-4\lambda+3=0\,,\]</span></p>
<p>y tienes las raíces <span class="math inline">\(\lambda_1=1\)</span> y <span class="math inline">\(\lambda_2=3\)</span>.</p>
<p>Resolvamos para los vectores propios:\</p>
<ol type="1">
<li><span class="math inline">\(\lambda_1=1\)</span> entonces</li>
</ol>
<p><span class="math display">\[Y_1-Y_2=0\]</span></p>
<p>y el vector propio es</p>
<p><span class="math display">\[{\textbf y}=\frac{1}{\sqrt{2}}\left(\begin{array}{c}
   1 \\
   1 \\
        \end{array}\right)\]</span> \</p>
<ol start="2" type="1">
<li><span class="math inline">\(\lambda_2=3\)</span> entonces</li>
</ol>
<p><span class="math display">\[-Y_1-Y_2=0\,,\]</span></p>
<p>y el vector propio es</p>
<p><span class="math display">\[{\textbf y}=\frac{1}{\sqrt{2}}\left(\begin{array}{c}
   1 \\
   -1 \\
        \end{array}\right)\,.\]</span></p>
<p>Estas soluciones representan los modos oscilatorios de un sistema físico de dos bolas oscilando verticalmente. Los modos de oscilación se muestran en la figura. Los modos oscilan independientemente uno de otro, y la evolución del sistema es una combinación lineal de los dos modos. De esta forma, lo que estamos haciendo al resolver el problema de las bolas en una cuerda elástica es precisamente la solución de un sistema “propio”.</p>
</section>
<section id="definición-de-feos" class="level3">
<h3 class="anchored" data-anchor-id="definición-de-feos">Definición de FEOs</h3>
<p>Un análisis en FEOs busca estructuras en los datos que explican la mayor cantidad de varianza de un conjunto de datos bidimensional. La primera dimensión es la dimension en la que deseamos encontrar una estructura, y la otra dimension es la dimensión en la que se muestrean las diferentes realizaciones. Por ejemplo, un conjunto de series temporales de datos distribuidos espacialmente (i.e.&nbsp;arreglo de anclajes). La primera dimensión es espacio y la segunda es tiempo. Las estructuras en la dimensión espacial son las FEOs, mientras que las estructuras en la dimensión de muestreo se denominan Componentes Principales (CPs). \ Tanto las FEOs como las PCs son ortogonales en sus dimensiones. Las FEOs/PCs pueden entenderse de diferentes formas: \ (i) Transforma variables correlacionadas en un conjunto de variables no correlacionadas que expresan mejor la relación dinámica entre los datos originales. \ (ii) Identifica y ordena los vectores ortogonales (o dimensiones) a lo largo de los cuales nuestro conjunto de datos presenta la mayor varianza. \ (iii) Una vez definidas las FEOs, es posible encontrar la mejor aproximación de los datos originales con el mínimo número de vectores ortogonales.</p>
<p>En general, aplicaremos el análisis en FEOs para describir de manera mas sencilla conjuntos de datos organizados en matrices <span class="math inline">\(M\times N\)</span>:}<br>
</p>
<ol type="1">
<li><p>Una matriz espacio-tiempo: Medidas de una variable en <span class="math inline">\(M\)</span> localizaciones y <span class="math inline">\(N\)</span> tiempos.<br>
</p></li>
<li><p>Una matriz parámetro-tiempo: Medidas de <span class="math inline">\(M\)</span> variables en una localización y <span class="math inline">\(N\)</span> tiempos.<br>
</p></li>
<li><p>Una matriz parámetro-espacio: Medidas de <span class="math inline">\(M\)</span> variables tomadas en <span class="math inline">\(N\)</span> localizaciones al mismo tiempo.<br>
</p></li>
</ol>
<p><strong>Nota</strong>: Un error común es considerar que las FEOs se corresponden con modos de físicos oceánicos. Eso no es cierto!. Los modos físicos en el océano son modos de oscilación que se obtienen considerando las ecuaciones que rigen el movimiento y condiciones de frontera; las FEOs son simplemente funciones ortogonales que explican la mayor cantidad de varianza de un conjunto de datos.</p>
<p>Aunque los procesos físicos dominantes son representados por los primeros modos de oscilación, no existe una correspondencia uno a uno entre modos físicos y FEOs.</p>
</section>
<section id="teoría" class="level3">
<h3 class="anchored" data-anchor-id="teoría">Teoría</h3>
<p>Supongamos M localizaciones de medición con series temporales de temperatura de N elementos. Queremos descomponer la serie temporal de temperatura en una localización dada <span class="math inline">\(k\)</span> como una combinación lineal de <span class="math inline">\(M\)</span> funciones ortogonales <span class="math inline">\({\textbf b}_i\)</span> cuyas amplitudes son pesadas con <span class="math inline">\(M\)</span> coeficientes dependientes del tiempo, es decir,</p>
<p><span class="math display">\[{\textbf T}(t)=\sum\limits^M_{i=1}[\alpha_i(t){\textbf b}_{i}]\,,\]</span></p>
<p>donde <span class="math inline">\(\alpha_i(t)\)</span> es la amplitud del modo ortogonal <span class="math inline">\(i\)</span> al tiempo <span class="math inline">\(t=t_n(1\le n\le N)\)</span>. Los coeficientes <span class="math inline">\(\alpha_i(t)\)</span> nos informan de como varian los modos <span class="math inline">\({\textbf b}_{i}\)</span> con el tiempo. Necesitamos tantas funciones ortogonales como estaciones con series temporales tenemos para poder describir la varianza total de los datos originales de temperatura a cada tiempo. Sin mebargo, en términos prácticos podemos explicar una gran cantidad de varianza de los datos originales con las primeras FEOs. Podemos ver el problema al revés, es decir, tenemos <span class="math inline">\(N\)</span> funciones temporales cuyas amplitudes son pesadas por <span class="math inline">\(M\)</span> coeficientes que varian en el espacio. En este caso hablamos de PCs. Ya sea la reducción de los datos en funciones ortogonales espaciales (FEOs) o temporales (PCs), obtenemos los mismos resultados.</p>
<p>Puesto que queremos <span class="math inline">\({\textbf b}_{i}\)</span> ser ortogonal, requerimos <span class="math display">\[{\textbf b}^T_i {\textbf b}_{j}=\delta_{ij}\,,\]</span> y los coeficientes temporales <span class="math inline">\(\alpha_i={\textbf b}^T_{i}{\textbf T}(t)\)</span>. Son precisamente estos coeficientes temporales las CPs, es decir, la proyección de los datos originales sobre las FEOs o la expresión de los datos originales en la nueva base de vectores ortogonales (el nuevo sistema de coordenadas).</p>
<p>El objetivo del análisis es encontrar una base de vectores ortogonales tal que las funciones <span class="math inline">\(\alpha_i(t)\)</span> no esten correlacionadas <span class="math display">\[&lt;\alpha_i \alpha_j&gt;=&lt;{\textbf b}^T_{i}{\textbf T}{\textbf b}^T_{j}{\textbf T}&gt;=&lt;{\textbf b}^T_{i}{\textbf T}{\textbf T}^T{\textbf b}_{j}&gt;={\textbf b}^T_{i}&lt;{\textbf T} {\textbf T}^T&gt;{\textbf b}_{j}=\delta_{ij}&lt;\alpha^2_i&gt;\,,\]</span> donde <span class="math display">\[&lt;\alpha_i^2&gt;=\frac{1}{N}\sum\limits^N_{n=1}=\alpha^2_i(t_n)\,.\]</span> Es decir, la matriz de covarianza de <span class="math inline">\(\alpha_i(t)\)</span> será una matriz diagonal <span class="math inline">\({\textbf D}\)</span>. Para <span class="math inline">\(M\)</span> posiciones se puede reescribir como</p>
<p><span class="math display">\[{\textbf B}^T&lt;{\textbf T}{\textbf T}^T&gt;{\textbf B}={\textbf D}\,\,\,\,\,\,\,\,\,{ o}\,\,\,\,\,\,\,\,{\textbf B}^T{\textbf T}{\textbf T}^T{\textbf B}=N{\textbf D}\]</span></p>
<p>donde <span class="math inline">\({\textbf B}\)</span> es una matriz ortogonal cuyas columnas son los vectores ortogonales <span class="math inline">\({\textbf b}_i\)</span> y <span class="math inline">\({\textbf D}\)</span> es una matriz diagonal compuesta por las varianzas de las funciones temporales <span class="math inline">\(\alpha_i(t)\)</span>. Si multiplicamos por <span class="math inline">\({\textbf B}\)</span> llegamos a un sistema propio</p>
<p><span class="math display">\[&lt;{\textbf T}{\textbf T}^T&gt;{\textbf B}={\textbf B}{\textbf D}\,.\]</span></p>
<p>Este tipo de sistema propio es conocido. La diagonal de <span class="math inline">\({\textbf D}\)</span> esta compuesta por valores propios y las columnas de <span class="math inline">\({\textbf B}\)</span> son los vectores propios. Los vectores propios son denominados FEOs, y los valores propios son las varianzas de las amplitudes <span class="math inline">\(\alpha_i\)</span>. Básicamente hemos realizado una transformación de coordenadas de tal forma que los vectores propios <span class="math inline">\({\textbf b}_i\)</span></p>
<p>indican combinaciones lineales de datos que no estan correlacionados (i.e., <span class="math inline">\(&lt;\alpha_i \alpha_j&gt;=\delta_{ij}&lt;\alpha^2_i&gt;\)</span>). Esta descomposición de los datos en FEOs es óptima en el sentido de mínimos cuadrados. Imaginemos que queremos un conjunto de <span class="math inline">\(K\)</span> vectores que mejor aproxima <span class="math inline">\({\textbf T}\)</span></p>
<p><span class="math display">\[&lt;(\hat{\textbf T}-{\textbf T})^T(\hat{\textbf T}-{\textbf T})&gt;=&lt;{\textbf T}^T{\textbf T}&gt;
  -\sum\limits^K_{i=1}&lt;\alpha^2_i&gt;\,.\]</span></p>
<p>El problema en mínimos cuadrados, bajo las restricciones que las funciones <span class="math inline">\({\textbf b}_i\)</span> sean ortogonales, se puede escribir a partir de los multiplicadores de Lagrange</p>
<p><span class="math display">\[{\textbf \cal L}=\sum\limits^K_{i=1}\left[{\textbf b}^T_i&lt;{\textbf T}{\textbf T}^T&gt;{\textbf b}_i-
            \lambda_i({\textbf b}_i^T{\textbf b}_i-1)\right]\,.\]</span></p>
<p>Si derivamos <span class="math inline">\({\textbf \cal L}\)</span> respecto de <span class="math inline">\({\textbf b}_i\)</span> obtenemos el sistema propio</p>
<p><span class="math display">\[&lt;{\textbf T}{\textbf T}^T&gt;{\textbf b}_i=\lambda_i{\textbf b}_i\,.\]</span></p>
<p>De este análisis deducimos que las primeras <span class="math inline">\(K\)</span> funciones ortogonales o FEOs son las mejores funciones que explican la máxima varianza de los datos originales, donde los valores porpios estan ordenados de mayor a menor. Es decir, no existe un subset de datos mas pequeño que <span class="math inline">\(K\)</span> funciones ortogonales que produce el error cuadrático medio menor. En este sentido las FEOs son los mejores `descriptores’ de la varianza de los datos.</p>
<p>Una matriz de datos de <span class="math inline">\(M\)</span> localizaciones y <span class="math inline">\(N\)</span> tiempos se puede descomponer</p>
<p><span class="math display">\[{\textbf C}{\textbf B}={\textbf B}{\textbf \Lambda}\,,\]</span></p>
<p>donde</p>
<p><span class="math display">\[{\textbf C}={\textbf T}{\textbf T}^T=\left(\begin{array}{cccc}
   \sum\limits_{i=1}^N{T_{1}(t_i)T_{1}(t_i)} &amp; \sum\limits_{i=1}^N{T_{1}(t_i)T_{2}(t_i)} &amp; ... &amp; \sum\limits_{i=1}^N{T_{1}(t_i)T_{M}(t_i)}\\
   \sum\limits_{i=1}^N{T_{2}(t_i)T_{1}(t_i)} &amp; \sum\limits_{i=1}^N{T_{2}(t_i)T_{2}(t_i)} &amp; ... &amp; \sum\limits_{i=1}^N{T_{2}(t_i)T_{M}(t_i)}\\
                . &amp; . &amp; . \\
        . &amp; . &amp; . \\
        . &amp; . &amp; . \\
   \sum\limits_{i=1}^N{T_{M}(t_i)T_{1}(t_)} &amp; \sum\limits_{i=1}^N{T_{M}(t_i)T_{2}(t_i)} &amp; ... &amp; \sum\limits_{i=1}^N{T_{M}(t_i)T_{M}(t_i)}\\
        \end{array}\right)\,,\]</span></p>
<p>es la matriz de covarianza entre las series temporales de temperatura en localizaciones espaciales, unas con otras;</p>
<p><span class="math display">\[{\textbf T}=\left(\begin{array}{cccc}
   T_{1}(t_1) &amp; T_{1}(t_2) &amp; ... &amp; T_{1}(t_N)\\
   T_{2}(t_1) &amp; T_{2}(t_2) &amp; ... &amp; T_{2}(t_N)\\
                . &amp; . &amp; . \\
        . &amp; . &amp; . \\
        . &amp; . &amp; . \\
   T_{M}(t_1) &amp; T_{M}(t_2) &amp; ... &amp; T_{M}(t_N)\\
        \end{array}\right)\]</span> es la</p>
<p>matriz de datos de temperatura;</p>
<p><span class="math display">\[{\textbf B}=\left(\begin{array}{cccc}
   b_{11} &amp; b_{12} &amp; ... &amp; b_{1M}\\
   b_{21} &amp; b_{22} &amp; ... &amp; b_{2M}\\
                . &amp; . &amp; . \\
        . &amp; . &amp; . \\
        . &amp; . &amp; . \\
   b_{M1} &amp; b_{M2} &amp; ... &amp; b_{MM}\\
        \end{array}\right)\,,\]</span> es la matriz con los vectores propios <span class="math inline">\({\textbf b}_i\)</span> como columnas, y <span class="math display">\[{\textbf \Lambda}=\left(\begin{array}{cccc}
   \lambda_1 &amp; 0 &amp; ... &amp; 0\\
  0 &amp; \lambda_2 &amp; ... &amp;0\\
                . &amp; . &amp; . \\
        . &amp; . &amp; . \\
        . &amp; . &amp; . \\
   0 &amp; 0 &amp; ... &amp; \lambda_M\\
        \end{array}\right)\,,\]</span></p>
<p>es la matriz diagonal compuesta por los valores propios <span class="math inline">\(\lambda_i=&lt;\alpha_i^2&gt;\)</span><br>
</p>
<p>Otra forma de entender las FEOs (PCs) consiste en un análisis en valores propios de las matrices de dispersión de nuestras matrices de datos. La matriz de dispersión es el producto matricial de la matriz con su transpuesta, o a la inversa. La primera matriz de dispersión es</p>
<p><span class="math display">\[{\textbf C}={\textbf T}{\textbf T}^T=\left(\begin{array}{cccc}
   T_{1}(t_1) &amp; T_{1}(t_2) &amp; ... &amp; T_{1}(t_N)\\
   T_{2}(t_1) &amp; T_{2}(t_2) &amp; ... &amp; T_{2}(t_N)\\
                . &amp; . &amp; . \\
        . &amp; . &amp; . \\
        . &amp; . &amp; . \\
   T_{M}(t_1) &amp; T_{M}(t_2) &amp; ... &amp; T_{M}(t_N)\\
        \end{array}\right)
\left(\begin{array}{cccc}
   T_{1}(t_1) &amp; T_{2}(t_1) &amp; ... &amp; T_{M}(t_1)\\
   T_{1}(t_2) &amp; T_{2}(t_2) &amp; ... &amp; T_{M}(t_2)\\
                . &amp; . &amp; . \\
        . &amp; . &amp; . \\
        . &amp; . &amp; . \\
   T_{1}(t_N) &amp; T_{2}(t_N) &amp; ... &amp; T_{M}(t_N)\\
        \end{array}\right)=\]</span></p>
<p><span class="math display">\[=\left(\begin{array}{cccc}
   \sum\limits_{i=1}^N{T_{1}(t_i)T_{1}(t_i)} &amp; \sum\limits_{i=1}^N{T_{1}(t_i)T_{2}(t_i)} &amp; ... &amp; \sum\limits_{i=1}^N{T_{1}(t_i)T_{M}(t_i)}\\
   \sum\limits_{i=1}^N{T_{2}(t_i)T_{1}(t_i)} &amp; \sum\limits_{i=1}^N{T_{2}(t_i)T_{2}(t_i)} &amp; ... &amp; \sum\limits_{i=1}^N{T_{2}(t_i)T_{M}(t_i)}\\
                . &amp; . &amp; . \\
        . &amp; . &amp; . \\
        . &amp; . &amp; . \\
   \sum\limits_{i=1}^N{T_{M}(t_i)T_{1}(t_i)} &amp; \sum\limits_{i=1}^N{T_{M}(t_i)T_{2}(t_i)} &amp; ... &amp; \sum\limits_{i=1}^N{T_{M}(t_i)T_{M}(t_i)}\\
        \end{array}\right)\,,\]</span></p>
<p><span class="math display">\[\left( M \times M \right)\]</span></p>
<p>y la segunda es</p>
<p><span class="math display">\[{\textbf C}={\textbf T}^T{\textbf T}=
\left(\begin{array}{cccc}
   T_{1}(t_1) &amp; T_{2}(t_1) &amp; ... &amp; T_{M}(t_1)\\
   T_{1}(t_2) &amp; T_{2}(t_2) &amp; ... &amp; T_{M}(t_2)\\
                . &amp; . &amp; . \\
        . &amp; . &amp; . \\
        . &amp; . &amp; . \\
   T_{1}(t_N) &amp; T_{2}(t_N) &amp; ... &amp; T_{M}(t_N)\\
        \end{array}\right)\left(\begin{array}{cccc}
   T_{1}(t_1) &amp; T_{1}(t_2) &amp; ... &amp; T_{1}(t_N)\\
   T_{2}(t_1) &amp; T_{2}(t_2) &amp; ... &amp; T_{2}(t_N)\\
                . &amp; . &amp; . \\
        . &amp; . &amp; . \\
        . &amp; . &amp; . \\
   T_{M}(t_1) &amp; T_{M}(t_2) &amp; ... &amp; T_{M}(t_N)\\
\end{array}\right)=\]</span></p>
<p><span class="math display">\[=\left(\begin{array}{cccc}
   \sum\limits_{i=1}^M{T_{i}(t_1)T_{i}(t_1)} &amp; \sum\limits_{i=1}^M{T_{i}(t_1)T_{i}(t_2)} &amp; ... &amp; \sum\limits_{i=1}^i{T_{1}(t_1)T_{i}(t_N)}\\
   \sum\limits_{i=1}^M{T_{i}(t_2)T_{i}(t_1)} &amp; \sum\limits_{i=1}^M{T_{i}(t_2)T_{i}(t_2)} &amp; ... &amp; \sum\limits_{i=1}^M{T_{i}(t_2)T_{i}(t_N)}\\
                . &amp; . &amp; . \\
        . &amp; . &amp; . \\
        . &amp; . &amp; . \\
   \sum\limits_{i=1}^M{T_{i}(t_N)T_{i}(t_1)} &amp; \sum\limits_{i=1}^M{T_{i}(t_N)T_{i}(t_2)} &amp; ... &amp; \sum\limits_{i=1}^M{T_{i}(t_N)T_{i}(t_N)}\\
        \end{array}\right)\,.\]</span> <span class="math display">\[\left( N \times N \right)\]</span></p>
<p>Ambas matrices de dispersión obtenidas del producto de la matriz de datos consigo misma son matrices de covarianza simétricas. La primera matriz de dispersión <span class="math inline">\({\textbf T}{\textbf T}^T\)</span> es una matriz <span class="math inline">\(M \times M\)</span> con lo que hemos eliminado la dimensión temporal o dimensión de muestreo. En este caso, la matriz de dispersión es una matriz de covarianzas temporales (determinadas por sus variaciones temporales) de las estaciones unas con otras. En la segunda matriz <span class="math inline">\({\textbf T}^T{\textbf T}\)</span> se invierten las dimensiones, la matriz resultante es <span class="math inline">\(N\times N\)</span>, y es una matriz de covarianzas espaciales (determinadas por sus variaciones espaciales) entre los diferenetes tiempos.<br>
</p>
<p><strong>Ejemplo</strong>: Un ejmplo sencillo viene dado por el diagrama de dispersión de la figura. La primera EOF o vector propio que explica la mayor varianza sería la recta que se ajusta al conjunto de puntos y la segunda EOF sería la línea perpendicular a la ajustada.</p>
<p><strong>Relación entre FEOs y CPs</strong></p>
<p>Como ya hemos visto nuestro sistema propio es</p>
<p><span class="math display">\[{\textbf B}^T&lt;{\textbf T}{\textbf T}^T&gt;{\textbf B}={\textbf D}\,\,\,\,\,\,\,\,\,{ o}\,\,\,\,\,\,\,\,{\textbf B}^T{\textbf T}{\textbf T}^T{\textbf B}=N{\textbf D}\,,\]</span></p>
<p>donde <span class="math inline">\({\textbf B}\)</span> es una matriz cuyas columnas son los vectores propios y <span class="math inline">\({\textbf D}\)</span> es una matriz cuadrada con los <span class="math inline">\(M\)</span> valores propios en la diagonal principal. Si queremos expresar los datos originales en términos de los vectores propios, entonces debemos usar la definición</p>
<p><span class="math display">\[\alpha_i(t)={\textbf b}^T_{i}{\textbf T}(t)\,,\]</span></p>
<p>que en notación matricial se puede expresar como</p>
<p><span class="math display">\[{\textbf Z}={\textbf B}^T{\textbf T}\,,\]</span></p>
<p>y finalmente para recuperar los datos originales a partir de la base de FEOs usamos</p>
<p><span class="math display">\[{\textbf T}={\textbf B}{\textbf Z}\,,\]</span></p>
<p>ya que <span class="math inline">\({\textbf B}{\textbf B}^T={\textbf I}\)</span>. La matriz <span class="math inline">\({\textbf Z}\)</span> contiene los vectores de las CPs, que no son mas que las amplitudes por las cuales multiplicamos las FEOs para obtener los datos originales de vuelta. De esta forma podemos ir de un espacio (vectores propios) al otro (datos originales) con la matriz de FEOs. Supongamos que tenemos un conjunto de vectores propios ortogonales y normalizados. El primero de ellos por ejemplo sería</p>
<p><span class="math display">\[{\textbf e}=\left(\begin{array}{c}
   e_{11} \\
   e_{21} \\
         .\\
         .\\
     .\\
   e_{M1} \\
        \end{array}\right)\,.\]</span></p>
<p>Si ponemos todos los vectores propios en columna obtenemos la matriz cuadrada <span class="math inline">\({\textbf B}\)</span>, la cual es ortonormal <span class="math inline">\({\textbf B}^T{\textbf B}={\textbf I}\)</span>. Si queremos proyectar un vector propio sobre los datos originales y obtener la amplitud de este vector propio en cada tiempo, debemos de hacer</p>
<p><span class="math display">\[{\textbf e}^T{\textbf T}=\left(\begin{array}{ccccccc}
   e_{11} &amp;
   e_{21} &amp; . &amp; . &amp; . &amp; e_{M1} \\
        \end{array}\right)
    \left(\begin{array}{cccc}
   T_{1}(t_1) &amp; T_{1}(t_2) &amp; ... &amp; T_{1}(t_N)\\
   T_{2}(t_1) &amp; T_{2}(t_2) &amp; ... &amp; T_{2}(t_N)\\
                . &amp; . &amp; . \\
        . &amp; . &amp; . \\
        . &amp; . &amp; . \\
   T_{M}(t_1) &amp; T_{M}(t_2) &amp; ... &amp; T_{M}(t_N)\\
        \end{array}\right)=
    \left(\begin{array}{ccccccc}
   z_{11} &amp;
   z_{12} &amp; . &amp; . &amp; . &amp; z_{1N} \\
        \end{array}\right)\,,\]</span></p>
<p>donde, por ejemplo, <span class="math inline">\(z_{11}=e_{11}T_1(t_1)+ e_{21}T_2(t_1) + ... + e_{M1}T_M(t_1)\,.\)</span> Si hacemos lo mismo para todos los otros vectores propios, obtenemos series temporales de longitud <span class="math inline">\(N\)</span> para cada FEO, lo cual se denominan componentes principales de cada EOF</p>
<p><span class="math display">\[{\textbf Z}={\textbf B}^T{\textbf T}\,.\]</span> <span class="math display">\[M\times N\]</span></p>
<p>Las CPs también son ortogonales. Si substituimos <span class="math inline">\({\textbf T}={\textbf B}{\textbf Z}\)</span>, en <span class="math inline">\({\textbf B}^T{\textbf T}{\textbf T}^T{\textbf B}=N{\textbf D}\)</span> obtenemos</p>
<p><span class="math display">\[{\textbf B}^T{\textbf B}{\textbf Z}({\textbf B}{\textbf Z})^T{\textbf B}=N{\textbf D}\]</span></p>
<p><span class="math display">\[{\textbf I}{\textbf Z}{\textbf Z}^T{\textbf B}^T{\textbf B}=
N{\textbf D}\]</span></p>
<p><span class="math display">\[{\textbf I}{\textbf Z}{\textbf Z}^T{\textbf I}=N{\textbf D}\]</span></p>
<p><span class="math display">\[{\textbf Z}{\textbf Z}^T=N{\textbf D}\,\,\,\,\,\,{ o}\,\,\,\,\,\,&lt;{\textbf Z}{\textbf Z}^T&gt;={\textbf D}\,.\]</span></p>
<p>Por lo tanto no solo las FEOs sino también las CPs son ortogonales.</p>
<p><strong>Equivalencia con descomposición en valores singulares (SVD)</strong></p>
<p>Supongamos la matriz de datos compuesta por <span class="math inline">\(M\)</span> series temporales de temperatura de longitud <span class="math inline">\(N\)</span> <span class="math display">\[{\textbf T}=\left(\begin{array}{cccc}
   T_{1}(t_1) &amp; T_{1}(t_2) &amp; ... &amp; T_{1}(t_N)\\
   T_{2}(t_1) &amp; T_{2}(t_2) &amp; ... &amp; T_{2}(t_N)\\
                . &amp; . &amp; . \\
        . &amp; . &amp; . \\
        . &amp; . &amp; . \\
   T_{M}(t_1) &amp; T_{M}(t_2) &amp; ... &amp; T_{M}(t_N)\\
        \end{array}\right)\,.\]</span></p>
<p>La matriz de dispersión es <span class="math display">\[&lt;{\textbf T}{\textbf T}^T&gt;=\frac{1}{N} {\textbf T}{\textbf T}^T\,.\]</span></p>
<p>Sabemos que la matriz <span class="math inline">\({\textbf T}\)</span> puede descomponerse en una matriz ortogonal <span class="math inline">\({\textbf U}\)</span>, una matriz diagonal <span class="math inline">\({\textbf S}\)</span>, y la transpuesta de una matriz ortogonal <span class="math inline">\({\textbf V}\)</span>. Esto es <span class="math display">\[{\textbf T}={\textbf U}{\textbf S}{\textbf V}^T\,,\]</span> donde el número de valores singulares no nulos indican el rango de <span class="math inline">\({\textbf T}\)</span>. Si <span class="math inline">\(K&lt;N\)</span> y el número de filas (i.e., los datos) son linealmente independientes entonces el rango será <span class="math inline">\(K\)</span>. Ahora la matriz de covarianza es</p>
<p><span class="math display">\[\frac{1}{N} {\textbf T}{\textbf T}^T = \frac{1}{N} ({\textbf U}{\textbf S}{\textbf V}^T)({\textbf U}{\textbf S}{\textbf V}^T)^T=
\frac{1}{N}{\textbf U}{\textbf S}{\textbf V}^T{\textbf V}({\textbf U}{\textbf S})^T=\frac{1}{N} {\textbf U}{\textbf S}{\textbf V}^T{\textbf V}{\textbf S}^T{\textbf U}^T=\frac{1}{N} {\textbf U}{\textbf S}{\textbf S}^T{\textbf U}^T\,.\]</span></p>
<p>La derecha del igual es la descomposición en valores propios de la matriz de covarianza, donde <span class="math inline">\({\textbf S}{\textbf S}^T\)</span> es cuadrada y diagonal con los elementos igual a <span class="math inline">\(N\lambda_i\)</span>; y las columnas de <span class="math inline">\({\textbf U}\)</span> son las FEOs. Las amplitudes de las FEOs vienen dadas por las filas de la matriz</p>
<p><span class="math display">\[{\textbf U}^T{\textbf T}={\textbf S}{\textbf V}\,,\]</span></p>
<p>asociado con valores singulares no nulos.</p>
<p><strong>Interpretación de las FEOs</strong></p>
<p>Como comentario final hay que decir que las FEOs no son muy fáciles de interpretar. Matemáticamente son estructuras que representan la mayor cantidad de varianza de los datos originales y que son ortogonales entre ellas. En ocasiones estas estructuras nos dan estructuras con sentido físico en un conjunto de datos, en otras no. Las estructuras particulares encontradas dependerán de cómo hemos acomodado nuestra matriz bidimensional de datos. Algunas sugerencias para detectar si las FEOs tienen sentido físico son las siguientes:<br>
</p>
<ol type="1">
<li><p>¿La varianza de tu FEO es mas grande que lo que esperabas si los datos originales no tenían estructura alguna?<br>
</p></li>
<li><p>¿Existe una explicación apriori para las estructuras que has encontrado? ?`Se pueden explicar las estructuras en términos de alguna teoría? ¿Las estructuras se comportan consistentemente con la teoría apriori?<br>
</p></li>
<li><p>¿Cuán robustas son las estructuras a la elección del dominio de la estructura? ¿Si cambias el dominio del análisis, esas estructuras cambian significantemente? ¿Si las estructuras estan definidas en un espacio geográfico, y cambias el tamaño de la región, las estructuras cambian significativamente? ¿Si las estructuras estan definidas en el espacio de parámetros y añades o eliminas un parámetros, los resultados cambian de forma suave o aleatoriamente?<br>
</p></li>
</ol>
<p>(4)¿Cuán robustas son las estructuras a los datos usados? ¿Si divides los datos originales en fracciones menores y haces el analisis de cada fracción, obtienes las mismas estructuras?</p>
<p><strong>Ejemplo de descomposición en valores singulares</strong></p>
<p>&nbsp; Supongamos la matriz</p>
<p><span class="math display">\[{\textbf A}=\left(\begin{array}{ccc}
  3 &amp; 1 &amp; 1\\
  -1 &amp; 3 &amp; 1\\
      \end{array}\right)\,.\]</span></p>
<p>Para encontrar <span class="math inline">\({\textbf U}\)</span> debemos resolver el problema en vectores y valores propios de la matriz <span class="math inline">\({\textbf A}{\textbf A}^T\)</span></p>
<p><span class="math display">\[{\textbf A}{\textbf A}^T=\left(\begin{array}{ccc}
  3 &amp; 1 &amp; 1\\
  -1 &amp; 3 &amp; 1\\
      \end{array}\right)
      \left(\begin{array}{ccc}
  3 &amp; -1 \\
  1 &amp; 3 \\
  1 &amp; 1 \\
      \end{array}\right)=\left(\begin{array}{cc}
  11 &amp; 1 \\
  1 &amp; 11 \\
      \end{array}\right)\]</span></p>
<p>El sistema de ecuaciones `propio’ es</p>
<p><span class="math display">\[\left(\begin{array}{cc}
  11 &amp; 1 \\
  1 &amp; 11 \\
      \end{array}\right)\left(\begin{array}{cc}
  x_1 \\
  x_2 \\
      \end{array}\right)=\lambda\left(\begin{array}{cc}
  x_1 \\
  x_2 \\
      \end{array}\right)\]</span></p>
<p>Si resolvemos para <span class="math inline">\(\lambda\)</span></p>
<p><span class="math display">\[\left|\begin{array}{cc}
  11-\lambda &amp; 1 \\
  1 &amp; 11-\lambda \\
      \end{array}\right|=0\,,\]</span></p>
<p>lo que deja el polinomio característico</p>
<p><span class="math display">\[(\lambda-10)(\lambda-12)=0\,,\]</span></p>
<p>con raices (valores propios) <span class="math inline">\(\lambda_1=12\)</span> y <span class="math inline">\(\lambda_2=10\)</span>.</p>
<p>Si sustituimos en el sistema de ecuaciones `propio’ el primer valor propio <span class="math inline">\(\lambda_1=12\)</span> <span class="math display">\[(11-12)x_1 + x_2=0\]</span></p>
<p><span class="math display">\[x_1=x_2\]</span></p>
<p>Para <span class="math inline">\(x_1=1\)</span> obtenemos que <span class="math inline">\(x_2=1\)</span>. Entonces obtenemos el vector propio <span class="math inline">\({\textbf v}_1=[1,1]\)</span>“”</p>
<p>Si sustituimos en el sistema de ecuaciones “propio” el primer valor propio <span class="math inline">\(\lambda_1=10\)</span> <span class="math display">\[(11-10)x_1 + x_2=0\]</span></p>
<p><span class="math display">\[x_1=-x_2\]</span> Para <span class="math inline">\(x_1=1\)</span></p>
<p>obtenemos que <span class="math inline">\(x_2=-1\)</span>. Entonces obtenemos el vector propio <span class="math inline">\({\textbf v}_2=[1,-1]\)</span>.</p>
<p>Si organizamos la matriz con columnas correspondientes a los vectores propios asociados a los valores propios de mayor a menor, obtenemos</p>
<p><span class="math display">\[\left(\begin{array}{cc}
  1 &amp; 1 \\
  1 &amp; -1 \\
      \end{array}\right)\]</span></p>
<p>Finalmente, sabemos que <span class="math inline">\({\textbf U}\)</span> tiene que ser ortonormal. Vamos a usar el proceso de Gram-Schmidt para ortonormalizar las columnas de <span class="math inline">\({\textbf U}\)</span>. Empezamos normalizando la primera columna</p>
<p><span class="math display">\[{\textbf u}_1=\frac{{\textbf v}_1}{|{\textbf v}_1|}=\frac{[1,1]}{\sqrt{2}}=\left[ \frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}} \right]\,,\]</span></p>
<p>y calculamos el vector ortonormal como</p>
<p><span class="math display">\[{\textbf w}_2={\textbf v}_2-{\textbf u}_1\cdot{\textbf v}_2{\textbf u}_1=[1,-1]-\left[ \frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}} \right]
\cdot[1,-1]\left[ \frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}} \right]=[1,-1]-[0,0]=[1,-1]\,.\]</span></p>
<p>Si lo normalizamos</p>
<p><span class="math display">\[{\textbf u}_2=\frac{{\textbf w}_2}{|{\textbf w}_2|}=\left[ \frac{1}{\sqrt{2}}, \frac{-1}{\sqrt{2}} \right]\,,\]</span></p>
<p>dejando la matriz</p>
<p><span class="math display">\[{\textbf U}=\left(\begin{array}{cc}
  \frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \\
  \frac{1}{\sqrt{2}} &amp; -\frac{1}{\sqrt{2}} \\
\end{array}\right)\,.\]</span></p>
<p>Similarmente, para calcular la matriz <span class="math inline">\({\textbf V}\)</span> debemos resolver el problema en vectores y valores propios de la matriz <span class="math inline">\({\textbf A}^T{\textbf A}\)</span></p>
<p><span class="math display">\[{\textbf A}^T{\textbf A}=\left(\begin{array}{ccc}
  3 &amp; -1 \\
  1 &amp; 3 \\
  1 &amp; 1 \\
      \end{array}\right)\left(\begin{array}{ccc}
  3 &amp; 1 &amp; 1\\
  -1 &amp; 3 &amp; 1\\
      \end{array}\right)=
      \left(\begin{array}{ccc}
  10 &amp; 0 &amp; 2 \\
  0 &amp; 10 &amp; 4\\
  2 &amp; 4 &amp; 2\\
      \end{array}\right)
      \]</span></p>
<p>El sistema de ecuaciones `propio’ es</p>
<p><span class="math display">\[\left(\begin{array}{ccc}
  10 &amp; 0 &amp; 2 \\
  0 &amp; 10 &amp; 4\\
  2 &amp; 4 &amp; 2\\
      \end{array}\right)\left(\begin{array}{c}
  x_1 \\
  x_2 \\
  x_3 \\
      \end{array}\right)=\lambda \left(\begin{array}{c}
  x_1 \\
  x_2 \\
  x_3 \\
      \end{array}\right)\]</span></p>
<p><span class="math display">\[(10-\lambda)x_1 + 2x_3=0\]</span> <span class="math display">\[(10-\lambda)x_2 + 4x_3=0\]</span> <span class="math display">\[2x_1 + 4x_2 + (2-\lambda)x_3=0\]</span></p>
<p>Si resolvemos para <span class="math inline">\(\lambda\)</span></p>
<p><span class="math display">\[\left|\begin{array}{ccc}
  10-\lambda &amp; 0 &amp; 2 \\
  0 &amp; 10-\lambda &amp; 4\\
  2 &amp; 4 &amp; 2-\lambda\\
      \end{array}\right|=0\]</span></p>
<p>lo que deja el polinomio característico</p>
<p><span class="math display">\[\lambda (\lambda-10)(\lambda-12)=0\,,\]</span></p>
<p>con raices (valores propios) <span class="math inline">\(\lambda_1=12\)</span>, <span class="math inline">\(\lambda_2=10\)</span>, y <span class="math inline">\(\lambda_3=0\)</span>. Substituyendo en el sistema `propio’ para <span class="math inline">\(\lambda_1=12\)</span></p>
<p><span class="math display">\[(10-12)x_1 + 2x_3 = -2x_1+2x_3=0\]</span> <span class="math display">\[x_1=1;\,\,x_3=1\]</span></p>
<p><span class="math display">\[(10-12)x_2 + 4x_3 = -2x_2+4x_3=0\]</span> <span class="math display">\[x_2=2x_3\]</span></p>
<p><span class="math display">\[x_2=2\]</span> Entonces, para <span class="math inline">\(\lambda_1=12\)</span> obtenemos el vector propio <span class="math inline">\({\textbf v}_1=[1,2,1]\)</span>.<br>
</p>
<p>Para <span class="math inline">\(\lambda_2=10\)</span></p>
<p><span class="math display">\[(10-10)x_1 + 2x_3=2x_3=0\]</span> <span class="math display">\[x_3=0\]</span></p>
<p><span class="math display">\[2x_1+4x_2=0\]</span> <span class="math display">\[x_1=-2x_2\]</span></p>
<p><span class="math display">\[x_1=2;\,\,x_2=-1\]</span></p>
<p>y obtenemos <span class="math inline">\({\textbf v}_2=[2,-1,0]\)</span> para <span class="math inline">\(\lambda_2=10\)</span>. \ Finalmente, para <span class="math inline">\(\lambda_3=0\)</span></p>
<p><span class="math display">\[10x_1+2x_3=0\]</span> <span class="math display">\[x_3=-5\]</span> <span class="math display">\[10x_1-20=0\]</span></p>
<p><span class="math display">\[x_2=2\]</span> <span class="math display">\[2x_1+8-10=0\]</span></p>
<p><span class="math display">\[x_1=1\]</span></p>
<p>lo que implica que para <span class="math inline">\(\lambda_3=0\)</span> <span class="math inline">\({\textbf v}_3=[1,2,-5]\)</span>.<br>
</p>
<p>Si organizamos los vectores de acuerdo con el valor de los valores propios (de mayor a menor) obtenemos la matriz</p>
<p><span class="math display">\[\left(\begin{array}{ccc}
  1 &amp; 2 &amp; 1 \\
  2 &amp; -1 &amp; 2\\
  1 &amp; 0 &amp; -5\\
      \end{array}\right)\]</span></p>
<p>Ahora vamos a ortonormalizarla con el proceso de Gram-schmidt</p>
<p><span class="math display">\[{\textbf u}_1=\frac{{\textbf v}_1}{|{\textbf v}_1|}=\frac{[1,2,1]}{\sqrt{6}}\,,\]</span> y calculamos el vector ortonormal como</p>
<p><span class="math display">\[{\textbf w}_2={\textbf v}_2-{\textbf u}_1\cdot{\textbf v}_2{\textbf u}_1=[2,-1,0]\]</span></p>
<p>Si lo normalizamos</p>
<p><span class="math display">\[{\textbf u}_2=\frac{{\textbf w}_2}{|{\textbf w}_2|}=\left[ \frac{2}{\sqrt{5}}, \frac{-1}{\sqrt{5}} , 0 \right]\]</span></p>
<p>El último vector ortonormal a calcular es</p>
<p><span class="math display">\[{\textbf w}_3={\textbf v}_3-{\textbf u}_1\cdot{\textbf v}_3{\textbf u}_1 - {\textbf u}_2\cdot{\textbf v}_3{\textbf u}_2=[\frac{-2}{3},\frac{-4}{3},\frac{10}{3}]\]</span></p>
<p>Si lo normalizamos</p>
<p><span class="math display">\[{\textbf u}_3=\frac{{\textbf w}_3}{|{\textbf w}_3|}=\left[ \frac{1}{\sqrt{30}}, \frac{2}{\sqrt{30}} , \frac{-5}{\sqrt{30}} \right]\]</span></p>
<p>dejando la matriz</p>
<p><span class="math display">\[{\textbf V}=\left(\begin{array}{ccc}
  \frac{1}{\sqrt{6}} &amp; \frac{2}{\sqrt{5}} &amp; \frac{1}{\sqrt{30}}\\
  \frac{2}{\sqrt{6}} &amp; -\frac{1}{\sqrt{5}} &amp; \frac{2}{\sqrt{30}}\\
  \frac{1}{\sqrt{6}} &amp; 0 &amp; \frac{-5}{\sqrt{30}}
      \end{array}\right)\,.\]</span></p>
<p>Finalmente, lo que realmente queremos es</p>
<p><span class="math display">\[{\textbf V}^T=\left(\begin{array}{ccc}
  \frac{1}{\sqrt{6}} &amp; \frac{2}{\sqrt{6}} &amp; \frac{1}{\sqrt{6}}\\
  \frac{2}{\sqrt{5}} &amp; -\frac{1}{\sqrt{5}} &amp; 0\\
  \frac{1}{\sqrt{30}} &amp; \frac{2}{\sqrt{30}} &amp; \frac{-5}{\sqrt{30}}\\
      \end{array}\right)\,.\]</span></p>
<p>Para calcular <span class="math inline">\({\textbf S}\)</span> debemos tomar las raices cuadradas de los valores propios diferentes de cero (<span class="math inline">\(\lambda_i\ne0\)</span>) y colocarlos en la diagonal principal en orden descendente. Es decir, el valor propio mayor en la posición <span class="math inline">\(s_{11}\)</span>, el siguiente mas grande en <span class="math inline">\(s_{22}\)</span>, y así sucesivamente. Los valores propios diferentes de cero son iguales para <span class="math inline">\({\textbf U}\)</span> y <span class="math inline">\({\textbf V}\)</span> con lo que no importa de cual los tomemos. Puesto que solo hay dos valores propios diferentes de cero y el orden de las matrices <span class="math inline">\({\textbf U}\)</span> y <span class="math inline">\({\textbf V}\)</span> es <span class="math inline">\(3\times3\)</span>, debemos añadir una columna de ceros a <span class="math inline">\({\textbf S}\)</span></p>
<p><span class="math display">\[{\textbf S}=\left(\begin{array}{ccc}
  \sqrt{12} &amp; 0 &amp; 0 \\
  0 &amp; \sqrt{10} &amp; 0\\
      \end{array}\right)\,.\]</span></p>
<p>Ahora ya tenemos todas las matrices de la descomposición en valores singulares:</p>
<p><span class="math display">\[{\textbf A}={\textbf U}{\textbf S}{\textbf V}^T=\left(\begin{array}{cc}
  \frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \\
  \frac{1}{\sqrt{2}} &amp; -\frac{1}{\sqrt{2}} \\
      \end{array}\right)
      \left(\begin{array}{ccc}
  \sqrt{12} &amp; 0 &amp; 0 \\
  0 &amp; \sqrt{10} &amp; 0\\
      \end{array}\right)
      \left(\begin{array}{ccc}
  \frac{1}{\sqrt{6}} &amp; \frac{2}{\sqrt{5}} &amp; \frac{1}{\sqrt{30}}\\
  \frac{2}{\sqrt{6}} &amp; -\frac{1}{\sqrt{5}} &amp; \frac{2}{\sqrt{30}}\\
  \frac{1}{\sqrt{6}} &amp; 0 &amp; \frac{-5}{\sqrt{30}}
      \end{array}\right)=\]</span></p>
<p><span class="math display">\[=\left(\begin{array}{ccc}
  \frac{12}{\sqrt{2}} &amp; \frac{\sqrt{10}}{\sqrt{2}} &amp; 0\\
  \frac{12}{\sqrt{2}} &amp; -\frac{\sqrt{10}}{\sqrt{2}} &amp; 0\\
      \end{array}\right)
      \left(\begin{array}{ccc}
  \frac{1}{\sqrt{6}} &amp; \frac{2}{\sqrt{5}} &amp; \frac{1}{\sqrt{30}}\\
  \frac{2}{\sqrt{6}} &amp; -\frac{1}{\sqrt{5}} &amp; \frac{2}{\sqrt{30}}\\
  \frac{1}{\sqrt{6}} &amp; 0 &amp; \frac{-5}{\sqrt{30}}
      \end{array}\right)=
      \left(\begin{array}{ccc}
  3 &amp; 1 &amp; 1 \\
  -1 &amp; 3 &amp; 1\\
      \end{array}\right)\,.\]</span></p>
</section>
</section>
<section id="análisis-espectral-o-análisis-de-fourier" class="level2">
<h2 class="anchored" data-anchor-id="análisis-espectral-o-análisis-de-fourier">Análisis espectral o análisis de Fourier</h2>
<section id="introducción" class="level3">
<h3 class="anchored" data-anchor-id="introducción">Introducción</h3>
<p>Una función periódica es aquella cuyos valores se repiten a intervalos regulares. El tiempo entre las sucesivas repeticiones se denomina periodo <span class="math inline">\(\tau\)</span>. Normalmente lo definimos entre sucesivas crestas. Matemáticamente, una función es periódica si <span class="math inline">\(f(t)=f(t+T)\)</span> para todo valor de <span class="math inline">\(T\)</span>.</p>
<p>La frecuencia de una función periódica se define como el inverso del periodo, <span class="math inline">\(f=1/\tau\)</span>, es decir el número de ciclos por unidad de tiempo (si es por segundo hablamos de Hercios, Hz). Si un ciclo equivale a <span class="math inline">\(2\pi\)</span> radianes, entonces el número de radianes por segundo es lo que se concoce por la frecuencia angular fundamental: <span class="math display">\[\omega=\frac{2\pi}{T}\,.\]</span></p>
<p>Las funciones periódicas también se pueden definir en el espacio. Entonces el periodo se define como <span class="math display">\[\tau=\lambda/v_p\,,\]</span> donde <span class="math inline">\(\lambda\)</span> es la longitud de onda y <span class="math inline">\(v_p=\lambda/\tau=\omega/k\)</span> es la velocidad de fase. La longitud de onda es una distancia entre estados de la onda que se repiten, e.j. entre dos crestas. El número de onda <span class="math inline">\(k\)</span> es el número de ondas contenidas en una unidad de distancia <span class="math display">\[k=\frac{2\pi}{\lambda}=\frac{\omega}{v_p}\]</span></p>
<p>El valor promedio de una función periódica es: <span class="math display">\[f_m=\frac{1}{\tau}\int\limits^{\tau}_0 f(t) dt\,,\]</span> y su valor cuadrático medio (o RMS, en inglés) es: <span class="math display">\[f_{rms}=\sqrt{\frac{1}{\tau}\int\limits^{\tau}_0 f^2(t) dt}\,,\]</span> donde las integrales se han definido en el intervalo 0,<span class="math inline">\(\tau\)</span>, aunque se pueden definir en cualquier intervalo que abarque un periodo, e.j. de -<span class="math inline">\(\tau\)</span>/2 a <span class="math inline">\(\tau\)</span>/2.</p>
<p>Una de las ondas periódicas mas utilizadas es la sinusoidal o cosenosoidal. <span class="math display">\[f(t)=A sen(\omega t + \theta)\,,\]</span> siendo <span class="math inline">\(A\)</span> la amplitud y <span class="math inline">\(\theta\)</span> su fase inicial. En este caso el valor medio es cero y su rms es <span class="math inline">\(A/\sqrt(2)\)</span>. % Recordar que existen dos frecuencias básicas: (i) Frecuencia de Nyquist <span class="math inline">\(f_N=1/(2\Delta t)\)</span> (la frecuencia mas alta que podemos resolver) y (ii) Frecuencia fundamental <span class="math inline">\(f_0=1/(\Delta t N)=1/T\)</span> (la frecuencia mas baja que podemos resolver).</p>
</section>
<section id="serie-de-fourier" class="level3">
<h3 class="anchored" data-anchor-id="serie-de-fourier">Serie de Fourier</h3>
<p>El principio básico del análisis de Fourier es que cualquier función periódica <span class="math inline">\(f(t)\)</span> definida en el intervalo <span class="math inline">\([0,T]\)</span> se puede descomponer en suma de funciones simples, sinusoidales y cosinusoidales, o series de Fourier de la forma <span class="math display">\[f(t)=\bar{f(t)} + \sum\limits_p [A_p cos(\omega_p t) + B_p sin (\omega_p t)]\,,\]</span> donde <span class="math inline">\(\overline{f(t)}\)</span> es el valor medio de la serie temporal, <span class="math inline">\(A_p\)</span> y <span class="math inline">\(B_p\)</span> son constantes denominados coeficientes de Fourier, y <span class="math inline">\(\omega_p=2 \pi p f_0=2\pi p/\tau\)</span> es múltiplo de la frecuencia angular fundamental.</p>
<p>Si tenemos suficientes componentes de Fourier cada valor de la serie original se puede reconstruir. La contribución que cada componente tiene sobre la varianza de la serie temporal es una medida de la importancia de una frecuencia particular en la serie original. El punto clave aqui es que el conjunto de coeficientes de Fourier con amplitudes <span class="math inline">\(A_p\)</span> y <span class="math inline">\(B_p\)</span> forman un espectro el cual define la contribución de cada componente oscilatoria <span class="math inline">\(\omega_p\)</span> sobre la `energía’ total de la señal original. En concreto, el spectro de potencia (Power spectrum) define la energía por unidad de banda de frecuencia de una serie temporal. Puesto que debemos definir dos amplitudes <span class="math inline">\(A_p\)</span> y <span class="math inline">\(B_p\)</span>, hay dos grados de libertad por estimación espectral.</p>
<p>Como ya hemos dicho el primer armónico (<span class="math inline">\(p=1\)</span>) oscila con frecuencia fundamental <span class="math inline">\(\omega_1=2\pi f_1\)</span>. El armónico <span class="math inline">\(N/2\)</span>, el cual nos da la componente con la frecuencia más alta que puede ser resuelta tiene frecuencia <span class="math inline">\(f_N=N/2/N\Delta t=1/(2\Delta T)\)</span> ciclos por unidad de tiempo y un periodo de <span class="math inline">\(2\Delta t\)</span>. Esta es la frecuencia de Nyquist.</p>
<p>Las series de Fourier se definen como <span class="math display">\[f(t)=\frac{1}{2}A_0 +\sum\limits^\infty_{p=1}[A_p cos(\omega_p t) + B_p sen(\omega_p t)]\,,\]</span> en la cual <span class="math display">\[\omega_p=2\pi f_p=2\pi p f_1=2\pi p /T;\,\,\,\,p=1,2,...\,,\]</span> es la frecuencia de la componente <span class="math inline">\(p\)</span>ésima en radianes por unidad de tiempo (<span class="math inline">\(f_p\)</span> es en ciclos por unidad de tiempo) y <span class="math inline">\(A_0/2\)</span> es la media de la serie temporal.</p>
<p>Para obtener los coeficientes <span class="math inline">\(A_p\)</span>, debemos multiplicar la expresión de la descomposición de Fourier por <span class="math inline">\(cos(\omega_r t)\)</span> e integrar sobre la serie completa. <span class="math display">\[\int\limits^{T}_{0}f(t)cos(\omega_r t)dt=\frac{1}{2}A_0\int\limits^{T}_{0} cos(\omega_r t) dt + \]</span> <span class="math display">\[+\sum\limits^\infty_{p=1} A_p \int\limits^{T}_{0}cos(\omega_p t)cos(\omega_r t) dt +\]</span> <span class="math display">\[+\sum\limits^\infty_{p=1} B_p \int\limits^{T}_{0}sin(\omega_p t)cos(\omega_r t) dt\,.\]</span></p>
<p>Si usamos las siguientes condiciones de ortogonalidad: <span class="math display">\[\int\limits^{T}_{0}sin(\omega_p t)cos(\omega_r t) dt=0\]</span> <span class="math display">\[\int\limits^{T}_{0}cos(\omega_p t)cos(\omega_r t) dt=
\left\lbrace
  \begin{array}{l}
     T, p=r=0 \\
     T/2,p=r&gt;0 \\
     0, p\ne r \\
  \end{array}
  \right.\]</span> <span class="math display">\[\int\limits^{T}_{0}sin(\omega_p t)sin(\omega_r t) dt=
\left\lbrace
  \begin{array}{l}
     0, p=r=0 \\
     T/2,p=r&gt;0 \\
     0, p\ne r \\
  \end{array}
  \right.\]</span> entonces encontramos que para <span class="math inline">\(r=0;p\ne r\)</span> la ecuación de arriba se reduce a <span class="math display">\[\int\limits^{T}_{0}f(t)dt=\frac{A_0}{2}T\,,\]</span> es decir, <span class="math display">\[A_0=\frac{2}{T}\int\limits^{T}_{0}f(t)dt=2\overline{f(t)}\,,\]</span> dos veces el valor medio de la serie <span class="math inline">\(f(t)\)</span>. Es por ello que se añade el factor de <span class="math inline">\(1/2\)</span> en la serie de Fourier. Es decir, para que el primer término de la serie de Fourier sea igual a la media de la serie temporal <span class="math inline">\(\overline{f(t)}=1/2A_0\)</span>.</p>
<p>Cuando <span class="math inline">\(p\ne0\)</span> el único término no despreciable de la derecha de la expresión de arriba sucede cuando <span class="math inline">\(r=p\)</span> <span class="math display">\[\int\limits^{T}_{0}f(t)cos(\omega_p t)dt=\frac{A_p}{2}T\,,\]</span> y entonces <span class="math display">\[A_p=\frac{2}{T}\int\limits_0^{T} f(t) cos(\omega_p t) dt,\,\,\,\,p=1,2,...\]</span></p>
<p>Los otros coeficientes <span class="math inline">\(B_p\)</span> son obtenidos igualmente multiplicando por <span class="math inline">\(sen(\omega_r t)\)</span> en lugar de <span class="math inline">\(cos(\omega_r t)\)</span> <span class="math display">\[B_p=\frac{2}{T}\int\limits_0^{T} f(t) sen(\omega_p t) dt,\,\,\,\,p=1,2,...\,.\]</span></p>
<p>También podemos representar la serie de Fourier en notación compacta como: <span class="math display">\[f(t)=\frac{1}{2}C_0 +\sum\limits^\infty_{p=1}C_p cos(\omega_p t - \theta_p)\,,\]</span> en la cual la amplitud de la <span class="math inline">\(p\)</span>ésima componente es <span class="math display">\[C_p=\sqrt{A_p^2+B_p^2}\,,\,\,\,\,p=1,2,....\]</span> donde <span class="math inline">\(C_0=A_0 (B_0=0)\)</span> es dos veces el valor promedio de la serie y <span class="math display">\[\theta_p=arctg[B_p/A_p]\,,\,\,\,\,p=1,2,...\]</span> es el ángulo de fase de la componente al tiempo <span class="math inline">\(t=0\)</span>. El ángulo de fase nos informa del “desfase” <em>lag</em> relativo de las componente en radianes (o grados) medido en el sentido contrario a las agujas del reloj desde el eje real definido por <span class="math inline">\(B_p=0, A_p&gt;0\)</span>. El correspondiente tiempo de desfase para la componente <span class="math inline">\(p\)</span>ésima es <span class="math inline">\(t_p=\theta_p/2\pi f_p\)</span> en el cual <span class="math inline">\(\theta_p\)</span> esta medida en radianes. La energía espectral se define como las amplitudes de los coeficientes de Fourier al cuadrado, lo cual representa la varianza y entonces la energía <span class="math display">\[C^2_p=A_p^2 + B_p^2\,.\]</span> \ \ De igual forma con las relaciones trigonométricas de arriba se puede expresar las series de Fourier en notación compleja. Usando <span class="math display">\[sen\omega_p t=\frac{e^{i\omega_pt}-e^{-i\omega_pt}}{2i}\,\,\,\,\,\,\,y\,\,\,\,\,\,\,
cos\omega_p t=\frac{e^{i\omega_pt}+e^{-i\omega_pt}}{2}\,,\]</span> obtenemos <span class="math display">\[f(x)=\frac{1}{2}A_0 +\sum\limits^\infty_{p=1}[A_p cos(\omega_p t) + B_p sen(\omega_p t)]=\]</span> <span class="math display">\[=\frac{1}{2}A_0 +\sum\limits^\infty_{p=1}\left[ A_p \frac{e^{i\omega_pt}+e^{-i\omega_pt}}{2} + B_p \frac{e^{i\omega_pt}-e^{-i\omega_pt}}{2i}\right]=\]</span> <span class="math display">\[=\frac{1}{2}A_0+\sum\limits^\infty_{p=1}\left[\frac{A_p e^{i\omega_p t}}{2} + \frac{A_p e^{-i\omega_p t}}{2} -
\frac{iB_p e^{i\omega_p t}}{2} + \frac{iB_p e^{-i\omega_p t}}{2} \right]=\]</span> <span class="math display">\[=\frac{1}{2}A_0+\sum\limits^\infty_{p=1} e^{i\omega_p t}\frac{A_p-iB_p}{2} +
                  \sum\limits^\infty_{p=1} e^{-i\omega_p t}\frac{A_p+iB_p}{2}=\]</span> <span class="math display">\[=C^*_0 + \sum\limits^\infty_{p=1}C^*_pe^{i\omega_p t} + \sum\limits^\infty_{p=1}C^*_{-p} e^{-i\omega_p t}=
\sum\limits^\infty_{p=-\infty} C^*_p e^{i\omega_p t}\,,\]</span> donde hemos definido las siguientes relaciones entre los coeficientes de Fourier complejos y reales: <span class="math display">\[C^*_0=\frac{1}{2}A_0\,,\]</span> <span class="math display">\[C^*_p=\frac{1}{2}(A_p-iB_p)\,,\]</span> <span class="math display">\[C^*_{-p}=\frac{1}{2}(A_p+iB_p)\,.\]</span></p>
<p>En resumen, podemos reconstruir la serie periódica <span class="math inline">\(f(t)\)</span> con la transformada de Fourier <span class="math display">\[f(x)=\sum\limits^\infty_{p=-\infty} C^*_p e^{i\omega_p t}\,,\]</span> e inversamente podemos calcular los coeficientes de Fourier <span class="math inline">\(C^*_p\)</span> a partir de la <span class="math inline">\(f(t)\)</span> <span class="math display">\[C^*_p=\frac{1}{2}(A_p-iB_p)=\frac{1}{2}\left(\frac{2}{T}\int\limits_0^{T} f(t) cos(\omega_p t) dt -
                                             i \frac{2}{T}\int\limits_0^{T} f(t) sen(\omega_p t) dt \right)=\]</span> <span class="math display">\[=\frac{1}{T}\int\limits_0^{T} f(t) [cos(\omega_p t) - isen(\omega_p t) ]dt = \frac{1}{T}\int\limits_0^{T} f(t) e^{-i \omega_p t} dt\,,\]</span> es decir, podemos pasar del espacio temporal <span class="math inline">\(f(t)\)</span> al espacio espectral o de Fourier <span class="math inline">\(C^*_p\)</span> e inversamente regresar al espacio temporal de nuevo.</p>
<p>El teorema de Parseval es precisamente el que demuestra que la el valor cuadrático medio de la serie de Fourier es igual al error cuadrático medio de los coeficientes de Fourier. La varianza de la serie de Fourier es <span class="math display">\[\frac{1}{T}\int\limits^{T}_0 f^2(t) dt=\frac{1}{T}\int\limits^{T}_0
\left(\frac{1}{2}A_0 +\sum\limits^\infty_{p=1}[A_p cos(\omega_p t) + B_p sen(\omega_p t)]\right) dt=\]</span> <span class="math display">\[=\frac{1}{T}\frac{1}{4} A^2_0 \int\limits^{T}_0 dt + \frac{1}{T}A_0 \sum\limits^\infty_{p=1} A_p \int\limits^{T}_0 cos(\omega_p t)dt + \frac{1}{T}A_0\sum\limits^\infty_{p=1}B_p \int\limits^{T}_0 sen(\omega_p t)dt +\]</span> <span class="math display">\[+\frac{1}{T}\sum\limits^\infty_{p=1}A^2_p \int\limits^{T}_0 cos^2(\omega_p t)dt +
\frac{1}{T}\sum\limits^\infty_{p=1}B^2_p \int\limits^{T}_0 sen^2(\omega_p t)dt+\]</span> <span class="math display">\[+\frac{1}{T}2\sum\limits^\infty_{p=1}A_p \sum\limits^\infty_{p=1}B_p\int\limits^{T}_0 sen(\omega_p t)cos(\omega_p t)dt=
\frac{1}{T}\frac{1}{4}A^2_0 T + \frac{1}{T}\sum\limits^\infty_{p=1}A^2_p\left[ \frac{t}{2} + \frac{sen(2\omega_p t)}{4\omega_p}\right]^T_0+\]</span> <span class="math display">\[+\frac{1}{T}\sum\limits^\infty_{p=1}B^2_p\left[ \frac{t}{2} - \frac{sen(2\omega_p t)}{4\omega_p}\right]^T_0=
\frac{1}{4}A^2_0 + \frac{1}{2}\sum\limits^\infty_{p=1}A_p^2 + \frac{1}{2}\sum\limits^\infty_{p=1}B_p^2=\]</span> <span class="math display">\[=\frac{1}{4}A^2_0 + \frac{1}{2}\sum\limits^\infty_{p=1} A_p^2 + B_p^2\,.\]</span></p>
<p>Utilizando las siguientes identidades <span class="math display">\[|C^*_p|^2=|C^*_{-p}|^2=\frac{1}{4}(A_p^2 + B_p^2)\,,\]</span> <span class="math display">\[C^*_0=\frac{1}{2}A_0\,,\]</span> el teorema de Parseval en términos de los coeficientes de Fourier complejos es <span class="math display">\[\frac{1}{T}\int\limits^{T}_0 f^2(t) dt=(C^*_0)^2 + \frac{1}{2}\sum\limits^\infty_{p=1} 4|C^*_p|^2=
(C^*_0)^2 + \sum\limits^\infty_{p=1} |C^*_p|^2 + \sum\limits^\infty_{p=1} |C^*_p|^2=\]</span> <span class="math display">\[=(C^*_0)^2 + \sum\limits^\infty_{p=1} |C^*_p|^2 + \sum\limits^\infty_{p=1} |C^*_{-p}|^2=
\sum\limits^\infty_{p=-\infty} |C^*_p|^2\]</span></p>
<p>Esto da lugar a la relación entre la amplitud de las componentes de Fourier en el dominio espectral (de frecuencia) y la varianza de la serie en el dominio temporal.<br>
</p>
<p><strong>NOTA:</strong> En el análisis de Fourier es importante recalcar que debemos de eliminar la tendencia de la serie antes de calcular los coeficientes. Sino lo hacemos, el análisis de Fourier pondrá erroneamente la varianza de la tendencia en las componentes de baja frecuencia de la expansión de Fourier. En Matlab eso lo podemos hacer con el comando detrend.m o bien simplemente extrayendo el promedio temporal.</p>
<p>{jemplo de cálculo de coeficientes de Fourier}</p>
<p>Imaginemos la siguiente onda cuadrada representada por la función <span class="math display">\[f(t)=
\left\lbrace
  \begin{array}{l}
     -1,\text{para} -\frac{1}{2}T\le t&lt; 0 \\
     +1,\text{para} 0\le t&lt; \frac{1}{2}T  \\
  \end{array}
  \right.\]</span></p>
<p>Puesto que función de arriba es impar, es decir cumple la condición de simetría <span class="math inline">\(-f(t)=f(-t)\)</span>, entonces la serie de Fourier resultante solamente contendrá componentes sinusoidales. Entonces <span class="math display">\[B_p=\frac{2}{T}\int\limits_{-T/2}^{T/2} f(t) sen(\omega_p t) dt=\]</span> <span class="math display">\[=\frac{2}{T}\int\limits_{-T/2}^{0} f(t) sen(\omega_p t) dt + \frac{2}{T}\int\limits_{0}^{T/2} f(t) sen(\omega_p t) dt=\]</span> <span class="math display">\[\frac{2}{T}\left[ (-1) \int\limits_{-T/2}^{0} sen(\omega_p t) dt +
                   (1) \int\limits_{0}^{T/2} sen(\omega_p t) dt \right]=\]</span></p>
<p><span class="math display">\[=\frac{2}{T}\left[ \left|\frac{cos(\omega_p t)}{\omega_p}\right|_{-T/2}^{0}
                     -\left|\frac{cos(\omega_p t)}{\omega_p}\right|_{0}^{T/2} \right]=\]</span> <span class="math display">\[=\frac{2}{\omega_p T}\left[ 1 - cos(\omega_p T/2) - cos(\omega_p T/2) + 1  \right]=
\frac{2}{\omega_p T}\left[2 -2cos(\omega_p T/2)\right]=\]</span> <span class="math display">\[=\frac{4}{\omega_p T}\left[1-cos(\omega_p T/2)\right]=
\frac{2}{\pi p}\left[1-cos(\pi p)\right]\]</span> \ \ Los coeficientes son cero (<span class="math inline">\(B_p=0\)</span>) si <span class="math inline">\(p\)</span> es par y <span class="math inline">\(B_p=4/\pi p\)</span> si <span class="math inline">\(p\)</span> es impar. Finalmente nuestra serie de Fourier es <span class="math display">\[f(t)=\sum\limits^\infty_{p=1}B_p sen(\omega_p t)=\]</span> <span class="math display">\[=\sum\limits^\infty_{p=1}B_p sen\left(\frac{2\pi p}{T} t\right)=
B_1 sen\left(\frac{2\pi 1}{T} t\right) + B_3 sen\left(\frac{2\pi 3}{T} t\right) +\]</span> <span class="math display">\[+B_5 sen\left(\frac{2\pi 5}{T} t\right) + ... =\frac{4}{\pi}sen\left(\frac{2\pi 1}{T} t\right) +\]</span> <span class="math display">\[+\frac{4}{3\pi}sen\left(\frac{2\pi 3}{T} t\right) +\frac{4}{5\pi}sen\left(\frac{2\pi 5}{T} t\right)=\]</span> <span class="math display">\[=\frac{4}{\pi}\left( \frac{sen{\omega_1 t}}{1} + \frac{sen{3\omega_1 t}}{3} + \frac{sen{5\omega_1 t}}{5}\right)+...\]</span></p>
<p>{Series de Fourier Discretas}</p>
<p>En general, vamos a muestrear de forma discreta el océano y consecuentemente las series temporales que obtenemos son discretas en el tiempo. Segun el teorema de Parseval, la varianza de estas series discretas <span class="math display">\[\sigma^2=\frac{1}{N-1}\sum\limits^N_{t=1}(f(t)-\overline{f(t)})^2\]</span> se puede obtener sumando las contribuciones individuales de los armónicos de Fourier. La descomposición de series temporales discretas en armónicos específicos da lugar al concepto de espectro de Fourier. Para encontrar el espectro de Fourier debemos calcular los coeficientes <span class="math inline">\(A_p,B_p\)</span> o, equivalentemente, las amplitudes <span class="math inline">\(C_p\)</span> y el ángulo de fase <span class="math inline">\(\theta_p\)</span>.</p>
<p>Supongamos la serie de Fourier para un registro finito de longitud par <span class="math inline">\(N\)</span> definido en los tiempos <span class="math inline">\(t_1, t_2,....,t_N\)</span> <span class="math display">\[f(t_n)=\frac{1}{2}A_0+\sum\limits^{N/2}_{p=1}[A_p cos(\omega_p t_n) + B_p sen(\omega_p t_n)]\,,\]</span> donde ya sabemos <span class="math inline">\(\omega_p=2\pi f_p=2\pi p/T\)</span>. Sabiendo que <span class="math inline">\(t_n=n \Delta t\)</span>, esta serie se puede reescribir como <span class="math display">\[f(t_n)=\frac{1}{2}A_0+\sum\limits^{N/2}_{p=1}[A_p cos(2\pi p n \Delta t / \Delta t N) + B_p sen(2\pi p n \Delta t / \Delta t N)]=\]</span> <span class="math display">\[=\frac{1}{2}A_0+\sum\limits^{N/2}_{p=1}[A_p cos(2\pi p n  / N) + B_p sen(2\pi p n / N)]=
\frac{1}{2}C_0 + \sum\limits^{N/2}_{p=1}[C_p cos[(2\pi p n  / N) - \theta_p]\,,\]</span> donde los términos <span class="math inline">\(A_0/2\)</span> y <span class="math inline">\(C_0/2\)</span> son los valores medios de toda la serie <span class="math inline">\(f(t)\)</span>. Los coeficientes se calculan de igual forma usando las condiciones de ortogonalidad. La única diferencia es que en lugar de tratar con integrales (serie contínua) tratamos con sumatorios (serie discreta) <span class="math display">\[A_p=\frac{2}{N}\sum\limits_{n=1}^{N} f(t_n) cos(2\pi p n /N),\,\,\,\,p=1,2,...,N/2\]</span> <span class="math display">\[A_0=\frac{2}{N}\sum\limits_{n=1}^{N} f(t_n),\,\,\,\,B_0=0\]</span> <span class="math display">\[A_{N/2}=\frac{1}{N}\sum\limits_{n=1}^{N} f(t_n)cos(n\pi),\,\,\,\,B_{N/2}=0\]</span> <span class="math display">\[B_p=\frac{2}{N}\sum\limits_{n=1}^{N} f(t_n) sen(2\pi p n /N),\,\,\,\,p=1,2,...,N/2\]</span></p>
<p>El <span class="math inline">\(N/2\)</span> se debe a que es el armónico con mayor frecuencia que podemos resolver, es decir, aquel que oscila con frecuencia de Nyquist. Para <span class="math inline">\(p&gt;N/2\)</span> las funciones trigonométricas simplemente darán coeficientes de Fourier repetidos ya obtenidos en el intervalo <span class="math inline">\(1\le p\le N/2\)</span>. Para calcular la serie discreta de Fourier, primero debemos calcular los argumentos de las funciones trigonométricas <span class="math inline">\(2\pi n p / N\)</span> para cada entero <span class="math inline">\(p\)</span> y <span class="math inline">\(n\)</span>. Segundo, evaluamos las funciones <span class="math inline">\(cos(2\pi n p / N)\)</span> y <span class="math inline">\(sen(2\pi n p / N)\)</span>, y sumamos para los términos <span class="math inline">\(f(t_n)cos(2\pi n p / N)\)</span> y <span class="math inline">\(f(t_n)sen(2\pi n p / N)\)</span>. Por último, incrementamos <span class="math inline">\(p\)</span> y repetimos los dos pasos anteriores.</p>
<p>{jemplo de Series Temporales Discretas (modificado de Emery and Thompson, p387)}</p>
<p>Considera la serie temporal de temperatura promedia mensual por un periodo de tres años (ver tabla y Figura).<br>
</p>
<p><br>
</p>
<p>Utilizando las expresiones de arriba podemos calcular las frecuencias <span class="math inline">\(f_p\)</span>, amplitudes <span class="math inline">\(A_p\)</span>, <span class="math inline">\(B_p\)</span>, <span class="math inline">\(C_p\)</span>, las fases <span class="math inline">\(\theta_p\)</span> y finalmente la serie de Fourier <span class="math inline">\(f(t)\)</span>. Los valores para las primeras 8 componentes estan reflejados en la tabla<br>
</p>
<p>{Serie de Fourier para variables vectoriales (complejas)}</p>
<p>En este caso la transformada de Fourier se aplica a una cantidad vectorial en lugar de una cantidad escalar como temperatura, salinidad, densidad, etc. Supongamos que tenemos las dos componentes de la velocidad <span class="math inline">\(u\)</span> y <span class="math inline">\(v\)</span> las cuales expandemos en series de Fourier <span class="math display">\[u(t)=\frac{1}{2}A_0+\sum\limits^{N/2}_{p=1}[A_p cos(\omega_p t_n) + B_p sen(\omega_p t_n)]\]</span> <span class="math display">\[v(t)=\frac{1}{2}C_0+\sum\limits^{N/2}_{p=1}[C_p cos(\omega_p t_n) + D_p sen(\omega_p t_n)]\,,\]</span> lo cual se puede escribir en versión compleja como <span class="math display">\[R(t)=u(t)+i v(t)=\frac{1}{2}A_0+\sum\limits^{N/2}_{p=1}[A_p cos(\omega_p t_n) + B_p sen(\omega_p t_n)]
+\]</span> <span class="math display">\[+ i \left(\frac{1}{2}C_0+\sum\limits^{N/2}_{p=1}[C_p cos(\omega_p t_n) + D_p sen(\omega_p t_n)]\right)=\]</span> <span class="math display">\[=\left[\frac{1}{2}A_0 + i \frac{1}{2}C_0\right] + \sum\limits^{N/2}_{p=1} \left[ (A_p + iC_p) cos(\omega_p t_n) + (B_p + i D_p) sen(\omega_p t_n) \right]\,,\]</span> donde <span class="math inline">\(\frac{1}{2}A_0 + i \frac{1}{2}C_0=\overline{u(t)} +i \overline{v(t)}\)</span> es la velocidad media, <span class="math inline">\(\omega_p=2\pi f_p=2\pi p/N\Delta t\)</span> la frecuencia angular, <span class="math inline">\(t_n=n\Delta t\)</span> es el eje de tiempo, y <span class="math inline">\((A_p\,,B_p\,,C_p\,,D_p)\)</span> y son las amplitudes y fases de cada componente de Fourier, tanto las reales como las imaginarias. A diferencia de la serie de Fourier real en este caso las componentes van de <span class="math inline">\(p=1\)</span> hasta <span class="math inline">\(p=N\)</span> y, por lo tanto, estamos cubriendo ambas frecuencias <em>positivas</em> y <em>negativas</em>.</p>
<p>Si extraemos la velocidad media</p>
<p><span class="math display">\[R'(t)=R(t)-[\overline{u(t)} +i \overline{v(t)}]=\sum\limits^{N/2}_{p=1} \left[ (A_p + iC_p) cos(\omega_p t_n) + (B_p + i D_p) sen(\omega_p t_n) \right]\,.\]</span></p>
<p>Ahora vamos a escribir la anomalía de la serie compleja <span class="math inline">\(R'(t)\)</span> en términos de dos componentes rotatorias ortogonales, es decir, una componente que gira en el sentido de las agujas del reloj con amplitud <span class="math inline">\(R^{-}\)</span> y otra que gira en el sentido opuesto a las agujas del reloj y amplitud <span class="math inline">\(R^{+}\)</span> <span class="math display">\[R'(t)=\sum\limits^{N/2}_{p=1} \left[ e^{i\omega_p t} + e^{-i\omega_p t}\right]=\]</span> <span class="math display">\[    =\sum\limits^{N/2}_{p=1}  R_p^{+}\left[cos\left(\omega_p t_n\right) + i sen\left(\omega_p t_n\right )\right] +
       \sum\limits^{N/2}_{p=1}  R_p^{-}\left[cos\left(\omega_p t_n\right) - i sen\left(\omega_p t_n\right))\right]=\]</span> <span class="math display">\[=\sum\limits^{N/2}_{p=1}\left[ (R_p^+ + R_p^{-})cos\left(\omega_p t_n\right) + (R_p^+ - R_p^{-})i sen\left(\omega_p t_n\right) \right]\]</span></p>
<p>Note que <span class="math inline">\(e^{i\omega_p t}=cos\left(\omega_p t_n\right) + i sen\left(\omega_p t_n\right)\)</span> rota en el sentido contrario a las agujas del reloj y <span class="math inline">\(e^{-i\omega_p t}=cos\left(\omega_p t_n\right) - i sen\left(\omega_p t_n\right)\)</span> rota en el sentido de las agujas del reloj. Si comparamos las dos expresiones obtenemos las siguientes identidades <span class="math display">\[A_{p} + i C_{p}=R_p^+ + R_p^{-}\]</span> <span class="math display">\[B_{p} + i D_{p}=(R_p^+ - R_p^{-})i\]</span> y de ahí obtenemos que <span class="math display">\[R_p^+=\frac{1}{2}\left[ A_{p} + D_{p} +i(C_{p}-B_{p})\right]\]</span> <span class="math display">\[R_p^-=\frac{1}{2}\left[ A_{p} - D_{p} +i(C_{p}+B_{p})\right]\,,\]</span> y las magnitudes de las componentes rotatorias es <span class="math display">\[|R_p^+|=\frac{1}{2}\left[ (A_{p} + D_{p})^2 +(C_{p}-B_{p})^2\right]^{1/2}\]</span> <span class="math display">\[|R_p^-|=\frac{1}{2}\left[ (A_{p} - D_{p})^2 +(C_{p}+B_{p})^2\right]^{1/2}\,,\]</span> y las fases de las componentes rotatorias son <span class="math display">\[\epsilon_p^+=actan\left(\frac{C_{p}-B_{p}}{A_{p} + D_{p}}\right)\,,\]</span> <span class="math display">\[\epsilon_p^-=actan\left(\frac{C_{p}+B_{p}}{A_{p} - D_{p}}\right)\,.\]</span></p>
<p>La rotación de las componentes <em>clockwise</em> y <em>anticlockwise</em> dibujan una elipse en el plano <span class="math inline">\(u\)</span> vs <span class="math inline">\(v\)</span>. Puesto que ambas componentes rotan en sentido contrario pero con la misma frecuencia, habrán momentos que ambás apuntaran en la misma dirección (aditivas), otras ocasiones en dirección opuesta (cancelativas). Esos tiempos de adición y cancelación definen el eje mayor de la elipse <span class="math inline">\(L_E=R_p^+ + R_p^-\)</span> y el eje menor de la elipse <span class="math inline">\(L_e=R_p^+ - R_p^-\)</span>. La orientación (inclinación) y la fase de estas elipses a <span class="math inline">\(t=0\)</span> es <span class="math display">\[\theta_e=\frac{1}{2}(\epsilon_p^+ + \epsilon_p^-)\,,\]</span> <span class="math display">\[\phi_e=\frac{1}{2}(\epsilon_p^+ - \epsilon_p^-)\,.\]</span></p>
<p>Una propiedad interesante es el coeficiente rotatorio <span class="math display">\[r(\omega)=\frac{R^+_p - R^-_p}{R^+_p + R^-_p}\,,\]</span> que toma valores entre 0 y 1. Para <span class="math inline">\(r=-11\)</span> tenemos movimiento en el sentido de las agujas del reloj, para <span class="math inline">\(r=0\)</span> tenemos un flujo unidireccional, y para <span class="math inline">\(r=+1\)</span> tenemos movimiento en el sentido contrario de las agujas del reloj.</p>
<p>{Transformada Rápida de Fourier y espectros de potencia}</p>
<p>La FFT (por sus siglas en inglés) es un algoritmo para calcular la serie de Fourier discreta de forma mas eficiente computacionalmente hablando. En este caso la FFT se debería aplicar a series temporales con longitudes múltiples de <span class="math inline">\(2\)</span>. En caso contrario, es útil rellenar de ceros nuestra serie para obtener longitudes múltiplos de <span class="math inline">\(2\)</span>. A eso se le llama `padding’. Básicamente, el algoritmo obtiene los coeficientes de la serie discreta de Fourier</p>
<p><span class="math display">\[f(t)=\sum\limits^\infty_{p=-\infty} C^*_p e^{i\omega_p t_n}\,\,\,\,\,\,\,;\,\,\,\,\,\,\,\omega_p=2 \pi p f_0=2\pi p/T\,,\]</span> <span class="math display">\[F(p)=C^*_p=\frac{1}{2}(A_p-iB_p)=\frac{1}{N} \sum\limits^{N-1}_{n=0} f(t) e^{-i \omega_p t_n} =
        \frac{1}{N} \sum\limits^{N-1}_{n=0}f(t) [cos(\omega_p t_n) - isen(\omega_p t_n) ]\,.\]</span></p>
<p>La parte real de la FFT me da las amplitudes <span class="math inline">\(A_p\)</span> y la parte imaginaria me da las amplitudes <span class="math inline">\(-B_p\)</span> <span class="math display">\[Re [F(p)]=A_p=\frac{2}{N}\sum\limits_{n=0}^{N-1} f(t_n) cos(2\pi p n /N),\,\,\,\,p=0,1,...,N/2\]</span> <span class="math display">\[Im [F(p)]=-B_p=-\frac{2}{N}\sum\limits_{n=0}^{N-1} f(t_n) sen(2\pi p n /N),\,\,\,\,p=0,1,...,N/2\]</span></p>
<p>En general, debemos de normalizar las amplitudes <span class="math inline">\(A_p\)</span> y <span class="math inline">\(B_p\)</span> por la longitud del registro <span class="math inline">\(N\)</span>. Así que en Matlab la amplitud de la FFT es <span class="math display">\[{ abs}({ fft}(f(t)))/N\,,\]</span> y la potencia de la FFT es <span class="math display">\[{ abs}({ fft}(f(t)).^2/N^2\,.\]</span></p>
<p>La FFT ha descompuesto una señal de <span class="math inline">\(N\)</span> elementos, <span class="math inline">\(f(t)\)</span>, en un conjunto de <span class="math inline">\(N/2 +1\)</span> ondas cosinusoidales y <span class="math inline">\(N/2 + 1\)</span> ondas sinusoidales, con las frecuencias definidas por el índice <span class="math inline">\(p=0,1,...,N/2\)</span>, i.e.&nbsp;$_p= 2f_p = 2p / T = 2p / N t $. Las amplitudes de los cosenos estan contenidas en <span class="math inline">\(Re [F(p)]\)</span> y las amplitudes de los senos en <span class="math inline">\(Im [F(p)]\)</span>. Note que las frecuencias son siempre <em>positivas</em>, es decir, los índices <span class="math inline">\(k\)</span> siempre van de cero a <span class="math inline">\(N/2\)</span>. Las frecuencias entre <span class="math inline">\(N/2\)</span> y <span class="math inline">\(N-1\)</span> son <em>negativas</em>. Recuerda que el espectro frecuencial de una señal discreta es periódico, y entonces las frecuencias son negativas entre <span class="math inline">\(N/2\)</span> y <span class="math inline">\(N-1\)</span> al igual que en el intervalo <span class="math inline">\(-N/2\)</span> y <span class="math inline">\(-1\)</span>. Los puntos <span class="math inline">\(0\)</span> y <span class="math inline">\(N/2\)</span> separan las frecuencias negativas de las positivas. Es por ello que, generalmente, solamente centramos nuestra atención en la parte positiva del espectro. La magnitud (o norma) de la transformada de Fourier discreta es</p>
<p><span class="math display">\[{ Magnitud}=|F(p)|=\sqrt{Re [F(p)]^2 + Im[F(p)]^2}\,,\]</span></p>
<p>y la fase es</p>
<p><span class="math display">\[Phase=tan^{-1}\left( \frac{Im[F(p)]}{Re [F(p)]} \right)\,.\]</span></p>
<p>La FFT organiza los coeficientes de Fourier (imaginarios y reales) en frecuencias <em>negativas</em> y <em>positivas</em> y reparte la varianza de la señal equitativamente entre ellas. En <span class="math inline">\(p=0\)</span> tenemos la media de la serie temporal, aunque debido a que hemos eliminado la media y la tendencia de la serie temporal no debemos de preocuparnos por ella. Entre <span class="math inline">\(p=1,...,N/2\)</span> tenemos los valores de los coeficientes de Fourier reales y entre <span class="math inline">\(p=N/2+1,...,N-1\)</span> tenemos los complejos conjugados de los primeros <span class="math inline">\(N/2\)</span> coeficientes. Si calculamos el valor absoluto de la transformada de Fourier (en Matlab <span class="math inline">\({ abs}({ fft}(f(t)))/N\)</span>) estamos calculando <span class="math inline">\(A_p^2 + B_p^2\)</span> y si solo nos quedamos con los primeros <span class="math inline">\(N/2\)</span> elementos de la FFT, debemos de multiplicar por un factor de <span class="math inline">\(2\)</span> para conservar la energía espectral.</p>
<section id="estimaciones-espectrales-o-autoespectros" class="level4">
<h4 class="anchored" data-anchor-id="estimaciones-espectrales-o-autoespectros">Estimaciones espectrales o autoespectros</h4>
<ol type="1">
<li><em>Espectro de amplitud</em></li>
</ol>
<p>La gráfica de la magnitud de los coeficientes complejos <span class="math inline">\(|C_p^*|\)</span> de la serie de Fourier <span class="math display">\[f(t)=\sum\limits^\infty_{p=-\infty} C^*_p e^{i\omega_p t_n}\]</span> frente a (versus) la frecuencia <span class="math inline">\(\omega_p\)</span> se denomina espectro de amplitud de la función periódica <span class="math inline">\(f(t)\)</span>. En Matlab (para las primeras <span class="math inline">\(N/2\)</span> componentes), <span class="math display">\[{ Amplitud}=2*|C^*_p|=abs({ fft}(f(t)))/N\,.\]</span></p>
<ol start="2" type="1">
<li><strong>Espectro de densidad de potencia</strong> (Power Spectral Density, PSD)</li>
</ol>
<p>El espectro de densidad de potencia (PSD, por sus siglas en inglés) es la potencia de la FFT por unidad de frecuencia <span class="math display">\[{\it PSD}(p)=2*|C^*_p|^2/\Delta f\]</span> donde <span class="math inline">\(\Delta f=1/N\Delta t\)</span> es la frecuencia fundamental.</p>
<p>La gráfica de <span class="math inline">\({ PSD}(p)\)</span> de la frente a (versus) la frecuencia <span class="math inline">\(\omega_p\)</span> se denomina espectro de densidad de potencia de la función periódica <span class="math inline">\(f(t)\)</span>. Si solo nos quedamos con las <span class="math inline">\(N/2\)</span> primeras componentes en Matlab se escribe <span class="math display">\[{\it PSD}(p)=2*abs({ fft}(f(t)))^2/N^2/\Delta f\,.\]</span></p>
<p>Esta normalización tiene su fundamento en el cumplimiento del teorema de Parseval, de tal forma que la energía total de la señal en el dominio temporal <span class="math inline">\(f(t)\)</span> (por unidad de tiempo) sea igual a la energía total de la señal en el dominio frecuencial definido por <span class="math inline">\(C^*_p\)</span>: <span class="math display">\[\frac{1}{T}\sum\limits^N_{n=1}|f(t_n)|^2 \Delta t =\frac{1}{N}\sum\limits^N_{n=1}|f(t_n)|^2 =
var(f(t))=\sum\limits^{\infty}_{p=-\infty} |C^*_p|^2=\sum\limits^{N/2}_{p=0}PSD(p)*\Delta f\,.\]</span><br>
<br>
Este teorema de conservacón de energía nos informa de que la integral bajo la curva espectral <span class="math inline">\({ PSD}(p)\)</span> debe ser igual a la varianza total de la serie temporal.</p>
<p><strong>Efectos de los extremos en estimaciones espectrales</strong></p>
<p>En general, para calcular un espectro promedio debemos fragmentar nuestra serie temporal en bloques de igual tamaño que contengan las frecuencias de interés, realizar espectros individuales de dichos fragmentos, y promediar todos ellos. Este método se le conoce como Welch. Los fragmentos pueden ser únicos, es decir, sin superposición o bien pueden ser recursivos, es decir, cuando utilizamos superposición de fragmentos. Por ejemplo, una superposición del 50% significa que cada fragmento empieza en la mitad del fragmento anterior. Además de eliminar el ruido, el método de Welch también reduce la transferencia de energía de las frecuencias pico hacia frecuencias colindantes (<code>leakage' en inglés). El método de Welch reduce el ruido causado por el uso de datos imperfectos y por el efecto</code>leakage’. Aqui les muestro un ejemplo de como promediar un espectro de densidad espectral con fregmentos de tamaño <span class="math inline">\(M\)</span>:</p>
<p>Este problema de transferencia de energía o `leakage’ es intrínseco al problema de que las series temporales oceanográficas son finitas y, por lo tanto, no son necesariamenteperiódicas, condición necesaria en el análisis de Fourier. Veamos esto con un ejemplo de una onda cosinusoidal periódica y no-periódica.</p>
<p>Podemos observar como en el caso de la serie no-periódica existe una transferencia de energía del pico espectral hacia las frecuencias colindantes de forma que se reduce la amplitud del pico de interés.</p>
<p><strong>Correlación</strong></p>
<p>y <span class="math inline">\(s_y\)</span> son las varianzas de las variables <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span>, respectivamente. \end{framed}</p>
<p>Kundu (1976b) define la función de correlación cruzada desfasada entre dos series de velocidad en las profundidades 1 y 2 como <span class="math display">\[\rho_{\tau}=\frac{\overline{u_1'(t)u_2'(t-\tau)}}
{\left[ \overline{u_1'(t)^2}\,\, \overline{u_2'(t)^2} \right]^{1/2}}\,,\]</span> donde las primas <span class="math inline">\('\)</span> indican anomalias. Esta función fue utilizada para estudiar la propagación vertical de ondas inercio-gravitatorias y la velocidad de fase de estas ondas <span class="math inline">\(c=\Delta_{12}/\tau\)</span>. Kundu (1976a) introduce el coeficiente de correlación complejo</p>
<p><span class="math display">\[\rho=\frac{\overline{w_1^*(t)w_2(t)}}
{\left[ \overline{w_1^*(t)w_1^*(t)}\,\, \overline{w_2^*(t)w_2(t)} \right]^{1/2}}\,,\]</span></p>
<p>donde <span class="math inline">\(w=u+i v\)</span>, los asteriscos <span class="math inline">\(*\)</span> indican complejo conjugados, y los subíndices <span class="math inline">\(1\)</span> y <span class="math inline">\(2\)</span> se refieren a dos estaciones de medida. La cantidad <span class="math inline">\(\rho\)</span> es un número complejo cuya magnitud (<span class="math inline">\(\le1\)</span>) nos da una medida de correlación promedia y cuyo ángulo de fase da el angulo promedio, medido en el sentido contrario a las agujas del reloj, del segundo vector con respecto del primero. Por ejemplo, un ángulo de fase negativo entre las profundidades <span class="math inline">\(50\)</span> y <span class="math inline">\(100\,{ m}\)</span> implica que la señal llega primero a <span class="math inline">\(z=-100\,{ m}\)</span> y luego a <span class="math inline">\(z=-50\,{ m}\)</span>, es decir, podría tratarse de una onda interna cuyas fases se propagan hacia arriba.</p>
</section>
<section id="espectro-cruzado" class="level4">
<h4 class="anchored" data-anchor-id="espectro-cruzado">Espectro cruzado</h4>
<p>Con análisis de espectros cruzados pretendemos comprender la relación entre dos series temporales en función de la frecuencia. Por ejemplo, observamos en dos localizaciones espectros con picos en las mismas frecuencias y queremos saber si dichos armónicos estan relacionados.</p>
<p>Supongamos dos series de Fourier <span class="math inline">\(x(t)\)</span> e <span class="math inline">\(y(t)\)</span> <span class="math display">\[x(t)=\bar{x}+\sum\limits^{N/2}_{p=1} A_{xk} cos(\omega_p t_n) + B_{xp}sen(\omega_p t_n)\,,\]</span> <span class="math display">\[y(t)=\bar{y}+\sum\limits^{N/2}_{p=1} A_{yk}cos(\omega_p t_n) + B_{yp}sen(\omega_p t_n)\,.\]</span> Utilizando las condiciones ortogonalidad entre las funciones sinusoidales y cosinusoidal, la covarianza entre las variables <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span> es <span class="math display">\[\overline{x'y'}=\sum\limits^{N/2}_{p=1} \frac{1}{2}( A_{xp}A_{yp}  + B_{xp}B_{yp})=\sum\limits^{N/2}_{p=1}Co(p)\,,\]</span> donde <span class="math inline">\(Co(p)\)</span> es el co-espectro de <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span>.</p>
<p>Supongamos dos series de Fourier <span class="math inline">\(x(t)\)</span> e <span class="math inline">\(y(t)\)</span> definidas en la forma compleja (el asterisco ha sido eliminado en esta notación) <span class="math display">\[x(t)=\bar{x}+\sum\limits^{N/2}_{p=1}
  C_{xp} e^{i\omega_p t_n}=\bar{x}+
  \sum\limits^{N/2}_{p=1}\frac{1}{2}\left(A_{xp} - iB_{xp} \right)e^{i\omega_p t_n}=\bar{x}+
  \sum\limits^{N/2}_{p=1}F_x(p)\]</span> <span class="math display">\[y(t)=\bar{y}+\sum\limits^{N/2}_{p=1}
  C_{yk} e^{i\omega_p t_n}=\bar{y}+
  \sum\limits^{N/2}_{p=1}\frac{1}{2}\left(A_{yp} - iB_{yp} \right)e^{i\omega_p t_n}=\bar{y}+
  \sum\limits^{N/2}_{p=1}F_y(p)\,.\]</span></p>
<p>Si ahora calculamos las varianzas <span class="math display">\[\overline{x'^2}=\sum\limits^{N/2}_{p=-N/2}F_{xx}(p)\,,\]</span></p>
<p>donde <span class="math display">\[F_{xx}(p)=2\frac{1}{2}\left(A_{xp} - iB_{xp} \right)e^{i\omega_p t_n}\frac{1}{2}\left(A_{xp} + iB_{xp} \right)e^{-i\omega_p t_n}
=2F_x(p) F^*_x(p)=|C_{xp}|^2\,,\]</span></p>
<p>y el asterisco indica complejo conjugado. Para la variable <span class="math inline">\(y\)</span> de igual forma obtenemos:</p>
<p><span class="math display">\[\overline{y'^2}=\sum\limits^{N/2}_{p=-N/2}F_{yy}\,;\,\,\,\,\,F_{yy}(p)=2F_y(p) F^*_y(p)=|C_{yp}|^2\]</span></p>
<p>De las expresiones anteriores se deduce que covarianza se puede calcular en el espacio espectral como</p>
<p><span class="math display">\[\overline{x'y'}=Re\left[\sum\limits^{N/2}_{p=-N/2}F_{xy}(p)\right]\,,\]</span> donde <span class="math display">\[F_{xy}(p)=2\frac{1}{2}\left(A_{xp} - iB_{xp} \right)e^{i\omega_p t_n}\frac{1}{2}\left(A_{yp} + iB_{yp} \right)e^{-i\omega_p t_n}
=2F_x(p) F^*_y(p)=|C_{xp}||C_{yp}|e^{i(\theta_{xp}-\theta_{yp})}\,.\]</span></p>
<p>El factor <span class="math inline">\(e^{i(\theta_{xp}-\theta_{yp})}\)</span> aparece para considerar que ambas series periódicas no estan en fase.</p>
<ol start="4" type="1">
<li><strong>Espectro cruzado complejo</strong></li>
</ol>
<p>Si escribimos <span class="math inline">\(F_{xy}(p)\)</span> en términos de los coeficientes de Fourier reales <span class="math display">\[F_x(p) F^*_y(p)=\frac{1}{2}\left(A_{xp} - iB_{xp} \right)e^{i\omega_p t_n}\frac{1}{2}\left(A_{yp} + iB_{yp} \right)e^{-i\omega_p t_n}=\]</span> <span class="math display">\[=\frac{1}{4}\left[A_{xk}A_{yk}  + B_{xk}B_{yp} + i\left(A_{xp}B_{yp} - A_{yp}B_{xp}\right)\right]\,.\]</span> Para el caso de series <span class="math inline">\(x(t)\)</span> e <span class="math inline">\(y(t)\)</span> reales sabemos que las frecuencias negativas son los complejos conjugados de las frecuencias positivas y entonces <span class="math display">\[A_k=A_{-k}\,\,\,\,\,{ y}\,\,\,\,\,B_k=B_{-k}\,,\]</span> y <span class="math display">\[F_x(p)F^*_y(p)=F_x(-p)F_y^*(-p)\,,\]</span> y como conclusión</p>
<p><span class="math display">\[F_{xy}(p)+F_{xy}(-p)=\frac{1}{2}\left[ A_{xp}A_{yp}  + B_{xp}B_{yp} + i\left(A_{xp}B_{yp} - A_{yp}B_{xp}\right)\right]\,,\]</span></p>
<p>que es espectro cruzado de <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span> para el armónico <span class="math inline">\(p\)</span>. De esta expresión encontramos que</p>
<p><span class="math display">\[F_{xy}(p)+F_{xy}(-p)=2F_{xy}(p)=Co(p) + i Q(p)\,,\]</span> donde <span class="math inline">\(Co(p)=\frac{1}{2}( A_{xp}A_{yp} + B_{xp}B_{yp})\)</span> es el co-espectro del armónico p</p>
<p>y <span class="math inline">\(Q(p)=\frac{1}{2}(A_{xp}B_{yp} - A_{yp}B_{xp})\)</span> es el espectro de cuadratura del armónico <span class="math inline">\(k\)</span>.</p>
<p>En notación compleja el espectro cruzado <span class="math display">\[F_{xy}(p)=C_{xp}C_{yp}e^{i(\theta_{xp}-\theta_{yp})}=C_{xp}C_{yp}\left(cos(\theta_{xp}-\theta_{yp})+ isen(\theta_{xp}-\theta_{yp}) \right)\,.\]</span></p>
<p><span class="math display">\[\theta_{xp}=\theta_{yp}\,\,\,\,\,\text{entonces}\,\,\,\,F_{xy}(p) \text{es real}\]</span> <span class="math display">\[\theta_{xp}\ne\theta_{yp}=\pm\frac{\pi}{2}\,\,\,\,\,\text{entonces}\,\,\,\,F_{xy}(p) \text{es complejo}\]</span></p>
<p>Entonces el co-espectro (la parte real del espectro cruzado) esta en fase con la señal y el espectro de cuadratura esta totalmente desfasado.</p>
<ol start="5" type="1">
<li><strong>Espectro de coherencia</strong></li>
</ol>
<p>Para una única componente <span class="math inline">\(p\)</span>, el espectro de coherencia al cuadrado entre dos series <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span> se define</p>
<p><span class="math display">\[Coh^2(p)=\frac{|F_{xy}(p)|^2}{F_{xx}F_{yy}}=\frac{|C_{xp}C_{yp}|^2}{C_{xp}^2C_{yp}^2}\,,\]</span></p>
<p>donde <span class="math inline">\(|Coh^2(p)|^{1/2}\)</span> es su magnitud y <span class="math inline">\(\phi_{xy}(p)\)</span> es el ángulo de desfase entre las dos componentes <span class="math inline">\(p\)</span> de <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span>.</p>
<p>El espectro de coherencia al cuadrado nos indica el grado de correlación existente entre dos señales. Dos señales estan altamente correlacionadas si la magnitud del espectro de coherencia al cuadrado es <span class="math inline">\(\simeq 1\)</span> y su fase es <span class="math inline">\(\phi_{xy}(p)\simeq 0\)</span>.</p>
</section>
</section>
</section>
<section id="métodos-de-filtrado-y-suavizado" class="level2">
<h2 class="anchored" data-anchor-id="métodos-de-filtrado-y-suavizado">Métodos de filtrado y suavizado</h2>
<section id="convolución-y-funciones-respuesta-ventanas-espectrales" class="level3">
<h3 class="anchored" data-anchor-id="convolución-y-funciones-respuesta-ventanas-espectrales">Convolución y funciones respuesta (ventanas espectrales)</h3>
<p>La convolución de dos funciones <span class="math inline">\(f(t)\)</span> y <span class="math inline">\(g(t)\)</span> sobre un registro finito <span class="math inline">\([0,T]\)</span> se define como</p>
<p><span class="math display">\[[f*g](t)=\frac{1}{T}\int\limits^{T}_{0}f(\tau)g(t-\tau) d\tau\,.\]</span></p>
<p>O también se puede expresar sobre un registro infinito como</p>
<p><span class="math display">\[[f*g](t)=\int\limits^{\infty}_{-\infty}f(\tau)g(t-\tau) d\tau = \int\limits^{\infty}_{-\infty}g(\tau)f(t-\tau) d\tau\,.\]</span></p>
<p>La convolución satisface las siguientes propiedades</p>
<ul>
<li><p><span class="math display">\[f*g=g*f\]</span></p></li>
<li><p><span class="math display">\[f*(g*h)=(f*g)*h\]</span></p></li>
<li><p><span class="math display">\[f*(g+h)=(f*g)+(f*h)\]</span></p></li>
</ul>
<p>Ahora retomemos las definiciones de serie de Fourier y la transformada de Fourier:</p>
<p>Vamos ahora a deducir el teorema de la convolución. Para ello vamos a partir de la definición de convolución:</p>
<p><span class="math display">\[f*g=\frac{1}{T}\int\limits^{T}_{0}f(\tau)g(t-\tau) d\tau=
      \frac{1}{T}\int\limits^{T}_{0}f(\tau)\sum\limits^{\infty}_{p=-\infty} G(\omega_p) e^{i\omega_p (t-\tau)} d\tau=\]</span></p>
<p><span class="math display">\[=\sum\limits^{\infty}_{p=-\infty} G(\omega_p) \left[\frac{1}{T}\int\limits^{T}_{0}f(\tau)e^{-i\omega_p \tau} d\tau \right]
e^{i\omega_p t}=\sum\limits^{\infty}_{p=-\infty} G(\omega_p)F(\omega_p)
e^{i\omega_p t}={\cal F}^{-1}[G(\omega)F(\omega)](t)\,.\]</span></p>
<p>Si aplicamos transformada de Fourier a ambos lados del igual obtenemos:</p>
<p><span class="math display">\[{\cal F}(f*g)=G(\omega_p)F(\omega_p)={\cal F}[g(t)]{\cal F}[f(t)]\,,\]</span></p>
<p>es decir, la transformada de Fourier de la convolución de <span class="math inline">\(f\)</span> y <span class="math inline">\(g\)</span> es equivalente a multiplicar en el espacio espectral las transformadas de Fourier de las funciones individuales. La correlación cruzada desfasada de <span class="math inline">\(f(t)\)</span> y <span class="math inline">\(g(t)\)</span> en forma integral se puede definir como</p>
<p><span class="math display">\[C_{fg}(\tau)=\frac{1}{T}\int\limits^{T}_{0} f(\tau) g(t+\tau) d\tau={\cal F}^{-1}[G(\omega)F(-\omega)](t)=
{\cal F}^{-1}[G(\omega)F^*(\omega)](t)\,,\]</span></p>
<p>es decir, si multiplicamos la transformada de Fourier de una función por el complejo conjugado de la transformada de Fourier de otra función es equivalente a la transformada de Foruier de la correlación cruzada desfasada entre ellas. Este se le conoce por el teorema de correlación. Para el caso particular que sea la misma función <span class="math inline">\(g(t)\)</span> la que se correlaciona, entonces:</p>
<p><span class="math display">\[{\cal F}[C_{gg}(\tau)]=G(\omega)G^*(\omega)=|G(\omega)|^2\,,\]</span></p>
<p>es decir, la transformada de Fourier de la autocorrelación es igual al espectro de potencia de la función <span class="math inline">\(g(t)\)</span>. Este se denomina el teorema de Weiner-Khinchin.</p>
<p>El concepto de convolución es útil en el filtrado de señales periódicas. En general vamos a convolucionar nuestra señal <span class="math inline">\(f(t)\)</span> con la denominada función respuesta <span class="math inline">\(r(t)\)</span>. La función <span class="math inline">\(r(t)\)</span> es típicamente una función pico que cae a cero en ambas direcciones desde el máximo (o pico).</p>
<p>Puesto que la función respuesta es mas ancha que algunas estructuras de pequeña escala de nuestra señal original, estas serán suavizadas tras realizar la convolución. \ \ {OTA:} Por el teorema de convolución filtrar en el dominio temporal convolucionando es equivalente a multiplicar la transformada de Fourier de la señal con la transformada de Fourier de la función respuesta.</p>
<p>Las ventanas mas comunes para suavizar señales son las de:&nbsp;</p>
<ol type="1">
<li>`Boxcar’ <span class="math display">\[\begin{equation*}
  r(t)=
  \left\lbrace
  \begin{array}{l}
1 \text{ if } 0\le t\le T \\
0 \text{ if } t&gt;T \\
  \end{array}
  \right.
\end{equation*}\]</span></li>
</ol>
<p>La transformadad de Fourier es la función <em>sinc</em></p>
<p><span class="math display">\[R(\omega)={\it sinc}=\frac{sin\left(\frac{\omega T}{2}\right)}{\frac{\omega T}{2}}\,.\]</span></p>
<p>Esta función respuesta tiende a cero cuando <span class="math inline">\(\omega T / 2\)</span> se acerca a cero, es decir, para <span class="math inline">\(\omega T=2 n \pi;\,\,\text{para}\,\,n=1,2,3,...\)</span>. Esta no es una ventana o función respuesta recomendable debido a los lóbulos de menor amplitud alrededor del pico. En general respuestas tipo ondas sinusoidales o cosinusoidales amortiguadas a ambos lados del pico no son deseables.</p>
<ol start="2" type="1">
<li><strong>Hanning</strong></li>
</ol>
<p><span class="math display">\[\begin{equation*}
  r(t)=\left\lbrace
  \begin{array}{l}
     \frac{1}{2}\left( 1-cos\frac{2\pi t}{T}\right)
      \text{ if } -T/2 \le t \le T/2 \\
     0 \text{ if } { contrario} \\
  \end{array}
  \right.
  \end{equation*}\]</span> &nbsp; (3) <strong>Hamming</strong></p>
<p><span class="math display">\[\begin{equation*}
  r(t)=\left\lbrace
  \begin{array}{l}
     \left(0.54  + 0.46 cos(\frac{\pi t}{T}) \right) \text{ if } -T/2 \le t \le T/2 \\
     0 \text{ if } { contrario} \\
  \end{array}
  \right.
  \end{equation*}\]</span></p>
<p>Es evidente que este tipo de filtro es un suavizado o filtro pasa bajo. Sin embargo, siempre podemos recuperar facilmente la señal de alta frecuencia (filtro pasa altas) restando a la señal original la serie suavizada con convolución.</p>
<p>Imaginemos que tenemos una serie temporal <span class="math inline">\(u(t)\)</span> con un paso temporal de <span class="math inline">\(dt=1\,{ h}\)</span>. Entonces, para suavizar <span class="math inline">\(u(t)\)</span> de tal forma que se eliminen las señales con periodos menores de <span class="math inline">\(T=48\,{ h}\)</span>, es decir, un filtro pasa baja con frecuencia de corte <span class="math inline">\(1/48\,{ h}^{-1}\)</span> debemos convolucionar <span class="math inline">\(u(t)\)</span> con una función de resouesta <span class="math inline">\(r(t)\)</span> o ventana. En este ejemplo <em>Matlab</em> se muestra como programar un suavizado</p>
<p>Ya hemos visto que podemos suavizar una señal simplemente con la convolución en el dominio temporal de la señal con una ventana o función respuesta (Boxcar, Hanning, Hamming,etc.). Suavizar una señal es comparable a un filtro de pasa baja, es decir, un filtro que solamente deja pasar las frecuencias bajas y elimina (pone a cero) las altas frecuencias. De forma ideal los filtros en el dominio frecuencial los representamos como: \</p>
<ol type="1">
<li>Pasa baja (filtra las altas frecuencias), <span class="math display">\[\begin{equation*}
|R(\omega)|=
  \left
  \lbrace
  \begin{array}{l}
1 \text{ si } |\omega| \le \omega_c \\
0 \text{ si } \omega_c \le \omega   \\
  \end{array}
  \right.
  \end{equation*}\]</span> \</li>
<li>Pasa banda (filtra las frecuencias fuera de la banda) <span class="math display">\[\begin{equation*}
|R(\omega)|=\left\lbrace
  \begin{array}{l}
1 \text{ si } \omega_{c1}\le|\omega|\le \omega_{c2} \\
0 \text{ si } \text({ lo}\,\,\,{ contrario}) \\
  \end{array}
  \right.
  \end{equation*}\]</span> \</li>
<li>Pasa alta (filtra las bajas frecuencias) <span class="math display">\[\begin{equation*}
|R(\omega)|=\left\lbrace
  \begin{array}{l}
0 \text{ si } |\omega|\le \omega_c \\
1 \text{ si } \omega_c\le\omega \\
  \end{array}
  \right.
  \end{equation*}\]</span></li>
</ol>
</section>
<section id="promedio-corrido" class="level3">
<h3 class="anchored" data-anchor-id="promedio-corrido">Promedio corrido</h3>
<p>Veamos primero un ejemplo sencillo de filtrado paso bajo con un promedio corrido de dos puntos. Este sería el caso de filtrar utilizando la función <em>smooth.m</em> de Matlab. Para dos puntos sería <span class="math inline">\(&gt;&gt; f_{suavizada}=smooth(f,2);\)</span>. \ Promedio corrido con dos puntos es simplemente el valor promedio <span class="math display">\[y(n)=\frac{s(n) + s(n-1)}{2}\,,\]</span> donde <span class="math inline">\(s(n)\)</span> es una señal periódica, <span class="math inline">\(s(-1)=s(N)\)</span>, <span class="math inline">\(N\ge 2\)</span>. La función <span class="math inline">\(y(n)\)</span> es una versión suavizada con altas frecuencias eliminadas y bajas frecuencias mantenidas. Para ver esto definimos <span class="math inline">\(s(n)=sen(2\pi f n/N)\)</span>, y entonces <span class="math display">\[y(n)=\frac{1}{2} s(n) + \frac{1}{2} s(n-1)=\frac{1}{2} sen(2\pi f n/N) + \frac{1}{2} sen(2\pi f (n-1)/N)=\]</span> <span class="math display">\[=\frac{1}{2}sen(2\pi f n/N) + \frac{1}{2}\left[ sen(2\pi f n/N)cos(2\pi f/N) - cos(2\pi f n/N)sen(2\pi f/N)\right]=\]</span> <span class="math display">\[=\frac{1}{2}\left[ 1+cos(2\pi f/N)\right]sen(2\pi f n/N) - \frac{1}{2} cos(2\pi f n/N)sen(2\pi f/N)=\]</span> <span class="math display">\[=A_1sen(2\pi f n/N) - A_2cos(2\pi f n/N)\,,\]</span> donde <span class="math display">\[A_1=\frac{1}{2}\left[ 1+cos(2\pi f/N)\right]\,\,;\,\,\,\,\,A_2=sen(2\pi f/N)\,.\]</span></p>
<p>Para bajas frecuencias, es decir, <span class="math inline">\(f\sim 0\)</span> se cumple que <span class="math inline">\(A_1\sim 1\)</span> y <span class="math inline">\(A_2 \sim 0\)</span> y entonces <span class="math display">\[y(n)\sim sen(2\pi f n/N)=s(n)\]</span> y las bajas frecuencias son prácticamente mantenidas. Por el contrario para altas frecuencias, es decir, <span class="math inline">\(f\sim N/2\)</span>, <span class="math inline">\(A_1\sim 0\)</span>, <span class="math inline">\(A_2\sim0\)</span>, y entonces <span class="math inline">\(y(n)\sim0\)</span> y consecuentemente las altas frecuencias son prácticamente eliminadas.</p>
<p>La fórmula general para el promedio corrido es <span class="math display">\[y(n)=\frac{1}{M}\sum\limits^{(M-1)/2}_{p=-(M-1)/2} s(n+p)\,,\]</span> donde <span class="math inline">\(y( )\)</span> es el valor de la serie filtrada y <span class="math inline">\(s( )\)</span> es la serie original sin filtrar, <span class="math inline">\(M\)</span> es el número de puntos usados en el promedio. Por ejemplo, en un promedio corrido de <span class="math inline">\(5\)</span> puntos, el valor en el punto <span class="math inline">\(30\)</span> será <span class="math display">\[y(30)=\frac{s(28)+s(29)+s(30)+s(31)+s(32)}{5}\,.\]</span></p>
<p>Es evidente que este tipo de filtro es un suavizado o filtro pasa bajo. Sin embargo, podemos recuperar facilmente la señal de alta frecuencia restando a la señal original la serie filtrada.</p>
</section>
<section id="filtros-generales-coseno" class="level3">
<h3 class="anchored" data-anchor-id="filtros-generales-coseno">Filtros generales coseno</h3>
<p>Supongamos un simple filtro simétrico obtenido como la convolución entre una función de pesos <span class="math inline">\(r(t)\)</span> y la señal <span class="math inline">\(x(t)\)</span></p>
<p><span class="math display">\[y_n=\sum\limits^{\infty}_{p=-\infty} r_p x_{n-p}\,\,\,\,{ donde}\,\,\,\,r_p=r_{-p}\,,\]</span></p>
<p>son pesos elegidos adecuadamente. El efecto de filtrado se observa mejor en el dominio frecuencial. Queremos calcular la transformada de Fourier de una serie temporal <span class="math inline">\(f(t)\)</span>, la cual ha sido desfasada un tiempo <span class="math inline">\(\Delta t = a\)</span>:</p>
<p><span class="math display">\[f(t\pm a)=\int\limits^{\infty}_{-\infty} F(\omega)e^{i\omega (t\pm a)} d{\omega}=
\int\limits^{\infty}_{-\infty}  \left[F(\omega) e^{i \omega t}\right]e^{\pm i \omega a} d{\omega}\]</span></p>
<p>De esta expresión deducimos que la transformada de Fourier de una serie desfasada por un intervalo de tiempo <span class="math inline">\(\Delta t\)</span> es igual a la transformada de Fourier de la serie no desfasada multiplicada por un factor <span class="math display">\[e^{\pm i\omega \Delta t}\,.\]</span> Usando este resultado, la transformada de Fourier de <span class="math inline">\(y_n\)</span> se puede escribir como <span class="math display">\[Y(\omega)={\cal F}[y_n]=\sum\limits^{\infty}_{p=-\infty} r_p e^{-i\omega_p \Delta t} X(\omega)\,,\]</span> donde <span class="math inline">\(X(\omega)\)</span> y <span class="math inline">\(Y(\omega)\)</span> son la transformada de Fourier de <span class="math inline">\(y(t)\)</span> y <span class="math inline">\(x(t)\)</span>, y la función respuesta en el dominio frecuencial es</p>
<p><span class="math display">\[R(\omega)=\frac{Y(\omega)}{X(\omega)}=\sum\limits^{\infty}_{p=-\infty} r_p e^{-i\omega_p \Delta t}\,.\]</span></p>
<p>{}Puesto que <span class="math inline">\(r_p=r_{-p}\)</span> y</p>
<p><span class="math display">\[{ cos}(x)=\frac{e^{ix} + e^{-ix}}{2}\,,\]</span></p>
<p>podemos escribir la función respuesta del filtrado deseado como</p>
<p><span class="math display">\[R(\omega)=\sum\limits^{\infty}_{p=-\infty} r_p e^{-i\omega_p \Delta t}=
r_0 + \sum\limits^{\infty}_{p=1} r_p e^{i\omega_p \Delta t}
+ \sum\limits^{\infty}_{p=1} r_{-p} e^{-i\omega_p \Delta t} =
r_0 + \sum\limits^{\infty}_{p=1} r_p \left[ e^{i\omega_p \Delta t} + e^{-i\omega_p \Delta t}\right]=
\]</span> <span class="math display">\[=r_0 + 2\sum\limits^{\infty}_{p=1} r_p \left[ \frac{e^{i\omega_p \Delta t} + e^{-i\omega_p t}}{2}\right]=
r_0 + 2\sum\limits^{\infty}_{p=1}r_p cos(\omega_p \Delta t)\]</span></p>
<p>En general los pesos <span class="math inline">\(r_p\)</span> se van a calcular utilizando la siguiente expresión: <span class="math display">\[r_p=\frac{1}{\omega_N}\int\limits^{\omega_N}_0 R(\omega_p) cos(\omega_p \Delta t
)d{\omega}\]</span></p>
<p>Por ejemplo para un filtro pasa bajo <span class="math inline">\(R(\omega)=1\)</span> para <span class="math inline">\(0&lt;|\omega_p|\le \omega_c\)</span> y la integral para calcular los pesos queda <span class="math display">\[r_p=\frac{1}{\omega_N}\int\limits^{\omega_c}_0 R(\omega_p) cos(\omega_p \Delta t
)d{\omega}=\frac{\omega_c}{\omega_N}\frac{sen(\omega_c p \Delta t)}{\omega_c p \Delta t}=
      \frac{1}{\omega_N}\frac{sen(\pi p \omega_c / \omega_N)}{\pi p / \omega_N}=\]</span> <span class="math display">\[=\frac{sen(\pi p \omega_c / \omega_N)}{\pi p}\,\,\,\,\,\,\,\,\,\,p=1,2,...,N\]</span></p>
<p>Para <span class="math inline">\(p=0\)</span> entonces</p>
<p><span class="math display">\[r_0=\frac{1}{\omega_N}\int\limits^{\omega_c}_0 R(\omega_p) cos(\omega_p \Delta t
)d{\omega}=\frac{1}{\omega_N}\int\limits^{\omega_c}_0 d{\omega}=\frac{\omega_c}{\omega_N}\,.\]</span></p>
<p>Y la función respuesta es</p>
<p><span class="math display">\[R(\omega)=\frac{\omega_c}{\omega_N} + 2\sum\limits^{\infty}_{p=1}
\frac{sen(\pi p \omega_c / \omega_N)}{\pi p}cos(\pi p \omega / \omega_N)\]</span></p>
<p>Veamos que forma tiene este filtro en el dominio frecuencial, asumiendo un número finito de <span class="math inline">\(N\)</span> coeficientes de Fourier, i.e., <span class="math inline">\(p=1,2,...,N\)</span>, frecuencia de Nyquist <span class="math inline">\(f_N=1\)</span> y frecuencia de corte <span class="math inline">\(f_c=1\)</span>:</p>
<p>En la figura observamos oscilaciones con longitud de onda <span class="math display">\[\lambda=\frac{4f_N}{2N+1}\,,\]</span></p>
<p>Esta longitud de onda coincide con el ancho de banda de transición del filtro, es decir, del pico hasta la base indicado en la figura por las líneas rojas. Para filtrar únicamente debemos: &nbsp;</p>
<ol type="i">
<li>multiplicar la respuesta espectral <span class="math inline">\(R(f)\)</span> por la transformada de Fourier de la señal y regresar con la transformada inversa</li>
</ol>
<p><span class="math display">\[x(t)[filtrado]={\cal F}^{-1}[R(f)X(f)]\,,\]</span><br>
(ii) convolucionar la respuesta en el dominio temporal <span class="math inline">\(r(t)\)</span> por la serie temporal. <span class="math display">\[x(t)[{ filtrado}]=[r(t)*x](t)\,.\]</span><br>
</p>
<p>{}Si queremos un filtro pasa alta, usamos <span class="math inline">\(r_p({ pasa}\,\,\,{ alto})=1-r_p\)</span>. Y la función respuesta sería <span class="math display">\[R(\omega)[{ pasa}\,\,\,{ alto}]=1-R(\omega)=1-\frac{\omega_c}{\omega_N} - 2\sum\limits^{\infty}_{p=1}
\frac{sen(\pi p \omega_c / \omega_N)}{\pi p}cos(\pi p \omega / \omega_N)\]</span><br>
Veamos ahora de nuevo el promedio corrido pero esta vez usando el método de Fourier. De nuevo decir que el promedio corrido reemplaza el valor central de la ventana por el promedio de los valores que rodean a ese punto. Para este ejemplo los pesos son siempre iguales <span class="math inline">\(r_p=1/T\)</span> para el intervalo <span class="math inline">\(-N&lt;p&lt;N\)</span>, donde <span class="math inline">\(T=1/(2N+1)\)</span> es el tamaño de la ventana `boxcar’. De esta forma &nbsp;</p>
<p><span class="math display">\[T=2N+1=3\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
R(\omega)=\frac{1}{3} + \frac{2}{3}cos(\omega \Delta t)
\,\,\,\,\,\,\,\,\,\,\,\,0&lt;\omega&lt;\pi/\Delta t\]</span></p>
<p><span class="math display">\[T=2N+1=5\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
R(\omega)=\frac{1}{5} + \frac{2}{5}cos(\omega \Delta t)+ \frac{2}{5}cos(2\omega \Delta t)
\,\,\,\,\,\,\,\,\,\,\,\,0&lt;\omega&lt;\pi/\Delta t\]</span></p>
<p><span class="math display">\[T=2N+1=7\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
R(\omega)=\frac{1}{7} + \frac{2}{7}cos(\omega \Delta t)+ \frac{2}{7}cos(2\omega \Delta t)+ \frac{2}{7}cos(3\omega \Delta t)
\,\,\,\,\,\,\,\,\,\,\,\,0&lt;\omega&lt;\pi/\Delta t\]</span></p>
<p>Como volvemos a observar, las transformadas de Fourier de funciones de peso cuadradas (`boxcar’) no son adecuadas debido a las oscilaciones o lóbulos menores.</p>
</section>
<section id="filtro-lanczos-pasabaja" class="level3">
<h3 class="anchored" data-anchor-id="filtro-lanczos-pasabaja">Filtro Lanczos pasabaja</h3>
<p>Sea <span class="math inline">\(R(f)\)</span> la función respuesta de un filtro pasa baja, en donde <span class="math inline">\(f\)</span> corresponde a la frecuencia en ciclos por unidad de tiempo, <span class="math inline">\(f_N\)</span> es la frecuencia de Nyquist, y <span class="math inline">\(fc\)</span> es la frecuencia de corte.</p>
<p>Lanzcos se dio cuenta que las oscilaciones de los filtros cosinusoidales con longitud de onda <span class="math inline">\(\lambda(f)=4f_N/2N+1\)</span> podían ser reducidas si se realiza un suavizado de la fucnión respuesta <span class="math inline">\(H(f)\)</span>. Para ello realizó un promedio corrido de tamaño igual a la longitud de onda de las oscilaciones, es decir, <span class="math inline">\(\lambda\)</span>. Esto se puede escribir como <span class="math display">\[\widetilde{R}(f)=\frac{1}{\lambda(f)}\int\limits^{f+\lambda/2}_{f-\lambda/2} R(f) d{f}\,,\]</span> donde ya hemos visto que <span class="math display">\[R(f)=\frac{f_c}{f_N} + 2\sum\limits^{N}_{p=1}
r_p cos(\pi p f / f_N)\,.\]</span></p>
<p>Un filtro de media corrida no tiene efecto en el promedio, entonces <span class="math display">\[\widetilde{R}(f)=\frac{f_c}{f_N} + \frac{1}{\lambda}
\int\limits^{f+\lambda/2}_{f-\lambda/2} 2 \sum\limits^{N}_{p=1}
r_pcos(\pi p f / f_N)d{f}=\]</span></p>
<p><span class="math display">\[=\frac{f_c}{f_N} + \frac{2}{\lambda}\sum\limits^{N}_{p=1} r_p
\left[\frac{1}{\pi p /f_N} sen\left( \frac{\pi p
f}{f_N}\right)\right]^{f+\lambda/2}_{f-\lambda/2}=\]</span></p>
<p><span class="math display">\[=\frac{f_c}{f_N}+ \frac{2}{\lambda}\sum\limits^{N}_{p=1}
r_p\frac{f_N}{\pi p}\left[sen\left( \frac{\pi p
(f+\lambda/2)}{f_N}\right)-sen\left( \frac{\pi p
(f-\lambda/2)}{f_N}\right) \right]=\]</span></p>
<p><span class="math display">\[=\frac{f_c}{f_N}+ \frac{2}{\lambda}\sum\limits^{N}_{p=1}
r_p\frac{f_N}{\pi p}\left[2cos\left( \frac{\pi p
f}{f_N}\right)sen\left( \frac{\lambda \pi p}{2f_N}\right) \right]=
\frac{f_c}{f_N}+ \frac{2}{\lambda}\sum\limits^{N}_{p=1}
r_p\frac{f_N}{\pi p}\left[2cos\left( \frac{\pi p
f}{f_N}\right)sen\left( \frac{2 \pi p}{2N+1}\right) \right]=\]</span></p>
<p><span class="math display">\[=\frac{f_c}{f_N}+ \frac{2(2N+1)}{4f_N}\sum\limits^{N}_{p=1}
r_p\frac{f_N}{\pi p}\left[2cos\left( \frac{\pi p
f}{f_N}\right)sen\left( \frac{2 \pi p}{2N+1}\right) \right]=\]</span></p>
<p><span class="math display">\[=\frac{f_c}{f_N}+ 2\sum\limits^{N}_{p=1}
r_p\frac{2N+1}{2 \pi p}\left[cos\left( \frac{\pi p
f}{f_N}\right)sen\left( \frac{2 \pi p}{2N+1}\right) \right]=\]</span></p>
<p><span class="math display">\[=\frac{f_c}{f_N}+ 2\sum\limits^{N}_{p=1}
r_p\frac{1}{\frac{2 \pi p}{2N+1}}\left[cos\left( \frac{\pi p
f}{f_N}\right)sen\left( \frac{2 \pi p}{2N+1}\right) \right]=
\frac{f_c}{f_N}+ 2\sum\limits^{N}_{p=1}
r_p \sigma_p cos\left( \frac{\pi p
f}{f_N}\right)\,,\]</span></p>
<p>donde</p>
<p><span class="math display">\[\sigma_p=\frac{sen\left( \frac{2 \pi p}{2N+1}\right)}{\frac{2 \pi p}{2N+1}}= sinc\left(\frac{2 \pi p}{2N+1} \right)\,,\]</span></p>
<p>es una función <em>sinc</em> como la respuesta espectral de un filtro rectangular o <em>boxcar</em>. A este factor de suavizado se le suele llamar peso sigma.</p>
<p>En la figura se observa que la frecuencia de corte es la la frecuencia que corta el 50% de la magnitud de la respuesta. La frecuencia efectiva es <span class="math inline">\(f=f_c+\lambda/2\)</span>.</p>
</section>
<section id="filtro-lanczos-pasabanda" class="level3">
<h3 class="anchored" data-anchor-id="filtro-lanczos-pasabanda">Filtro Lanczos pasabanda</h3>
<p>En el dominio de las frecuencias, el filtro de pasa-banda se obtiene convolucionando el filtro pasa-bajas con la transformada de Fourier de la función coseno: <span class="math display">\[\widetilde{R}_b(f)=\widetilde{R}(f)*\left[\delta(f-f_o) + \delta(f+f_0) \right]\,,\]</span> donde * significa convolución, <span class="math display">\[{\cal F}[cos(2\pi f_0 x)](f)=\int\limits^{\infty}_{-\infty} e^{-2\pi i f x} cos(2\pi
f_0 x) dx=\int\limits^{\infty}_{-\infty} e^{-2\pi i f x}
\left(\frac{e^{2\pi i f_0 x} +e^{-2\pi i f_0 x}}{2}\right) dx = \]</span> <span class="math display">\[=\frac{1}{2}\int\limits^{\infty}_{-\infty}\left[e^{-2\pi i (f-f_0) x} + e^{-2\pi i (f+f_0) x} \right] dx\]</span> <span class="math display">\[=\frac{1}{2}\left[ \delta(f-f_0) + \delta(f+f_0)\right]\,,\]</span> y la delta de dirac se define como <span class="math display">\[\delta(x)={\cal F}[1](f\pm f_0)=\int\limits^{\infty}_{-\infty} e^{-2\pi i (f\pm f_0) x} dx\]</span></p>
<p>Vemos que la transformada de Fourier del coseno se ha multiplicado por un factor de 2 para que la respuesta del filtro sea unitaria (normalización). El resultado de la convolución es</p>
<p><span class="math display">\[\widetilde{R}_b(f)=\widetilde{R}(f)*\left[\delta(f-f_o) + \delta(f+f_0) \right]=
\widetilde{R}(f-f_0)+\widetilde{R}(f+f_0)=\]</span></p>
<p><span class="math display">\[=\frac{f_c}{f_N}+ 2\sum\limits^{N}_{p=1}
{r_p} \sigma_p cos\left( \frac{\pi p (f-f_0)}{f_N}\right) +
\frac{f_c}{f_N}+ 2\sum\limits^{N}_{p=1}
{r_p} \sigma_p cos\left( \frac{\pi p (f+f_0)}{f_N}\right)=\]</span></p>
<p><span class="math display">\[=2\frac{f_c}{f_N}+ 2\sum\limits^{N}_{p=1}
{r_p} \sigma_p \left[ cos\left( \frac{\pi p f}{f_N} - \frac{\pi p f_0}{f_N}\right) +
cos\left( \frac{\pi p f}{f_N} + \frac{\pi p f_0}{f_N}\right)\right]=\]</span></p>
<p><span class="math display">\[=2\frac{f_c}{f_N}+ 2\sum\limits^{N}_{p=1} r_p \sigma_p\left[
2cos\left( \frac{\pi p f}{f_N}\right) cos\left( \frac{\pi p f_0}{f_N}\right)\right]=\]</span></p>
<p><span class="math display">\[=2\frac{f_c}{f_N}+ 4\sum\limits^{N}_{p=1} r_p \sigma_p\left[
cos\left( \frac{\pi p f}{f_N}\right) cos\left( \frac{\pi p f_0}{f_N}\right)\right]\]</span></p>
<p>Para el filtro pasa bajas: <span class="math display">\[x(t)[{ filtrado}]=[r_p \sigma_p * x](t)\,,\]</span> o <span class="math display">\[x(t)[{ filtrado}]={\cal F}^{-1}[\widetilde{R}(f)X(f)]\,.\]</span></p>
<p>Para el filtro pasa-banda: <span class="math display">\[x(t)[{ filtrado}]=[r_p \sigma_p cos\left( \frac{\pi p f_0}{f_N}\right)*x](t)\,,\]</span> o <span class="math display">\[x(t)[{ filtrado}]={\cal F}^{-1}[\widetilde{R}_b(f)X(f)]\,.\]</span></p>
<p>Todo esto se puede programar facilmente en <em>Matlab</em> como se muestra en el siguiente:</p>
<p>% OUTPUT\</p>
<p>Y una vez tenemos los coeficientes solamente tenemos que convolucionar o multiplicar:</p>
<p>Aqui se muestra un ejemplo sintético del uso de las subrutinas presentadas.</p>
</section>
</section>
<section id="temas-selectos" class="level2">
<h2 class="anchored" data-anchor-id="temas-selectos">Temas selectos</h2>
<section id="análisis-armónico" class="level3">
<h3 class="anchored" data-anchor-id="análisis-armónico">Análisis Armónico</h3>
<p>Se trata de un ajuste por mínimos cuadrados de una serie temporal dominada por armónicos específicos. Por ejemplo, en el caso del océano, es muy comúnencontrar en series temporales de temperatura, salinidad, velocidad, etc.. señales de las mareas que no son nada mas que corrientes periódicas generadas por fuerzas astronómicas con una frecuencia de oscilación determinada (24h, 12h, etc…).</p>
<p>El método consiste en elejir las frecuencias de los armónicos y usar cuadrados mínimos para ajustarlos a la serie temporal. Supongamos <span class="math inline">\(M\)</span> armónicos a ajustar</p>
<p><span class="math display">\[y(t_n) = \overline{y(t)} + \sum\limits^{M}_{q=1}C_q cos(\omega_q t_n -\theta_q)+y_r(t_n)\,,\]</span></p>
<p>donde <span class="math inline">\(\overline{y(t)}\)</span> es el promedio de la serie, y <span class="math inline">\(y_r(t_n)\)</span> es el residuo de la serie temporal (donde hay el resto de armónicos presentes en la serie), <span class="math inline">\(\omega_q=2\pi q/N\Delta t\)</span>. En términos de las amplitudes <span class="math inline">\(A_q\)</span> y <span class="math inline">\(B_q\)</span></p>
<p><span class="math display">\[y(t_n) = \overline{f(t)} + \sum\limits^{M}_{q=1}[A_q cos(\omega_q t_n)+B_q sen(\omega_q t_n)] +y_r(t_n)\,,\]</span> donde <span class="math display">\[C_q=\sqrt{A_q^2+B_q^2}\,,\,\,\,\,q=1,2,....\]</span> <span class="math display">\[\theta_q=arctg[B_q/A_q]\,,\,\,\,\,q=1,2,...\]</span></p>
<p>Antes de empezar el análisis debemos de extraer la media, <span class="math inline">\(\overline{y}\)</span>, a la serie temporal. El método de mínimos cuadrados consiste en minimizar la suma de los errores cuadrados <span class="math inline">\(SEC\)</span>, es decir, <span class="math display">\[SEC= \sum\limits^N_{n=1} y_r^2(t_n) = \sum\limits^N_{n=1}  \left( y(t_n) - \left[ \overline{y(t)} +
\sum\limits^{M}_{q=1} A_q cos(\omega_q t_n)+B_q sen(\omega_q t_n) \right] \right)^2 =\]</span> <span class="math display">\[=\sum\limits^N_{n=1}  \left(y(t_n) - \left[ \overline{y(t)} +
\sum\limits^{M}_{q=1} A_q cos(2\pi q n/ N)+B_q sen(2\pi q n/ N) \right] \right)^2\]</span></p>
<p>Como siempre derivamos respecto los coeficientes e igualamos a cero para obtener un sistema de <span class="math inline">\(2M+1\)</span> equaciones <span class="math display">\[\frac{\partial{SEC}}{\partial{A_q}}=0=2\sum\limits^N_{n=1} \left(y(t_n) - \left[ \overline{y(t)} +
\sum\limits^{M}_{q=1} A_q cos(2\pi q n/ N)+B_q sen(2\pi q n/ N) \right]-cos(2\pi q n/ N) \right)\]</span> <span class="math display">\[\frac{\partial{SEC}}{\partial{B_q}}=0=2\sum\limits^N_{n=1} \left(y(t_n) - \left[ \overline{y(t)} +
\sum\limits^{M}_{q=1} A_q cos(2\pi q n/ N)+B_q sen(2\pi q n/ N) \right]-sen(2\pi q n/ N) \right)\]</span></p>
<p>Soluciones del sistema requiere una equación matricial de la forma <span class="math inline">\({\textbf D}{\textbf z}={\textbf y}\)</span>, donde <span class="math display">\[{\textbf D}=\left( \begin{array}{cccccccccccccc}
  N &amp; c_1 &amp; c_2 &amp; ... &amp; c_M &amp; s_1 &amp; s_2 &amp; ... &amp; s_M \\
  c_1 &amp; cc_{11} &amp; cc_{12} &amp; ... &amp; cc_{1M} &amp; cs_{11} &amp; cs_{12} &amp; ... &amp; cs_{1M} \\
  c_2 &amp; cc_{21} &amp; cc_{22} &amp; ... &amp; cc_{2M} &amp; cs_{21} &amp; cs_{22} &amp; ... &amp; cs_{2M} \\
  ... &amp; ... &amp; ... &amp; ... &amp; ... &amp; ... &amp; ... &amp; ... &amp; ... \\
  ... &amp; ... &amp; ... &amp; ... &amp; ... &amp; ... &amp; ... &amp; ... &amp; ... \\
  c_M &amp; cc_{M1} &amp; cc_{M2} &amp; ... &amp; cc_{MM} &amp; cs_{M1} &amp; cs_{M2} &amp; ... &amp; cs_{MM} \\
  s_1 &amp; sc_{11} &amp; sc_{12} &amp; ... &amp; sc_{1M} &amp; ss_{11} &amp; ss_{12} &amp; ... &amp; ss_{1M} \\
  s_2 &amp; sc_{21} &amp; sc_{22} &amp; ... &amp; sc_{2M} &amp; ss_{21} &amp; ss_{22} &amp; ... &amp; ss_{2M} \\
  ... &amp; ... &amp; ... &amp; ... &amp; ... &amp; ... &amp; ... &amp; ... &amp; ... \\
  ... &amp; ... &amp; ... &amp; ... &amp; ... &amp; ... &amp; ... &amp; ... &amp; ... \\
  s_M &amp; sc_{M1} &amp; sc_{M2} &amp; ... &amp; sc_{MM} &amp; ss_{M1} &amp; ss_{M2} &amp; ... &amp; ss_{MM} \\
      \end{array} \right)\]</span> y</p>
<p><span class="math display">\[{\textbf y}=\left( \begin{array}{ccc}
yc_0 \\
yc_1 \\
yc_2 \\
... \\
... \\
yc_M \\
ys_1 \\
ys_2 \\
... \\
... \\
ys_M \\
\end{array} \right)\,\,\,\,\,\,\,\,\,\,
{\textbf z}=\left( \begin{array}{ccc}
A_0\\
A_1\\
A_2\\
...\\
...\\
A_M\\
B_1\\
B_2\\
...\\
...\\
B_M
\end{array} \right)\]</span></p>
<p>Los coeficientes de las matrices son:</p>
<p><span class="math display">\[yc_i=\sum\limits^N_{n=1}y(t_n)cos(\omega_i t_n)\,\,\,\,\,,\,\,\,\,\,ys_i=\sum\limits^N_{n=1}y(t_n)sen(\omega_i t_n)\]</span> <span class="math display">\[c_i=\sum\limits^N_{n=1}cos(\omega_i t_n)\,\,\,\,\,,\,\,\,\,\,s_i=\sum\limits^N_{n=1}sen(\omega_i t_n)\]</span> <span class="math display">\[cc_{ij}=cc_{ji}=\sum\limits^N_{n=1}[cos(\omega_i t_n)cos(\omega_j t_n)]\]</span> <span class="math display">\[ss_{ij}=ss_{ji}=\sum\limits^N_{n=1}[sen(\omega_i t_n)sen(\omega_j t_n)]\]</span> <span class="math display">\[cs_{ij}=sc_{ji}=\sum\limits^N_{n=1}[cos(\omega_i t_n)sen(\omega_j t_n)]\,,\]</span> donde <span class="math inline">\(t_n= n \Delta\)</span>, <span class="math inline">\(\omega_i = 2\pi f_i\)</span> es la frecuencia angular de las componentes de interés <span class="math inline">\(i\)</span>, y <span class="math inline">\(\phi_i(n)=\omega_i t_n\)</span> es el argumento de las funciones de Fourier.</p>
<p>{Ejemplo de Ajuste de armónicos (Emery and Thompson, p395)}</p>
<p>Asumamos la siguiente serie temporal de temperatura promedia mensual. \</p>
<p>\ \</p>
<p>Deseamos encontrar las componentes mareales dominantes en la serie temporal de temperatura. A simple vista podemos ver que existe una frecuencia dominante semianual. Por tanto, vamos buscar las amplitudes y frecuencias de interés, es decir, de las componentes anual y semianual que tienen unas frecuencias de <span class="math inline">\(f_1=1/12\)</span> meses (<span class="math inline">\(=0.0833\,{ cpm}\)</span>) y <span class="math inline">\(f_2=1/24\)</span> meses (<span class="math inline">\(=0.1667\,{ cpm}\)</span>). Los argumentos de las funciones de Fourier son <span class="math inline">\(\phi_1(n)=\omega_1 t_n=2\pi(1/12)*n*\Delta t=(\pi/6)*n*1=n\pi/6\)</span> y <span class="math inline">\(\phi_2(n)=\omega_2 t_n=2\pi(1/6)*n*\Delta t=(\pi/3)*n*1=n\pi/3\)</span> Para este problema las matrices son \</p>
<p><span class="math display">\[{\textbf D}=\left( \begin{array}{ccccc}
  N &amp; c_1 &amp; c_2 &amp; s_1 &amp; s_2 \\
  c_1 &amp; cc_{11} &amp; cc_{12} &amp; cs_{11} &amp; cs_{12}\\
  c_2 &amp; cc_{21} &amp; cc_{22} &amp; cs_{21} &amp; cs_{22} \\
  s_1 &amp; sc_{11} &amp; sc_{12} &amp; ss_{11} &amp; ss_{12} \\
  s_2 &amp; sc_{21} &amp; sc_{22} &amp; ss_{21} &amp; ss_{22} \\
      \end{array} \right)=\]</span> <span class="math display">\[=\scriptsize
\left( \begin{array}{ccccc}
N &amp; c_1 &amp; c_2 &amp; s_1 &amp; s_2 \\
  c_1 &amp; \sum\limits^N_{n=1}[cos(\phi_1(n))cos(\phi_1(n))] &amp; cc_{12} &amp; cs_{11} &amp; cs_{12}\\
  c_2 &amp; cc_{21} &amp; \sum\limits^N_{n=1}[cos(\phi_1(n))cos(\phi_2(n))] &amp; cs_{21} &amp; cs_{22} \\
  s_1 &amp; sc_{11} &amp; sc_{12} &amp; \sum\limits^N_{n=1}[sen(\phi_1(n))sen(\phi_1(n))] &amp; ss_{12} \\
  s_2 &amp; sc_{21} &amp; sc_{22} &amp; ss_{21} &amp; \sum\limits^N_{n=1}[sen(\phi_2(n))sen(\phi_2(n))] \\
      \end{array} \right) =
      \]</span> <span class="math display">\[
=
\left( \begin{array}{ccccc}
  24 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
  0 &amp; 12 &amp; 0 &amp; 0 &amp; 0\\
  0 &amp; 0 &amp; 12 &amp; 0 &amp; 0 \\
  0 &amp; 0 &amp; 0 &amp; 12 &amp; 0 \\
  0 &amp; 0 &amp; 0 &amp; 0 &amp; 12 \\
      \end{array} \right)
\]</span></p>
<p>La matriz <span class="math display">\[{\textbf y}=\left( \begin{array}{ccc}
\sum\limits^N_{n=1}y(t_n)cos(\omega_0 t_n) \\
\sum\limits^N_{n=1}y(t_n)cos(\omega_1 t_n) \\
\sum\limits^N_{n=1}y(t_n)cos(\omega_2 t_n) \\
\sum\limits^N_{n=1}y(t_n)sen(\omega_1 t_n) \\
\sum\limits^N_{n=1}y(t_n)sen(\omega_2 t_n) \\
\end{array} \right)=
\left( \begin{array}{ccc}
262.5 \\
-21.45 \\
-5.4 \\
-23.76 \\
-0.51 \\
\end{array} \right)\,\,^\circ{C}\]</span></p>
<p>Finalmente encontramos las amplitudes de los armónicos resolviendo el sistema <span class="math display">\[{\textbf z}=({\textbf D}^T{\textbf D})^{-1}{\textbf D}^{T}{\textbf y}={\textbf D}^{-1}{\textbf y}=\left( \begin{array}{ccc}
10.93\\
-1.78\\
-0.45\\
-1.98\\
-0.04
\end{array} \right)\]</span></p>
<p>El coeficiente de correlación entre la señal original y la serie de Fourier con 2 armónicos es <span class="math inline">\(r^2=0.92\)</span>, es decir, solamente con 2 armónicos podemos explicar el 92% de la varianza total.</p>
</section>
<section id="demodulación-compleja" class="level3">
<h3 class="anchored" data-anchor-id="demodulación-compleja">Demodulación compleja</h3>
<p>Este método es utilizado para conocer el comportamiento de una componente o armónico con frecuencia particular <span class="math inline">\(\omega\)</span>, tal como la marea diurna, o semidiurna, o las ondas inerciales. Aqui vamos a mostrar la forma clásica de demodular que consiste en ajustar por fragmentos de la serie un armónico teórico utilizando mínimos cuadrados. Cada fragmento de la serie debe, como mínimo, contener un ciclo del armónico a demodular. Para cada segmento, la anomalía de la componente de velocidad a la frecuencia de interés <span class="math inline">\(\omega\)</span> es <span class="math display">\[{\textbf u} - \overline{\textbf u}=[u(t)-\overline{u(t)} +iv(t)-\overline{v(t)}]=\]</span> <span class="math display">\[R^+ e^{i(\omega t + \epsilon^+)} + R^- e^{-i(\omega t + \epsilon^-)}\,,\]</span> donde <span class="math inline">\(\overline{u(t)}\)</span>, <span class="math inline">\(\overline{v(t)}\)</span> son las componentes de la velocidad promedio, <span class="math inline">\(R^+,\,\,R^-\)</span> y <span class="math inline">\(\epsilon^+\,\,\epsilon^-\)</span> son las amplitudes y fases de las componentes rotatorias que giran en el sentido de las agujas del reloj (+) y en el sentido contrario (-). La serie temporal esta definida para cada <span class="math inline">\(t_k\,(k=1,2,....,N)\)</span> y las soluciones son encontradas resolviendo el sistema de ecuaciones <span class="math display">\[{\textbf z}={\textbf D}^{-1}{\textbf y}\,,\]</span> donde <span class="math display">\[{\textbf y}=\left( \begin{array}{c}
u(t_1) \\
u(t_2) \\
... \\
u(t_n) \\
v(t_1) \\
v(t_2) \\
... \\
v(t_n) \\
\end{array} \right)\,;\,\,\,
{\textbf z}=\left( \begin{array}{c}
R^+cos(\epsilon^+) \\
R^+sen(\epsilon^+)\\
R^-cos(\epsilon^-) \\
R^-sen(\epsilon^-)\\
\end{array} \right)=
\left( \begin{array}{c}
ACP \\
ASP\\
ACM \\
ASM\\
\end{array} \right)\,,\]</span> y la matriz <span class="math inline">\({\textbf D}\)</span> es <span class="math display">\[{\textbf D}=\left( \begin{array}{cccc}
cos(\omega t_1) &amp; -sen(\omega t_1) &amp; cos(\omega t_1) &amp; sen(\omega t_1) \\
cos(\omega t_2) &amp; -sen(\omega t_2) &amp; cos(\omega t_2) &amp; sen(\omega t_2) \\
... \\
cos(\omega t_n) &amp; -sen(\omega t_n) &amp; cos(\omega t_n) &amp; sen(\omega t_n) \\
sen(\omega t_1) &amp; cos(\omega t_1) &amp; -sen(\omega t_1) &amp; cos(\omega t_1) \\
sen(\omega t_2) &amp; cos(\omega t_2) &amp; -sen(\omega t_2) &amp; cos(\omega t_2) \\
... \\
sen(\omega t_n) &amp; cos(\omega t_n) &amp; -sen(\omega t_n) &amp; cos(\omega t_n) \\
\end{array} \right)\,.\]</span></p>
<p>Una vez los valores de <span class="math inline">\({\textbf z}\)</span> son encontrados a partir de la solución de mínimos cuadrados de arriba, podemos encontrar los parámetros de la elipse como: <span class="math display">\[R^+=\sqrt{\left( ASP^2 + ACP^2\right)}\,;\,\,\,\,\,R^-=\sqrt{\left( ASM^2 + ACM^2\right)}\]</span> <span class="math display">\[\epsilon^+=tan^{-1}\left(\frac{ASP}{ACP}\right)\,;\,\,\,\,\,\epsilon^-=tan^{-1}\left(\frac{ASM}{ACM}\right)\]</span></p>
<p>Por ejemplo, si queremos demodular la amplitud y fase de las ondas inerciales observadas en un anclaje situado en latitudes medias, debemos de usar una frecuencia <span class="math inline">\(\omega=2\Omega sen\phi\)</span> y ajustar por mínimos cuadrados segmentos de <span class="math inline">\(24\,{ h}\)</span> sin superposición. La serie temporal medida por el anclaje debería de ser horaria para que existan mas datos por segmento que parámetros a ajustar.</p>
<p>Otra forma, tal vez mas sencilla, es la siguiente. Imaginemos que la serie original es <span class="math inline">\(X(t)\)</span> y se asume como una señal periódica con frecuencia igual a la de interés mas otras cosas que llamamos <span class="math inline">\(Z(t)\)</span> <span class="math display">\[X(t)=A(t)cos\left(\omega t + \varphi(t)\right) + Z(t)= \frac{1}{2}A(t)
       \left[e^{i(\omega t + \varphi(t))} + e^{-i(\omega t + \varphi(t))} \right] + Z(t)\,,\]</span> donde la amplitud <span class="math inline">\(A(t)\)</span> y la fase <span class="math inline">\(\varphi(t)\)</span> de la señal periódica se asumen que dependen del tiempo pero que varían “lentamente” en comparación a la frecuencia <span class="math inline">\(\omega\)</span>.</p>
<p>Para demodular tenemos que: \</p>
<ol type="1">
<li>Multiplicar <span class="math inline">\(X(t)\)</span> por <span class="math inline">\(e^{-i\omega t}\)</span>: <span class="math display">\[Y(t)=X(t)e^{-i\omega t}=\frac{1}{2}A(t)
   \left[e^{i(\omega t + \varphi(t))} + e^{-i(\omega t + \varphi(t))} \right]e^{-i\omega t} + Z(t)e^{-i\omega t}=\]</span> <span class="math display">\[=\frac{1}{2}A(t)e^{i(\omega t + \varphi(t))}e^{-i\omega t} + \frac{1}{2}A(t)e^{-i(\omega t + \varphi(t))}e^{-i\omega t} +
Z(t)e^{-i\omega t}=\]</span> <span class="math display">\[=\underbrace{\frac{1}{2}A(t)e^{i\varphi(t)}}_{(a)} + \underbrace{\frac{1}{2}A(t)e^{-i(2\omega t + \varphi(t))}}_{(b)} +
\underbrace{Z(t)e^{-i\omega t}}_{(c)}\,.\]</span></li>
</ol>
<p>El término (a) varía lentamente ya que <span class="math inline">\(\varphi(t)\)</span> también lo hace y no tiene energía (potencia espectral) a la frecuencia de demodulación <span class="math inline">\(\omega\)</span> o arriba de ella. El término (b) oscila a dos veces la frecuencia de demodulación, i.e., <span class="math inline">\(2\omega\)</span>. El término (c) varía a la frecuencia <span class="math inline">\(\omega\)</span>. Debido a que <span class="math inline">\(Z(t)\)</span> no tiene energía a la frecuencia <span class="math inline">\(\omega\)</span>, entonces el término (c) no tendrá tampoco energía en la frecuencia cero, i.e., <span class="math inline">\(\omega=0\)</span>.\</p>
<ol start="2" type="1">
<li><p>Filtro pasa-bajas de la serie <span class="math inline">\(Y(t)\)</span> para eliminar las ondas con frecuencia <span class="math inline">\(\omega\)</span> o por encima de <span class="math inline">\(\omega\)</span>. Esto eliminará prácticamente los términos (b) y (c), y suavizará (a). El resultado es <span class="math display">\[Y_s(t)=\frac{1}{2}A_s(t)e^{i\varphi_s(t)}\,,\]</span> donde el subíndice <span class="math inline">\(_s\)</span> significa suavizado o filtro pasa-bajas. \</p></li>
<li><p>Extraer <span class="math inline">\(A_s(t)\)</span> y <span class="math inline">\(\varphi_s(t)\)</span>: <span class="math display">\[\frac{1}{2}A_s(t)=|Y_s(t)|=2\left( Re[Y_s]^2 + Im[Y_s]^2\right)^{1/2}\]</span> <span class="math display">\[e^{i\varphi(t)}=2\frac{Y_s(t)}{A_s(t)}\,;\,\,\,\,\,\varphi_s(t)=tan^{-1}\left(\frac{Im[Y_s]}{Re[Y_s]}\right)\]</span></p></li>
</ol>
<p>Al suavizar hacemos dos cosas. Primero, eliminamos los términos no deseados (a) y (b). El tipo de filtrado o suavizado determina la anchura de la banda de frecuencias de las oscilaciones retenidas. Por ejemplo, si usamos un triángulo (ventana triangular) de longitud <span class="math inline">\(2T-1\)</span> donde <span class="math inline">\(T=2\pi/\omega\)</span> es el periodo de demodulación, entonces para la banda para la potencia-media (<span class="math inline">\(3\,{ dB}\)</span> desde el pico) será <span class="math inline">\(\omega \in [T/(1+0.44295),T/(1-0.44295)]\)</span>. Potencia media se refiere a la frecuencia a la cual la potencia se ha reducido a la mitad de su valor medio de la banda. Segundo, el filtrado suaviza las series de amplitud y la fase. \</p>
<ol start="4" type="1">
<li>La elección de la frecuencia de demodulación <span class="math inline">\(\omega\)</span> se puede validar ajustando localmente una línea a la fase,<span class="math inline">\(\varphi\simeq a + bt\)</span>. Típicamente esto lo haremos en fragmentos de longiutd <span class="math inline">\(T\)</span>. De esta forma si seleccionamos el origen en el tiempo central de cada fragmento (tal que <span class="math inline">\(a\simeq 0\)</span>) obtenemos que <span class="math inline">\(cos(\omega t + \varphi)\simeq cos(\omega t + bt)=cos(\hat{\omega} t)\)</span>.</li>
</ol>
<p>La frecuencia ajustada <span class="math inline">\(\hat{\omega}=\omega + a\)</span> es una validación de la elección inicial de nuestra frecuencia de demodulación <span class="math inline">\(\omega\)</span>.</p>
</section>
</section>
<section id="acknowledgments" class="level2">
<h2 class="anchored" data-anchor-id="acknowledgments">Acknowledgments</h2>
<p>Ayuda de Muchos</p>
</section>
<section id="open-research" class="level2">
<h2 class="anchored" data-anchor-id="open-research">Open research</h2>
<p>Disponible para todos</p>
</section>
<section id="references" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="references">References</h2>
<div id="refs" role="list">

</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>