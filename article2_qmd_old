---
title: "Notas Curso Análisis de datos"
format:
  pdf:
#    agu-pdf:
    fancy-doc-pdf: default
#        keep-tex: true
#    agu-html: default
author:
  - name:  Enric Pallas
    affiliations:
      - name:   Centro de Investigación Científica y de Educación Superior de Ensenada, CICESE 
        department: Physical Oceanography,
        address: Carretera Ensenada-Tijuana 3018
        city: Ensenada
        region: Baja California
        country: MEXICO
        postal-code: 22860
    orcid: 0000-0001-000-000
    email: epallas@cicese.mx
    url: https://cicese.edu.mx/~epallas
  - name: Julio Sheinbaum
    affiliations:
      - name: Centro de Investigación Científica y de Educación Superior de Ensenada, CICESE 
        department: Physical Oceanography,
        address: Carretera Ensenada-Tijuana 3018
        city: Ensenada
        region: Baja California
        country: MEXICO
        postal-code: 22860
    orcid: 0000-0001-7031-5225
    email: julios@cicese.mx
    url: https://jsheinbaum.github.io
    acknowledgements: Translated template to Quarto.

abstract: |
  En el océano conviven una gran cantidad de corrientes de
  diferentes escalas espaciales y temporales.
  Las escalas espaciales típicas de la circulación oceánica
  son la larga escala, la mesoescala, submesoescala, y microescala.
  La larga escala es del ${\cal O}(1000\,{km})$ y esta
  determinada por la circulación general en el océano
  como la termohalina y los grandes giros anticiclónicos
  de las grandes cuencas oceánicas; las escalas temporales
  de la larga escala varia entre meses y años. La mesoescala
  esta definida por corrientes del ${\cal O}(100\,{km})$
  como remolinos, corrientes costeras, filamentos, frentes,
  etc. Son corrientes mas regionales pero pueden tener
  gran influencia sobre la circulación general o de larga
  escala. Sus escalas temporales son de semanas a meses.
  La submesoescala corresponde a corrientes del
  ${\cal O}(10\,{km})$ de caracter local remolinos,
  filamentos, frentes, corrientes en playas, puertos, y
  estuarios. La submesoscala varía temporalmente con rapidez en
  tiempos que varían de horas a días. Finalmente podemos hablar
  de la microescala que son remolinos del orden de centímetros a
  metros y generalmente es la escala característica de la turbulencia
  que transfiere energía desde la submesoescala hacia la
  disipación molecular. Aquí podemos hablar de fenómenos del
  orden de segundos y minutos. 

keywords: [] 
key-points:
  - Probar interactividad y teoría 
  - Jupyter,Julia,Pluto.
  - Vamos viendo.
bibliography: bibliography.bib  
citation:
  container-title: Geophysical Research Letters
keep-tex: true
date: last-modified
---


\includepdf[pages={-}]{/Users/julios/curso_analisis_de_datos/Instrumentacion_2023.pdf}

## Estadística y conceptos de probabilidad

### Porqué estudiar la estadística en oceanografía
A pesar de nuestra formación determinista a la hora de resolver
problemas matemáticos y aunque asumamos que las ecuaciones de
Navier-Stokes que describen el movimiento del océano son
determinísticas, la estadística es ampliamente utilizada
en oceanografía debido a diferentes razones:

\begin{itemize}
\item {Para una descripción completa del océano es necesario especificar
una gran cantidad de variables, muchas de las cuales son desconocidas. 
Un ejemplo de ello son las parametrizaciones que se hacen en oceanografía para describir variables que no pueden medirse directamente. Una parametrización no es nadammas que un modelo estadístico que explica la evolución de una variable dependiente de otras variables independientes. Por ejemplo, parametrización del esfuerzo del viento en función del corte vertical o parametrización del coeficiente de arrastre en función de la velocidad del viento a 10m de la superfície del océano.}

\item {El océano es altamente no lineal. La evolución de
una cierta variable no se puede estudiar de forma aislada.}

\vspace{0.5cm}

Ejemplo:
\\
Supongamos el término de aceleración horizontal en las ecuaciones de Navier Stokes
para fluidos incompresibles,

\begin{equation}
\frac{\partial{{\textbf u}_h}}{\partial{t}} + 
{\textbf u}_h \cdot {\nabla}_h \textbf {u}_h
\end{equation}

Como ya sabemos por el curso de Mecánica de Fluidos, la aceleración de
un fluido es una derivada material y consta de un término local (aceleración local) y
de un término advectivo o aceleración advectiva.
En general, esta ecuación no se aplica a partículas de agua individuales. En
oceanografía hablamos de continuo. No estamos interesados en las características
cinemáticas de las partículas individuales sino en la manifestación promedia
del movimiento molecular, es decir, del fluido como un conjunto o
contínuo. Es decir, asumimos que el fluido es uniforme en el espacio que
ocupa sin considerar la estructura molecular.
Por ello debemos de promediar de alguna forma para explicar el comportamiento
conjunto del fluído y no de una partícula de agua específica ?`Y como se realiza
tal promedio? En general, el promediado se realiza de tal forma que nos permite
separar la larga escala que trataremos como determinística, de la pequeña escala
que consideramos un proceso aleatorio (turbulento). Supongamos entonces la separación de la velocidad horizontal en una velocidad promedio y una velocidad fluctuante alrededor de la media

\begin{equation}
{\textbf u_h} = <{\textbf u_h}> + {\textbf u'_h}
\end{equation}

 donde $ < > $ denotan promedio. Si aplicamos esta descomposición a la componente $x$ de la aceleración obtenemos:

\begin{equation}
\frac{\partial <{\textbf u}>}{\partial t} +  <{\textbf u}> \cdot  { \nabla} <{\textbf u} >  + < {\textbf u'} \cdot {\nabla} {\textbf u}'> 
\end{equation}
 %\dfrac{\partial \textbf(<u>}}{\partial t} + < {\textbf u} > \cdot {\textbf \nabla}{ \textbf <u> } + < {\textbf u'}\cdot {\textbf \nabla} {\textbf u}' >

%\dfrac{\partial \textbf(<u>}}{\partial t} + < {\textbf u} > \cdot {\textbf \nabla}{ \textbf <u> } + < {\textbf u'}\cdot {\textbf \nabla} {\textbf u}' >

Inevitablemente, las pequeñas escalas o fluctuaciones respecto a la larga escala
aparecen en la expresión de la aceleración de larga escala. De forma que la
separación que deseamos no es tan simple ya que debemos de conocer la
estadística de la pequeña escala para poder describir la circulación
promedia. El término $<{\textbf u}'_h u'>$ se denomina esfuerzo
de Reynolds y nos informa de la correlación entre las componentes fluctuantes
(alta frecuencia) de la velocidad. Por ejemplo, $<u'v'>=0$ significa que no existe
correlación y hablamos de isotropía. Si $<u'v'><0$, significa que las
fluctuaciones están inversamente correlacionadas, i.e., anisotropía.
Este es un gran problema no resuelto en la oceanografía física.
El esfuerzo de Reynolds aparece porque la advección es no-lineal de tal forma
que no podemos estudiar la larga escala sin conocer información de la pequeña
escala que es un proceso aleatorio. Por similitud con el flujo laminar, los términos
de esfuerzo de Reynolds se parametrizan estadísticamente como proporcionales a
los gradientes de velocidad. El factor de proporcionalidad es el coeficiente de
viscosidad, en este caso, turbulento. Es aqui donde utilizar herramientas
estadísticas tiene sentido.

\item {No podemos controlar las variables oceanográficas; estan en constante cambio
a medida que el sistema observado evoluciona.}

\vspace{0.5cm}
{\noindent Ejemplo:}\\
En el océano cohexisten mareas, ondas internas, remolinos, turbulencia de pequeña
escala,...las cuales enmascaran el fenómeno oceanográfico que estamos interesados en
estudiar. Estos procesos incontrolables por el oceanógrafo en ocasiones es útil considerarlos
aleatorios y utilizar herramientas estadísticas para caracterizarlos. 

Imaginemos que queremos conocer cual es la temperatura superficial promedio
en la bahía de Todos Santos. Una forma de proceder sería promediar todos los
datos de temperatura superficial que disponemos de los últimos 100 años y promediarlos
?`Pero, es realmente lo que deseamos? ?`Deberíamos de considerar la estaciones del año
y obtener un pormedio para cada estación? ?`Qué sucede en años del Niño, el cual
sabemos que afecta la temperatura del océano? En definitiva, {debemos de
definir sobre que conjunto de datos vamos a promediar, y ese promedio
va a reflejar efectivamente esa elección.}
\\

Este ejemplo precisa de la distinción entre lo que consideramos nuestra {\it señal}
(temperatura media) de los procesos que son {\it ruido} (Estaciones del año, los años Niño,
ondas internas, etc.). De esta forma, al definir el promedio estamos
haciendo explícita la separación entre {\it señal} y {\it ruido}. Finalmente, una vez
definido sobre que promediar, existen en literatura una gran cantidad de herramientas
estadísticas que podemos utilizar. Definir {\it señal} y {\it ruido}, y determinar
sobre que conjunto de datos vamos a calcular el promedio, es una tarea difícil.
Conocer como debemos muestrear el océano también debe hacerse
cuidadosamente.
\\
\end{itemize}

En oceanografía física se muestrea el océano de forma discontínua,
es decir, se obtienen medidas puntuales en el espacio y en el tiempo.
Como dijimos anteriormente, el océano contiene procesos de diferentes
escalas espaciales y temporales, nolineales, y aleatorios. Es por ello que es sumamente
importante saber escojer el intervalo de muestreo $\Delta{t}$ dependiendo del
fenómeno que se quiere muestrear. Debemos de tener en mente que
la frecuencia mas alta que podemos resolver es la frecuencia de Nyquist

$$f_N=1/(2\Delta{t})\,.$$

Por ejemplo, si medimos a
intervalos de $\Delta{t}=5\,{\rm h}$ podremos como máximo resolver
procesos que ocurren con frecuencia $f_N\le1/10\,{\rm cph}$. La frecuencia
mas baja que podemos resolver va a depender de la longitud del registro. A
esa frecuencia le llamamos frecuencia fundamental

$$f_0=1/(\Delta{t}N)\,,$$

{\noindent}donde $T=\Delta{t} N$ es la duración del muestreo y N es el número
de muestras o datos. En general, debemos de medir suficiente tiempo para registrar varios
ciclos del fenómeno de estudio para tener significancia estadística.
Por lo tanto, nuestra resolución frecuencial va a depender del intervalo
y duración del muestreo. El cociente
$f_N/f_0=(1/2\Delta{t})/(1/N\Delta{t})=N/2$ indica el número
máximo de componentes de Fourier que podemos estimar. Una señal
periódica se puede descomponer en la suma de un conjunto
(infinito) de funciones oscilatorias de senos y cosenos o
componentes de Fourier. Esto lo veremos en el capítulo~7.
A cada muestreo de un fenómeno le denominamos realización,
y a un conjunto de realizaciones se les denomina ensamble.

### Estadística básica

\label{Estadistica}

La estadística trata de describir las características de una población
continua a partir de muestras discretas de la misma. Hablamos de población
y de muestra de una población. Si calculamos, por ejemplo,
la media de una población, estamos calculando un {parámetro}.
Cuando calculamos la media de una muestra le llamamos un estadístico
de la población.
\\

La estadística nos ayuda a organizar, analizar, presentar datos, 
y nos da información de cómo planear la recolección de los mismos,
i.e. a muestrear.

\begin{center}
%\includegraphics[width=0.5\textwidth]{estadistica_poblacion.pdf}
\end{center}

\vspace{0.5cm}
\noindent (1) La media:
\\
La media de una muestra de N valores $x_i=x_1,x_2,...,x_N$ es

\begin{equation}
\bar{x}=\frac{1}{N}\sum^N_{i=1}x_i=<x>\,.
\end{equation}

La media debe de diferenciarse de la mediana. La media es el momento de orden cero.
La mediana de una población es aquel valor numérico que separa el 50\% de valores mas altos
del 50\% de valores mas bajos. Se puede calcular ordenando de menor a mayor
el conjunto de valores y escojer el valor central si el conjunto de datos
es impar o el promedio de los dos centrales si es par.
\\
\\
 { (2) La varianza:}\\
La varianza de un una muestra de N valores $x_i$ es

\begin{equation}
s^2=\frac{1}{N-1}\sum^N_{i=1}(x_i-\bar{x})^2=<x'^2>\,,
\end{equation}

donde las primas indican fluctuaciones alrededor de la media.
La varianza es una medida de cuán lejos estan los diferentes puntos de la
muestra de la media de la población. La varianza es el segundo momento
alrededor de la media. Al dividir por $N$ estamos subestimando la verdadera
varianza de la población. Al dividir por $N-1$ obtenemos un estimador insesgado.
\\
{\it NOTA: el sesgo de un estimador se refiere a la diferencia entre
su esperanza matemática y el valor numérico (real) del parámetro que se
estima. Un estimador que no tiene sesgo se dice insesgado. Por ejemplo,
para la media:}

$$E[x]-\mu \rightarrow {0}$$
$$\bar{x}-\mu \rightarrow {0}$$

{EJERCICIO:} Demostrar porqué hay que dividir por $N-1$ en lugar de $N$ para que
la definición de varianza sea un estimador insesgado.
\\

{\noindent (3) La desviación típica:}\\
Es la raíz cuadrada de la varianza. Se suele escribir como $\sigma$ para referirse a la
población o como $s$ en estadística

\begin{equation}
s=\sqrt{s^2}\,.
\end{equation}


\vspace{0.5cm}
 {\noindent (4) Momentos de orden superior:}\\
Podemos definir un momento alrededor de la media como:

\begin{equation}
m_p=\frac{1}{N}\sum^N_{i=1}(x_i-\bar{x})^p=<x'^p>\,.
\end{equation}

De esta forma $m_2$ es la varianza, $m_3$ es la asimetría, y $m_4$ la curtosis. El momento
$m_3$ indica la asimetría de la muestra alrededor de la media ($m_3>0$ implica distribución
con cola larga en la parte positiva y viceversa). $m_4$ indica el grado de esparcimiento
de las muestras alrededor de la media. Una mayor curtosis indica mayor concentración de
puntos alrededor de la media. Los momentos de orden superior ($>2$) se suelen adimensionalizar
dividiendo por la desviación estandar:

\begin{equation}
    m_3=\frac{1}{N}\sum^N_{i=1}\left[\frac{x_i-\bar{x}}{\sigma}\right]^3=<(x/\sigma)'^3>
\end{equation}

\begin{equation} 
    m_4=\frac{1}{N}\sum^N_{i=1}\left[\frac{x_i-\bar{x}}{\sigma}\right]^4-3=<(x/\sigma)'^4>-3
\end{equation}

donde el factor $-3$ hace que la curtosis tome el valor cero para una distribución
Normal.


\vspace{0.5cm}

\begin{center}
%\centering
%\includegraphics[width=9cm,angle=0]{skewness.png}
\end{center}

\begin{center}
Figura 1.1. Distribuciones con $m_3<0$ (izquierda) y $m_3>0$ (derecha).
\end{center}

\begin{center}
%\centering
%\includegraphics[width=9cm,angle=0]{Kurtosis.jpg}
\end{center}
\begin{center}
Figura 1.2. Distribuciones con diferentes grados de curtosis; $m_4>0$ (Leptocúrtica),
$m_4=0$ (Normal o Mesocúrtica), y $m_4<0$ (Platicúrtica).
\end{center}

\vspace{0.5cm}
 {\noindent  (5) Covarianza y correlación:}\\
La covarianza entre dos variables $x$ e $y$ puede
definirse como un estadístico que relaciona $x$ e $y$ de la
siguiente forma

$$C_{xy}=<x'y'>=<(x-\bar{x})(y-\bar{y})>=\frac{1}{N-1}\sum\limits^N_{i=1} (x_i-\bar{x})(y_i-\bar{y})\,.$$

La correlación es una covarianza normalizada

$$\rho_{x y}=\frac{C_{x y}}{s_x s_y}=\frac{<x' y'>}{\sqrt{<x'^2><y'^2>}}\,. $$

Consideremos el modelo estadístico lineal de media cero (es una recta que pasa
por $(\overline{x},\overline{y})=(0,0)$)

$$\hat{y}=\alpha x\,,$$

donde $\alpha$ es una constante. El error cometido
por este estimador se define como el error cuadratico
medio

$$\epsilon=<(\hat{y}-y)^2>=\alpha^2<x^2>+<y^2>-2\alpha<xy>$$

y si queremos minimizar dicho error entonces tenemos que encontrar que $\alpha$
es el que provoca que la derivada $\partial{\epsilon}/\partial{\alpha}\rightarrow{0}$. Es decir

$$\partial{\epsilon}/\partial{\alpha}=2\alpha<x^2>-2<xy>=0\,,$$

y el $\alpha$ es

$$\alpha=\frac{<xy>}{<x^2>}\,.$$

El error mínimo cuadrado se encuentra substituyendo el valor de $\alpha$ en
la expresión del error $\epsilon$ de arriba

$$\epsilon=\frac{<xy>^2}{<x^2>} + <y^2> - 2\frac{<xy>^2}{<x^2>}=
           <y^2>\left(\frac{<xy>^2}{<x^2><y^2>}+1-2\frac{<xy>^2}{<x^2><y^2>}\right)=$$
	   $$=<y^2>(1-\rho^2_{xy})\,.$$

Si $\rho^2_{xy}=1$ entonces el error es cero, es decir, mínimo error. Opuestamente, si $\rho^2_{xy}=0$ entonces el
error es igual a la varianza, es decir, máximo error. Si
$\rho$ toma valores intermedios, i.e., $\rho^2_{xy}=0.5$, entonces el
error es $\epsilon=0.5<y^2>$, es decir, el error del modelo lineal
 es un $50\%$ de la varianza. Por lo tanto, la correlación al
cuadrado puede definirse también ciomo la eficiencia relativa del
estimador $\hat{y}^2$ o la fracción de varianza
explicada por el modelo lineal

$$\rho^2_{xy}=\frac{<\hat{y}^2>}{<y^2>}=\frac{{\rm varianza\,\,\,explicada}}{{\rm varianza\,\,\,total}}\,.$$

A este parámetro se le puede encontrar en literatura inglesa
como {\it skill} del modelo lineal.

\subsection{Probabilidad}

\vspace{0.5cm}
{\noindent  Distribuciones de probabilidad:}\\

La {función de distribución acumulativa} $D_x(r)$ se define como la probabilidad
que una variable aleatoria $x$ sea menor o igual a $r$, es decir, $P(x\le r)$.
Matemáticamente
$$D(x)=\int^r_{-\infty}F(x)dx\,,$$
donde
$$F(x)=\frac{d}{dx} D(x)$$
es la función de densidad de probabilidad (PDF, por su siglas en inglés).
La PDF nos informa de la probabilidad que $x$ sea igual a un cierto valor $r$,
$P(x=r)$.
\\

{\noindent}Algunas propiedades de $D(x)$:
\\
\\
(1) $D(r)\le D(s)\,\,\,{\rm if}\,\,\,r\le s$
\\
(2) $D(-\infty)=0$
\\
(3) $D(\infty)=1$
\\

{\noindent}Algunas propiedades de $F(x)$:
\\
\\
(1) $F(x)\ge0$
\\
(2) $\int^{\infty}_{-\infty} F(x) dx=1$
\\

La probabilidad que una variable aleatoria $x$
este contenida en el intervalo $[r,r+dr]$ es la
integral de la función de densidad de probabilidad
$$P(r\le x\le r+dr)=\int^{r+dr}_r F(x)dx\,.$$

{\noindent}Ambas definiciones son parecidas aunque no son lo mismo.
Para ello veamos el ejemplo de la suma del lanzamiento de
dos dados al aire.
\\
\begin{center}
%\centering
%\includegraphics[width=10cm,angle=0]{prob_acumul_dados.png}
\end{center}

La densidad de probabilidad de que la suma de los dos dados sea
7 es máxima y que sea 2 o 12 es mínima. Este ejemplo describe
dos propiedades fundamentales de funciones de probabilidad discretas:
(i) $P(X=x) \ge 0$ y (ii)$\sum{P(x)}=1$. La distribución de probabilidad
acumulativa y la función de densidad de probabilidad tienen las siguientes
distribuciones
\\
\begin{center}
%\centering
%\includegraphics[width=10cm,angle=0]{prob_acumul_dados_figure.png}
\end{center}

\vspace{0.5cm}
{\noindent Momentos estadísticos de una función de densidad de probabilidad:}\\

Los momentos centrados (o alrededor de la media) de una distribución de probabilidad se definen como
$$m_r=E[(x-E[x])^r]=\int^{\infty}_{-\infty} (x-\mu)^rF(x)dx\,.$$

Como caso particular, los momentos alrededor del origen (i.e., $\mu=0$) son:
$$m^0_r=E[x^r]=\int^{\infty}_{-\infty} x^rF(x)dx\,.$$

Entonces, los primeros tres momentos centrados se definen como

$$m_0=E[(x-E[x])^0]=E[1]=\int^{\infty}_{-\infty}F(x)dx=1\,,$$
$$m_1=E[(x-E[x])^1]=E[x]-\mu=\int^{\infty}_{-\infty} (x-\mu)^1 F(x)dx=0\,,$$
$$m_2=E[(x-E[x])^2]=E[x^2 + E[x]^2 -2xE[x]]=
      E[x^2]+E[x]^2-2E[x]E[x]=$$
$$E[x^2]-E[x]^2=\underbrace{E[x^2]}_{\sigma^2}-\mu^2=\int^{\infty}_{-\infty} (x-\mu)^2 F(x)dx=\sigma^2-\mu^2\,.$$

Los momentos alrededor de cero ($\mu=0$) tambien pueden ser estandarizados:
$$m^*_r=m_r/\sigma^r=\frac{E[(x-E[x])^r]}{(\underbrace{E[(x-E[x])^2]}_{\sigma^2})^{r/2}}\,.$$

Los cuatro primeros momentos estadísticos alrededor de cero estandarizados son:
$$m^*_1=m_1/\sigma^1=\frac{E[(x-\mu)^1]}{(E[(x-\mu)^2])^{1/2}}=\frac{\mu-\mu}{\sqrt{E[(x-\mu)^2]}}=0\,,$$
$$m^*_2=m_2/\sigma^2=\frac{E[(x-\mu)^2]}{(E[(x-\mu)^2])^{2/2}}=1\,,$$
$$m^*_3=m_3/\sigma^3=\frac{E[(x-\mu)^3]}{(E[(x-\mu)^2])^{3/2}}\,,$$
$$m^*_4=m_4/\sigma^4=\frac{E[(x-\mu)^4]}{(E[(x-\mu)^2])^{4/2}}\,.$$

\vspace{0.5cm}
\noindent (1) Distribución uniforme:
\\
La distribución de probabilidad uniforme viene dada por
$$F(x)=\frac{1}{b-a}\,\,\,, a \le x \le b$$
$$=0,\,\,\,\,fuera\,\,del\,\,intervalo$$

Se deduce de la expresión de área de un cuadrado:

$$Area=base*altura=(b-a)F(x)=1$$

La función de distribución acumulativa es
$$D(x)=0,\,\,\,x<a$$
$$D(x)=\frac{x-a}{b-a},\,\,\,a \le x \le b$$
$$D(x)=1,\,\,\,x \ge b$$
\\
\begin{center}
%\centering
%\includegraphics[width=10cm,angle=0]{distribucion_uniforme.png}
\end{center}

La media es $\mu=(b+a)/2$ y la varianza es $\sigma^2=1/3(a^2 + b^2 +ab)$.
Demostración:
\\

{\noindent}Los momentos estadísticos alrededor del origen de la distribución uniforme son
    $$m^0_r=E[x^r]=\int^{\infty}_{-\infty} x^rF(x)dx=\int^{b}_{a} \frac{x^r}{b-a}dx=$$
    $$\frac{1}{b-a}\int^{b}_{a}x^r dx=\frac{1}{b-a}\left[\frac{x^{r+1}}{r+1}\right]^b_a=
      \frac{1}{b-a}\left[\frac{b^{r+1}}{r+1}-\frac{a^{r+1}}{r+1}\right]=\frac{b^{r+1}-a^{r+1}}{(b-a)(r+1)}$$

y por lo tanto la media es

$$m^0_1=E(x)=\frac{b^2-a^2}{2(b-a)}=\frac{(b-a)(b+a)}{2(b-a)}=\frac{b+a}{2}\,,$$

y la varianza

$$m^0_2=E(x^2)=\frac{b^3-a^3}{3(b-a)}=\frac{(b-a)(a^2+b^2+ab)}{3(b-a)}=\frac{1}{3}(a^2 + b^2 +ab)\,.$$

{ Ejemplo de distribución uniforme:} La ruleta rusa.
Supongamos que puede tomar 360 valores, es decir, $0 \le x \le 360$. Entonces

$$F(x)=\frac{1}{360},\,\,\,0 \le x \le 360\,,$$

y, por ejemplo, la probabilidad de que la bola caiga entre el 50 y el 360 es

$$P(50\le x \le 360)=\int^{360}_{50}\frac{1}{360}dx=\frac{1}{360}\left[x\right]^{360}_{50}=\frac{310}{360}=0.8611\,(\sim86\%).$$

La función de distribución acumulativa es

$$D(x)=0,\,\,\,x<0$$
$$D(x)=\frac{x}{360},\,\,\,0 \le x \le 360$$
$$D(x)=1,\,\,\,x \ge 360$$

\vspace{0.5cm}
{\noindent  (2) Distribución normal o Gaussiana:}\\
La distribución normal es una de las distribuciones mas recurrente
en la naturaleza. En general cualquier variable aleatoria medida, especialmente
aquellas que son suma de otras variables aleatorias, tiene
una distribución normal alrededor de la media
$$F(x)=\frac{1}{\sigma\sqrt{2\pi}}{\rm exp}
   \left\{ -\frac{(x-\bar{x})^2}{2\sigma^2} \right\}\,.$$

\begin{center}
%\centering
%\includegraphics[width=6cm,angle=0]{normal_distribution_PDF}
\end{center}

La distribución de función acumulativa normal se obtiene integrando
la expresión de arriba. Para ello vamos a realizar el cambio de
variable (que no es nada mas que estandarizar la
variable aleatoria x)
$$z=\frac{x-\bar{x}}{\sigma\sqrt{2}}$$ y $$dz=\frac{dx}{\sigma\sqrt{2}}\,,$$
de lo que se deduce
$$D(z)=\frac{\sigma\sqrt{2}}{\sigma\sqrt{2\pi}}\int^z_{-\infty} {\rm exp}
  \left\{ -z^2 \right\}dz =
  \frac{1}{\sqrt{\pi}}\int^z_{-\infty} {\rm exp}
  \left\{ -z^2 \right\} dz\,,$$

donde $\frac{2}{\sqrt{\pi}}\int^z_{0}{\rm exp}\left\{ -t^2 \right\}dt={\rm erf}(z)\,.$


\begin{center}
%\centering
%\includegraphics[width=7cm,angle=0]{normal_cumulative}
\end{center}

Los momentos estadísticos alrededor del origen de la función de distribución Normal son

$$m^0_r=E[x^r]=\int^{\infty}_{-\infty} x^rF(x)dx=\frac{1}{\sigma\sqrt{2\pi}}\int^{\infty}_{-\infty}x^r{\rm exp}
   \left\{ -\frac{(x-\bar{x})^2}{2\sigma^2} \right\} dx$$

Hagamos el cambio de variable

$$u=\frac{x-\bar{x}}{\sigma\sqrt{2}}$$
$$du=\frac{dx}{\sigma\sqrt{2}}$$

Si substituimos en la expresión de $m_r$ obtenemos

$$m^0_r=\frac{\sigma\sqrt{2}}{\sigma\sqrt{2\pi}}\int^{\infty}_{-\infty}
  \left( \sigma \sqrt{2}u+\bar{x}\right)^r{\rm e}^{-u^2}du=
  \frac{1}{\sqrt{\pi}}\int^{\infty}_{-\infty}\left( \sigma \sqrt{2}u+\bar{x}\right)^r{\rm e}
   ^{-u^2}du\,.$$

\vspace{0.25cm}
{\noindent}{ Ejercicio}: Deducir los momentos estadísticos de orden 1 y 2 de la
distribución Normal, es decir, la media y la varianza. Integrales útiles:

$$\int e^{-ax^2}dx=\frac{\sqrt{\pi}}{2\sqrt{a}}{\rm erf}(x\sqrt{a})$$
$$\int xe^{-ax^2}dx=-\frac{1}{2a}e^{-ax^2}\,,$$

donde la función de error se define cómo:

$$erf(z)=\frac{2}{\sqrt{\pi}}\int_0^z e^{-t^2} dt\,.$$

La función de error cumple las siguientes identidades:
$${\rm erf}(0)=\frac{2}{\sqrt{\pi}}\int_0^0 e^{-t^2} dt=0$$,
$${\rm erf}(\infty)=\frac{2}{\sqrt{\pi}}\underbrace{\int_0^\infty e^{-t^2} dt}_{\frac{\sqrt{\pi}}{2}}=1$$.
$${\rm erf}(-\infty)=\frac{2}{\sqrt{\pi}}\underbrace{\int_0^{-\infty} e^{-t^2} dt}_{-\frac{\sqrt{\pi}}{2}}=-1$$.

\vspace{0.25cm}
{\noindent}{ Respuesta}: La media y la varianza son

$$m^0_1=E(x)=\mu\,\,\,\,\,\,;\,\,\,\,\,\,m_1=0$$
$$m^0_2=Var(x)=\mu^2 + \sigma^2\,\,\,\,\,\,;\,\,\,\,\,\,m_2=\sigma^2$$

\vspace{0.25cm}
{\noindent}{ Deducción de la media}:
El momento de orden 1 centrado es:
$$m_1=\frac{\sigma\sqrt{2}}{\sqrt{\pi}}\int^{\infty}_{-\infty}u{\rm e}
   ^{-u^2}du=-\frac{\sigma\sqrt{2}}{2\sqrt{\pi}}{\rm e}
   ^{-u^2}\Big|^{\infty}_{-\infty}=0\,.$$
\\
y alrededor de cero (momentos crudos):
$$m^0_1=\frac{1}{\sqrt{\pi}}\int^{\infty}_{-\infty}(\sigma \sqrt{2} u + \mu){\rm e}
   ^{-u^2}du=\frac{\sigma\sqrt{2}}{\sqrt{\pi}}\int^{\infty}_{-\infty}u{\rm e}
   ^{-u^2}du + \frac{1}{\sqrt{\pi}}\int^{\infty}_{-\infty}\mu{\rm e}
   ^{-u^2}du=$$
\\
$$=\underbrace{\frac{\sigma\sqrt{2}}{\sqrt{\pi}}\left( -\frac{1}{2}{\rm e}
   ^{-u^2}\right)}_{0} \Big|^{\infty}_{-\infty}+
   \frac{\mu}{\sqrt{\pi}}\left( \frac{\sqrt{\pi}}{2}{\rm erf}(u)\right) \Big|^{\infty}_{-\infty}=
   \frac{\mu}{2}[1-(-1)]=\mu\,,$$

\vspace{0.25cm}
{\noindent}{ Deducción de la varianza}:
El momento de orden 2 centrado es:\\
$$m_2=\frac{(\sigma\sqrt{2})^2}{\sqrt{\pi}}\int^{\infty}_{-\infty}u^2{\rm e}
   ^{-u^2}du$$

$$x=u \rightarrow dx=du$$
$$dy=u{\rm e}^{-u^2} \rightarrow y=\frac{1}{2}{\rm e}^{-u^2}$$

$$m_2=\frac{2\sigma^2}{\sqrt{\pi}}\left[ -\frac{1}{2}{\rm e}^{-u^2}
      \Big|^{\infty}_{-\infty} +\frac{\sqrt{\pi}}{4} {\rm erf}(u)\Big|^{\infty}_{-\infty}\right]=
  \frac{2\sigma^2}{\sqrt{\pi}}\left[0 + \frac{\pi}{4}\left({\rm erf}(\infty) - {\rm erf}(-\infty) \right)
  \right]=$$
  $$=\frac{2\sigma^2}{\sqrt{\pi}}\left[ \frac{\pi}{4}+\frac{\pi}{4}\right]=\sigma^2$$
\\
y alrededor de cero (momentos crudos):
$$m^0_2=\frac{1}{\sqrt{\pi}}\int^{\infty}_{-\infty}(\sigma \sqrt{2} u + \mu)^2{\rm e}^{-u^2}du=
   \frac{1}{\sqrt{\pi}}\int^{\infty}_{-\infty}2\sigma^2 u^2{\rm e}^{-u^2}du +$$
$$
   +\frac{\mu^2}{\sqrt{\pi}}\int^{\infty}_{-\infty}{\rm e}^{-u^2}du
   +\frac{2\sigma\sqrt{2}\mu}{\sqrt{\pi}}\int^{\infty}_{-\infty}u{\rm e}^{-u^2}du=$$
$$
   =\underbrace{\frac{2\sigma^2}{\sqrt{\pi}}\int^{\infty}_{-\infty}u^2{\rm e}^{-u^2}du}_{m_2=\sigma^2}
   +\mu \underbrace{\frac{1}{\sqrt{\pi}}\int^{\infty}_{-\infty}\mu{\rm e}^{-u^2}du}_{m_1^0=\mu}+
   +2\mu \underbrace{\frac{\sigma\sqrt{2}}{\sqrt{\pi}}\int^{\infty}_{-\infty}u{\rm e}^{-u^2}du}_{m_1=0}=
   \sigma^2+\mu^2\,,$$
\\
La probabilidad de que una variable normalmente distribuida
caiga en una desviación estandard de su valor medio viene
dada por
$$P(-1\le z \le 1)=\int^{+1}_{-1} F(z) dz=\frac{1}{\sqrt{2\pi}}\int^{+1}_{-1}e^{-\frac{1}{2}z^2}dz=$$
$$=\frac{1}{\sqrt{2\pi}}\frac{\sqrt{\pi}}{2\sqrt{1/2}}\left[{\rm erf}({z\sqrt{1/2}})\right]^1_{-1}=
   \frac{1}{2}\left[{\rm erf}({1/\sqrt{2}})-{\rm erf}({-1/\sqrt{2}}) \right]=$$
$$=\frac{1}{2}\left[0.6827-(-0.6827)\right]=0.6827\,(~68.27\%)\,,$$
y similarmente para 2 y 3 desviaciones estandard
 $$P(-2\le z \le 2)=\int^{+2}_{-2} F_x(z) dz=95.45\%$$
 $$P(-3\le z \le 3)=\int^{+3}_{-3} F_x(z) dz=99.73\%\,.$$
Entonces solo hay un $4.55\%$ de probabilidad de que una variable
normalmente distribuida caiga fuera de dos desviaciones
estandard respecto de la media. Puesto que es una probabilidad
con 2 colas, la probabilidad de que una variable normal exceda su
media por mas de $2\sigma$ es la mitad de esto, es decir $2.275\%$,
ya que la distribución normal es simétrica.
\\
\\
En la práctica una PDF se calcula como un histograma escalado.
Es por ello que necesitamos escojer el tamaño y localización
de los bins en el histograma. La demo muestra las consecuencias
de esta elección (pdf\_demo.m).

\begin{center}
%\centering
%\includegraphics[width=12cm,angle=0]{probability_normal}
\end{center}

\vspace{0.5cm}
{\noindent  (3) Distribución de Poisson:}\\

La distribución de Poisson expresa la probabilidad de que ocurra un
determinado número de eventos durante un cierto intervalo en el tiempo
o distancia en el espacio. Se usa generalmente para la ocurrencia de
sucesos con muy poca probabilidad o muy ``raros''. La expresión
para la función acumulativa es:

$$D(x)=P(x \le r)=e^{-\lambda}\sum^{|r|}_{k=0}\frac{\lambda^k}{k!}\,,$$

donde $\lambda$ es el valor promedio

\begin{center}
%\centering
%\includegraphics[width=6cm,angle=0]{distribucion_Poisson_CDF}
\end{center}

y la función de densidad de probabilidad

$$F(x)=P(x=k)=\frac{{\lambda}^k e^{-\lambda}}{k!}$$


\begin{center}
%\centering
%\includegraphics[width=6cm,angle=0]{distribucion_Poisson_PDF}
\end{center}

Los momentos estadísticos alrededor del origen de la función de distribución de Poisson se pueden
calcular directamente con sumatorios y expansion de Taylor:

$$m^0_r=E[x^r]=\sum k^r P(X=k)=\sum_{k\ge0} k^rF(x)= \sum_{k\ge0} k^r\frac{{\lambda}^k e^{-\lambda}}{k!}=$$
$$=\lambda e^{-\lambda}\sum_{k \ge 0}\frac{k^r}{k!}\lambda^{k-1}$$

{\noindent}Veamos el momento de orden 1 alrededor del orígen:

$$m^0_1=E[x]=\lambda e^{-\lambda}\sum_{k \ge 0}\frac{k}{k!}\lambda^{k-1}=
 \lambda e^{-\lambda}\sum_{k \ge 1}\frac{k}{(k-1)!k}\lambda^{k-1}=
 \lambda e^{-\lambda}\sum_{k \ge 1}\frac{1}{(k-1)!}\lambda^{k-1}$$
$$=\lambda e^{-\lambda}\sum_{j \ge 0}\frac{\lambda^{j}}{j!}\,,$$
para $j=k-1$. Finalmente expandiendo en series de Taylor la función exponencial

$$e^{\lambda}=\sum_{j \ge 0}\frac{1}{j!}\lambda^{j}$$, obtenemos:

$$m^0_1=\lambda e^{-\lambda}e^{\lambda}=\lambda$$

{\noindent}{ Ejercicio}: Demostrar que el momento estadístico alrededor del origen de orden 2 de la distribución de
Poisson es igual a $m^0_2=\lambda+\lambda^2$.

$$m^0_2=E[x^2]=\lambda e^{-\lambda}\sum_{k \ge 0}\frac{k^2}{k!}\lambda^{k-1}=
               \lambda e^{-\lambda}\left[ \sum_{k \ge 1}(k-1)\frac{1}{(k-1)!}\lambda^{k-1} +
	                                  \sum_{k \ge 1} \frac{1} {(k-1)!} \lambda^{k-1} \right]=$$
		       $$= \lambda e^{-\lambda}\left[ \lambda \sum_{k \ge 2}\frac{1}{(k-2)!}\lambda^{k-2} +
	                                     \sum_{k \ge 1}\frac{1}{(k-1)!}\lambda^{k-1}\right]=$$
		       $$= \lambda e^{-\lambda}\left[ \lambda \sum_{j \ge 0}\frac{1}{j!}\lambda^{j} +
	                                     \sum_{i \ge 0}\frac{1}{(i)!}\lambda^{i}\right]=
					     \lambda e^{-\lambda} \left[\lambda e^{\lambda} + e^{\lambda}\right]=$$
					     $$=\lambda (\lambda +1)=\lambda^2 + \lambda\,.$$

En el casso que fuera el momento de orden 2 centrado se escribiría:

$$E[(x-E[x])^2]=E[x^2]-(E[x])^2=\lambda^2 + \lambda - (\lambda)^2=\lambda\,.$$

\vspace{0.25cm}
{\noindent}{ Ejemplo:} En los últimos 160 años, han sucedido 680 tormentas
intensas en el Golfo de México, incluyendo depresiones, 
tormentas tropicales, y huracanes. Asumimos que la frecuencia de ocurrencia de una tormenta intensa en el Golfo de México 
sigue una distribución de Poisson (eventos ``raros'', poco frecuentes).
Calcula la probabilidad de que ocurran 2 huracanes en 1 año:\\

(a) El número promedio de tormentas intensas por año es:
$\mu=680/160=4.25$ huracanes/año.

(b) La probabilidad de que ocurran 2 huracanes en 1 año es:
$$P(x=2)=\frac{{\lambda}^2 e^{-\lambda}}{2!}=\frac{{4.25}^2 e^{-4.25}}{2!}=0.1288\,(\sim12\%)$$

La probabilidad es muy baja debido a que exigimos que sean exactamente 2
huracanes en un año y no, por ejemplo, $>2$. En el segundo caso, la
probabilidad aumentaría considerablemente

$$P(x>2)=P(x=3)+P(x=4)+....=1-P(x\le 2)=1-[P(x=0)+P(x=1)+P(x=2)]=$$
$$=1-[0.0143+0.0606+0.1288]=1-0.2037=0.7963\,(\sim80\%)$$

\vspace{0.5cm}
{\noindent  (4) Distribución Binomial:}\\

Supongamos que tenemos un conjunto de $n$ tiradas en los cuales
pueden suceder únicamente dos cosas: `acierto' o `fallo'. La
probabilidad de acertar en una tirada es p=P. Si $X$ es el
número total de aciertos en $n$ tiradas, entonces la probabilidad
de que el número de aciertos sea $k$ es:
$$P(X=k)=\left( \begin{array}{c}
 n \\ k
       \end{array} \right)
p^k (1-p)^{n-k}, \, k=0,1,2,3,....,n\,,$$

{\noindent}donde la expresión

$$\left( \begin{array}{c}
 n \\ k
       \end{array} \right)=C(n,k)\equiv\frac{n!}{(n-k)!k!}\,,$$
\\
{\noindent}es el número de diferentes combinaciones de grupos de k objetos
que pueden ser elegidos de un conjunto total de n objetos. Estos números se
denominan coeficientes binomiales. La probabilidad de que el número de
aciertos caiga en un rango de valores es

$$P(a\le X\le b)=\sum^b_a P(X)$$

\vspace{0.5cm}
 {\noindent  Ejemplo 1:}
 ?`Cual es la probabilidad de obtener exactamente 6 caras
 de 10 lanzamientos de moneda? Respuesta:

 $$P(x=6) = C(10,6)0.5^6(1-0.5)^{10-6} = \frac{10!}{(10-6)!6!}0.5^6(1-0.5)^{10-6} \simeq 0.205$$

 \vspace{0.5cm}
 {\noindent  Ejemplo 2:}
?`Cual es la probabilidad de obtener mas de 15 caras
 de 20 lanzamientos de moneda? Respuesta:

$$\sum^{20}_{k=16} \left( \begin{array}{c} 20 \\ k
       \end{array} \right) 0.5^k(1-0.5)^{20-k}=0.006\,.$$

Si realizas esta operación a mano se vuelve muy tediosa. Es por ello que
se utiliza la aproximación Normal a la distribución Binomila (DeMoivre-Laplace).

  \vspace{0.5cm}
 {\noindent  Teorema de DeMoivere-Laplace (aproximación de Binomial a Normal)}\\

 La distribución binomial de una variable X definida por n tiradas independientes cada una de las cuales tienen una probabilidad 
 $p$ de acertar, es aproximadamente una distribución Normal de media $np$ y desviación típica $\sqrt{np(1-p)}$, cuando n es suficientemente grande. Entonces se deduce que para cualquier número a y b,


$$lim_{n\rightarrow\infty} P \left( a<\frac{X-np}{\sqrt{np(1-p)}}<b\right)= \frac{1}{\sqrt{2\pi np(1-p)}}\int^b_a exp-\left[\frac{(x-np)^2}{2np(1-p)}\right]dx\,.$$\\
 
 Esto significa que el estadístico, $\frac{X-np}{\sqrt{np(1-p)}}$ , tiene
 una distribución Normal. Este teorema es un caso particular del teorema
 del límite central y nos permite de simplificar
 la solución de un problema binomial.

  \vspace{0.5cm}
 {\noindent  Ejemplo de la aproximación Normal a la distribución Binomial:}
 \\
 El 2\% de los XBTs fabricados por una empresa presentan defectos. Si hemos
 adquirido 2000 XBTs, ?`Cual es la probabilidad de que haya menos de 50 defectuosos?
 \\
 { Respuesta}: Se trata de una distribución binomial ya que solo pueden
 ser defectuosos o no defectuosos. La probabilidad que sea defectuoso
 es $p=0.02$ (2\%) y $n=2000$, lo que nos da una distribución
 Binomial $B(2000,0.02)$. Puesto que la $n$ es grande podemos hacer una aproximación a la
 distribución Normal. Calculamos la media y desviación estandar
 de la distribución Normal
 $\mu=np=200*0.02=40$ y $\sigma=\sqrt{np(1-p)}=\sqrt{2000*0.02*(1-0.02)}=6.26$
$x$ es $B(2000,0.02)$ y $x_N$ es $N(40,6.26)$.
\\
La probabilidad que $x<50$ es
$$p(x<50)=p(x_N\le 49)\,,$$
y si estandarizamos
$$p(x_N\le 49)=p\left(z\le \frac{49-40}{6.26} \right)=p(z\le 1.44)=0.9251\,.$$


\vspace{0.5cm}
 {\noindent  EJERCICIOS de estadística y probabilidad:}\\

 \vspace{0.5cm}
 {\noindent  Ejercicio 1:}
Calcule E[x] si x tiene la función de densidad de probabilidad

 $$f(x)=\Bigg\{\begin{array}{c}
 \frac{1}{4}xe^{-\frac{x}{2}}\,\,\,\,\,,x>0 \\ 0 \,\,\,\,\,,otherwise
       \end{array} \,.$$

La esperanza E[x] de la función $f(x)$ es entonces
$$E[x]=\int^\infty_0 x\left(\frac{1}{4}xe^{-\frac{x}{2}}\right)dx
      =\frac{1}{4}\int^\infty_0 x^2e^{-\frac{x}{2}}dx\,.$$
Definamos $y=x/2$; entonces $x=2y$ y $dx=2dy$ y obtenemos

  $$E[x]=\frac{1}{4}\int^\infty_0 x^2e^{-\frac{x}{2}}dx
        =\frac{1}{4}\int^\infty_0 (2y)^2e^{-y}2dy
	=2\int^\infty_0y^2e^{-y}dy\,.$$
Vamos ahora a resolver la integral por partes. Hacemos la siguiente sustitución:
$u=y^2$, $dv=e^{-y}$ y por ende $du=2ydy$ y $v=-e^{-y}$. La integral se puede
reescribir usando la expresión general de integración por partes
$h(x)=uv-\int vdv$:

$$E[x]=2\int^\infty_0y^2e^{-y}dy=2\left[ -y^2e^{-y}-\int-e^{-y}(2y)dy\right]
      =2\left[ -y^2e^{-y}+2\int ye^{-y}dy\right]\,.$$
Integramos de nuevo por partes. Usa $u=y$, $dv=e^{-y}$ y entonces $du=dy$ y $v=-e^{-y}$
$$E[x]=2\left[ -y^2 e^{-y} + 2 \Big\{ -y e^{-y} - \int -e^{-y}dy \Big\} \right]=$$
$$=2\left[ -y^2 e^{-y} + 2 \Big\{ -y e^{-y} - e^{-y}         \Big\} \right]=$$
$$=-2y^2e^{-y}-4ye^{-y}-4e^{-y}=$$
$$=\left[ -2e^{-y}(y^2+2y+2) \right]^{\infty}_0=$$
$$=\lim_{n \to\infty}\left[ -2e^{-y}(y^2+2y+2) \right]-\left[ -2e^{-y}(y^2+2y+2) \right]_{y=0}\,.$$
El límite es ahora del tipo $\infty/\infty$ y entonces usamos la regla de l'Hopital
$$E[x]=-2\lim_{y \to\infty}\frac{2y+2}{e^y}+2\left[e^{-y}(y^2+2y+2) \right]_{y=0}$$
Usamos la regla de l'Hopital de nuevo
$$E[x]=-2\lim_{y \to\infty}\frac{2}{e^y}+2\left[e^{-y}(y^2+2y+2) \right]_{y=0}
=0+2\left[e^{-y}(y^2+2y+2) \right]_{y=0}=4$$

\vspace{0.5cm}
 {\noindent  Ejercicio 2:}
Calcule E[x] si x tiene la función de densidad de probabilidad

 $$f(x)=\Bigg\{\begin{array}{c}
 c(1-x^2)\,\,\,\,\,,-1<x<1 \\ 0 \,\,\,\,\,,otherwise
       \end{array} \,.$$

$$E[x]=\int^1_{-1} x[c(1-x^2)]dx=c \int^1_{-1} x[(1-x^2)]dx=$$
$$=c \int^1_{-1} x-x^3dx=c\left[\frac{x^2}{2}+\frac{x^4}{4}\right]^{1}_{-1}=0$$

\vspace{0.5cm}
 {\noindent  Ejercicio 3:}
Calcule E[x] si x tiene la función de densidad de probabilidad

 $$f(x)=\Bigg\{\begin{array}{c}
 \frac{5}{x^2}\,\,\,\,\,,x>5 \\ 0 \,\,\,\,\,,x\le5
       \end{array} \,.$$

$$E[x]=\int^{\infty}_{5}x\frac{5}{x^2}dx=\int^{\infty}_{5}\frac{5}{x}dx=5\int^{\infty}_{5}\frac{1}{x}dx$$
$$=5[lnx]^{\infty}_5=5\left[\Big(\lim_{x \to\infty} ln{x}\Big)-ln{5}\right]\rightarrow \infty$$


\vspace{0.5cm}
 {\noindent  Ejercicio 4:}
La variable aleatoria $x$ tiene la siguiente función de densidad de probabilidad

  $$f(x)=\Bigg\{\begin{array}{c}
          k(2x+3)\,\,\,\,\,-1\le x \le 2 \\ 0 \,\,\,\,\,otherwise
         \end{array} \,.$$

\begin{itemize}
\item ?`Cuál es el valor de k?

$$\int^{\infty}_{-\infty} k(2x+3)dx=1$$

$$\int^{2}_{-1} k(2x+3)dx=(kx^2 +3kx)\Big|^{2}_{-1}=kx(3+x)\Big|^{2}_{-1}=10k+2k=1$$

$$12k=1 \rightarrow k=\frac{1}{12}$$


\item Calcula $E[x]$

$$E[x]=\int^{2}_{-1}x k(2x+3)dx=2k\frac{x^3}{3}\Bigg|^{2}_{-1} + 3k\frac{x^2}{2}\Bigg|^{2}_{-1}=$$
$$=k\left[ \frac{18}{3} + \frac{9}{2}\right]=\frac{21}{24}\,.$$

\end{itemize}

\vspace{0.5cm}
 {\noindent  Ejercicio 5:}
Sea la función $g(x)$ dada por


 $$g(x)=\Bigg\{\begin{array}{c}
 x+2\alpha\,\,\,\,\,,x\le-\alpha \\
 x \,\,\,\,\,,-\alpha \le x \le \alpha \\
 x-2 \alpha \,\,\,\,\,,x>\alpha
       \end{array} \,,$$

donde asumimos que x esta normalmente distribuida.
Calcula la media de $g(x)$.

$$E[g(x)]=\int^{\infty}_{-\infty} g(x) F(x) dx=\int^{-\alpha}_{-\infty} (x+2\alpha) F(x) dx +
\int^{\alpha}_{-\alpha} x F(x) dx + \int^{\infty}_{\alpha} (x-2\alpha) F(x) dx=$$
$$=\int^{-\alpha}_{-\infty} x F(x) dx + \int^{-\alpha}_{-\infty} 2 \alpha F(x) dx +
   \int^{\alpha}_{-\alpha} x F(x) dx + \int^{\infty}_{\alpha} x F(x) - \int^{\infty}_{\alpha} 2\alpha F(x)dx=$$
$$=\int^{-\infty}_{-\infty} x F(x) dx +  2\alpha \left[ \int^{-\alpha}_{-\infty} F(x) dx - \int^{\infty}_{\alpha}  F(x)\right]=$$
$$=\int^{-\infty}_{-\infty} x F(x) dx +  2\alpha \left[ D(x=-\alpha) - \Bigg(1-\int^{-\alpha}_{-\infty} F(x) dx\Bigg)\right]=$$
$$=\int^{-\infty}_{-\infty} x F(x) dx +  2\alpha \left[ D(x=-\alpha) - \Bigg(1-D(x=\alpha)\Bigg)\right]=$$
$$=\mu + 2 \alpha \left[ D(-\alpha) - 1 + D(\alpha) \right]\,.$$

donde la media de la distribución Normal es

$$E[x]=\int^{-\infty}_{-\infty} x F(x) dx=\int^{-\infty}_{-\infty} x \frac{1}{\sigma \sqrt{2\pi}}
e^ {-\frac{1}{2} \big( \frac{x-\mu}{\sigma} \big)^2 } dx=\mu\,.$$


\vspace{0.5cm}
 {\noindent  Teorema central del límite:}
 \\
{ Definición 1}: Sea $X_1,\,X_2,\,X_3,...,X_n$ un conjunto de variables aleatorias, independientes e
idénticamente distribuidas
con media $\mu$ y varianza $\sigma^2$ distinta de cero. Sea
$$S_n=X_1+X_2+....+X_n\,,$$
entonces
$$\lim_{n \to\infty} Pr (Z_n\le z)= \Phi(z)\,,$$
donde $\Phi(z)$ es una distribución Normal estandar y
$Z_n=\frac{S_n - n\mu} {\sigma\sqrt{n}} = \frac{\bar{X} - \mu} {\sigma/\sqrt{n}}$ es una estandarización
del sumatorio $S_n$ de tal forma que la media de la
nueva variable $Z_n$ sea cero y su desviación estándard sea igual a 1. De esta forma, las variables
$Z_n$ convergerán a una distribución normal estándar $N(0,1)$, cuando $n$ tienda a infinito.
\\
\\
{ Definición 2}: Sea $X_1,\,X_2,\,X_3,...,X_n$ un conjunto de variables aleatorias, independientes e
idénticamente distribuidas con media $\mu$ y varianza $\sigma^2$ distinta de cero. Entonces, si $n$ es suficientemente grande,
la variable aleatoria
$$\bar{X}=\frac{1}{n}\sum^n_{i=1}{X_i}$$
tiene aproximadamente una distribución normal con media $\mu(\bar{X})=\mu$ y desviación
típica $\sigma(\bar{X})=\sigma/\sqrt{n}$.
\\
\\
{ NOTA}: Es importante remarcar que el teorema del límite central no dice nada acerca de la
distribución de $X_i$, solo de la distibución de su media muestral $\bar{X}$.
%
%
\\
\\
{ Aplicación 1:} Calculo de probabilidades sobre la media muestral.
\\
\\
{ Ejemplo:} La recolección de muestras de agua con una roseta es una variable aleatoria con
media $\mu=150\,{\rm ml}$ y varianza de $\sigma^2=120\,{\rm ml}^2$. Si tomamos $n=40$ muestras
aleatorias de agua. (a) ?`Cual es la media y la desviación estándar de la media muestral?, (b) ?`Cual es la
probabilidad de que la media muestral contenga entre $145$ y $153\,{\rm ml}$ de agua?
\\

(a) $\mu(\bar{X})=150\,{\rm ml}$ y $\sigma(\bar{X})=\sigma/\sqrt{n}=\sqrt{120/40}= \sqrt{3}\,{\rm ml}$
\\
(b) Queremos calcular $Pr(145 \le \bar{X} \le 153)$. Si escribimos la probabilidad
    en forma estandarizada, entonces:

$$Pr(145 \le \bar{X} \le 153) =
      Pr\left( \frac{145-150}{\sqrt{3}} \le Z \le \frac{153-150}{\sqrt{3}}\right)
       \simeq Pr(-2.89 \le Z \le 1.73)=$$

$$=Pr(Z \le 1.73)- Pr(Z \le -2.89)=0.9582-(1-0.9981)=0.9582-0.0019=0.9563$$

\vspace{0.5cm}
 {\noindent  Función de densidad de probabilidad conjunta}
 \\
 La probabilidad que dos variables aleatorias $(x,y)$ caigan en la
 región $R$ (como por ejemplo un rectángulo) se obtiene integrando
 su función de probabilidad conjunta
 $$P((x,y)\in R)=\int\int_{R} F(x,y) dx dy\,.$$
 En particular, si $R$ es un rectángulo 2d ${(x,y):r\le x \le r+dr, s \le y \le s+ds}$, entonces
 $$P((x,y)\in R)=P(r\le x \le r+dr, s \le y \le s+ds)=\int^{r+dr}_r\int^{s+ds}_{s} F(x,y) dx dy\,.$$
Algunas propiedades:
\\
(1) $F(x,y)\ge0$ para todo x,y.
\\
(2) $\int^{\infty}_{-\infty}\int^{\infty}_{-\infty} F(x,y) dx dy=1$
\\

{ Definición}: La función de densidad de probabilidad marginal de variables aleatorias
$x$ e $y$ son:
\\
$Fx(x)=\int^{\infty}_{-\infty} F(x,y)dy$ y $Fy(y)=\int^{\infty}_{-\infty} F(x,y)dx\,.$

\vspace{0.5cm}
 {\noindent  Ejemplo del uso de la función de densidad de probabilidad conjunta}
 \\
Imaginemos que una empresa de instrumentación oceanográfica fabrica
boyas Lagrangianas de grosor $x$ y diámetro $y$, los cuales varian de
una boya a la otra. Imaginemos que la función de densidad de probabilidad conjunta
de la variable aleatoria `'dimensión del instrumento oceanográfico'' es:

$$F(x,y)=\frac{1}{6}(r+s)\,\,\,si\,\,\,(x,y)\in R=\{1\le x \le 2 ; 4 \le y \le 5\}$$

$$F(x,y)=0\,\,\,si\,\,\,(x,y)\,fuera\,de\,R$$

Ahora queremos saber que probabilidad hay de que una boya tenga un grosor $1 \le x \le 1.5m$
y un diámetro $4.5 \le y \le 5m$, es decir
$$P(1 \le x \le 1.5, 4.5 \le y \le 5)=\int^{1.5}_{1} \int^{5}_{4.5} \frac{1}{6}(r+s) ds dr = 0.253= 25\%$$


\vspace{0.5cm}
 {\noindent  Significancia estadística utilizando la distribución Normal}
 \\
Como vimos en el teorema central del límite, para una población infinita ($N\rightarrow\infty$) la
desviación estandar de la distribución de las medias muestrales es:
$$\sigma(\bar{x})=\frac{\sigma}{\sqrt{N}}=error\,estandar\,del\,estimado\,de\,la\,media\,.$$
Aqui $\sigma$ es la desviación estandard de la población y $N$ es el número de
datos utilizado para calcular la media muestral. Entonces, si promediamos
observaciones de una población de desviación estandar $\sigma$, la
desviación estandard de esos promedios disminuye como el inverso de
la raíz cuadrada del tamaño muestral $N$.
\\
\\
Si $N$ es suficientemente grande podemos usar las estimaciones de $\sigma$ y $\bar{x}$
para calcular el denominado estadístico $z$ que corresponde a una distribución normal
estandarizada de media $\mu=0$ y $\sigma=1$
$$z=\frac{\bar{x}-\mu}{\sigma(\bar{x})}=\frac{\bar{x}-\mu}{\frac{\sigma}{\sqrt{N}}}$$
\\
\\
La fórmula de arriba puede modificarse convenientemente para darnos un test de significancia
estadística para la diferencia entre medias muestrales con tamaños muestrales y desviación
estandar diferentes:
$$z=\frac{\bar{x}_1-\bar{x}_2-\Delta_{1,2}}
  {\frac{\sigma_1^2}{\sqrt{N_1}} + \frac{\sigma_2^2}{\sqrt{N_2}}}\,,$$
donde $\Delta_{1,2}$ es la diferencia esperada entre las dos medias, lo
que se suele asumir cero en la práctica.
\\
\\
Si el tamaño muestral $N$ es menor de $30$ entonces no podemos usar el
estadístico $z$, pero podemos utilizar la distribución {\it t-student};
o cuando queremos comparar varianzas, podemos usar la distribución
{\it chi-cuadrada}. La {\it t-student} converge a una distribución normal
para largos tamaños muestrales y se define como
$$t=\frac{\bar{x}-\mu}{\frac{s}{\sqrt{N-1}}}=\frac{\bar{x}-\mu}{\frac{\hat{s}}{\sqrt{N}}};
\hat{s}=\sqrt{\frac{N}{N-1}s}\,.$$
Si consideramos una población normalmente distribuida de media $\mu$ la
función de densidad de probabilidad de la {\it t-student} es
$$F_{x}(t)=\frac{f_0(\nu)}{\left(1+\frac{t^2}{\nu} \right)^{\frac{\nu+1}{2}}}\,,$$
donde $\nu=N-1$ es el número de grados de libertad y $f_0(\nu)$ es una
constante que depende en $\nu$ y permite que el área bajo la curva
$F_x(t)$ sea igual a la unidad. Los grados de libertad se definen como
el número de muestras independientes $N$ menos el número de parámetros
del estadístico que queremos estimar.
\\
\\
A diferencia del estadístico $z$, la {\it t-student} depende del número
de grados de libertad; la cola de la distribución es larga para números de
grados de libertad bajos (o $N$ pequeña). Para números altos de grados de libertad (o $N$ grande), la distribución {\it t-student} se acerca al estadístico $z$ o distribución Normal.

\vspace{0.5cm}
 {\noindent \textbf Intervalos de confianza}\\

Para calcular valores de los estadísticos $z$ y {\it t-student} debemos
de fijar el nivel de confianza definido como $1-\alpha$; porcentaje
del nivel de confianza $100(1-\alpha)\%$. Esto se puede escribir simbolicamente cómo
$$P(-z_{\alpha/2}<z<z_{\alpha/2})=1-\alpha$$

$$P(-t_{\alpha/2}<t<t_{\alpha/2})=1-\alpha\,.$$

Una vez definido el nivel de confianza y los grados de libertad $\nu$ (para la {\it t-student}) podemos
leer el valor de dichos estadísticos en tablas. En esas tablas
$z_{\alpha/2}$ es el valor de $z$ para el cual solo el $100*{\alpha/2}\%$ de los
valores de $z$ es esperado ser mas grande (cola de la derecha de la distribución).
Igualmente, $z_{-\alpha/2}=-z_{\alpha/2}$ es el valor de $z$ para el cual solo el $100*{\alpha/2}\%$
de los valores de $z$ es esperado ser mas pequeño (cola de la izquierda de la distribución).
O dicho de otra forma,$z_{\alpha/2}$ es el valor por encima del cual existe un área bajo la curva de
$\alpha/2$. Los valores de $z$ y $t$ son las integrales bajo las correspondientes funciones de
densidad de probabilidad.
\begin{center}
%\centering
%\includegraphics[width=17cm,angle=0]{intervalo_confianza}
\end{center}


\noindent (1) {\textbf Intervalo de confianza para $\mu$ ($N>30$, $\sigma$ conocida)}
\\
 Cuando $N>30$ y $\sigma$ es conocida, podemos usar el estadístico $z$
 para encontrar el intervalo de confianza para $\mu$. Hay un $100*(1-\alpha)\%$
que cualquier estadístico $z$ caiga en el intervalo

 $$z_{-\alpha/2}<\frac{\bar{x}-\mu}{\sigma}\sqrt{N}<z_{\alpha/2}$$
 $$\frac{\sigma}{\sqrt{N}}z_{-\alpha/2}<{\bar{x}-\mu}<\frac{\sigma}{\sqrt{N}}z_{\alpha/2}$$
 $$-1\frac{\sigma}{\sqrt{N}}z_{-\alpha/2}>{\mu-\bar{x}}>-1\frac{\sigma}{\sqrt{N}}z_{\alpha/2}$$
 $$\bar{x}-1\frac{\sigma}{\sqrt{N}}z_{-\alpha/2}>\mu>\bar{x}-1\frac{\sigma}{\sqrt{N}}z_{\alpha/2}$$

y sabiendo que es simétrica $-z_{\alpha/2}=z_{-\alpha/2}$:

 $$\bar{x}-z_{\alpha/2}\frac{\sigma}{\sqrt{N}} < \mu <
   \bar{x}+z_{\alpha/2}\frac{\sigma}{\sqrt{N}}\,.$$

 Supongamos que queremos encontrar el intervalo de confianza de $\mu$ al $95\%$
 de confianza, es decir, entonces $\alpha=0.05$. Entonces $z_{\alpha/2}=1.96$
 (de tablas estadísticas).
 \\
 \\
 {\textbf Ejemplo}: $N=40$, $\sigma=0.5^\circ{C}$, y $\bar{x}=12.7^\circ{C}$:
 $$\bar{x}-z_{0.025}\frac{\sigma}{\sqrt{N}}<\mu<\bar{x}+z_{0.025}\frac{\sigma}{\sqrt{N}}\,,$$
 $$\left[12.7-(1.96)0.5/\sqrt{40}\right]\,^\circ{C}<\mu<\left[12.7+(1.96)0.5/\sqrt{40}\right]\,^\circ{C}$$
 $$12.54^\circ{C} < \mu < 12.85^\circ{C}$$
\\
\noindent (2) {\textbf Intervalo de confianza para $\mu$ ($N<30$, $\sigma$ desconocida)}
\\
 Cuando $N<30$ y $\sigma$ es desconocida, podemos usar el estadístico $t$-student
 para encontrar el intervalo de confianza para $\mu$. Hay un $100*(1-\alpha)\%$
que cualquier estadístico $t$ caiga en el intervalo

 $$t_{-\alpha/2}<\frac{\bar{x}-\mu}{s}\sqrt{N-1}<t_{\alpha/2}\,,$$

 $$\bar{x}-t_{\alpha/2}\frac{s}{\sqrt{N-1}} < \mu <
   \bar{x}+t_{\alpha/2}\frac{s}{\sqrt{N-1}}\,.$$

Si $\alpha=0.05$, hay un 95\% de probabilidad que cualquier estadístico $t$ caiga en el
intervalo
$$t_{-0.025}<\frac{\bar{x}-\mu}{s}\sqrt{N-1}<t_{0.025}\,,$$
de lo cual podemos deducir que la verdadera media $\mu$ es
de esperar con un 95\% de confianza que caiga en el intervalo:
$$\bar{x}-t_{0.025}\frac{s}{\sqrt{N-1}}<\mu<\bar{x}+t_{0.025}\frac{s}{\sqrt{N-1}}\,.$$

De forma general, podemos definir el intervalo de confianza como:
$$\mu=\bar{x}\pm t_c\frac{\hat{s}}{\sqrt{N}}\,,$$
donde $t_c$ es el valor crítico del estadístico $t$ (límites del
intervalo), el cual depende del número de grados de libertad y del nivel
de confiabilidad deseado. El intervalo de confianza con el estadístico $z$,
el cual solo es apropiado para tamaños muestrales grandes ($N>30$)
donde la desviación estandar es conocida:
$$\mu=\bar{x}\pm z_c\frac{\sigma}{\sqrt{N}}\,.$$
Observamos que la teoría para tamaños muestrales pequeños reemplaza
el estadístico $z$ por el $t$ y utiliza una desviación estandar
muestral modificada
$$\hat{s}=\sqrt{\frac{N}{N-1}s}\,.$$
%\vspace{0.5cm}
% {\noindent \textbf Diferencias entre medias}\\
%Supongamos dos muestras de tamaño $N_1$ y $N_2$ extraidas de una población
%con distribución normal con desviaciones estandar siguales. Supongamos
%que las medias muestrales son $\bar{x_1}$ y $\bar{x_2}$ y las
%desviaciones estandar muestrales son $s_1$ y $s_2$. Para comprobar
%la hipótesis nula ($H_0$) que ambas muestras provienen de la misma población,
%es decir $\mu_1=\mu_2$ y $\sigma_1=\sigma_2$ podemos usar la
%siguiente expresión
%$$t=\frac{(\bar{x_1}-\bar{x_2})-(\mu_1-\mu_2)}{\sigma\sqrt{\frac{1}{N_1} + \frac{1}{N_2}}};
%    \sigma=\sqrt{\frac{N_1 s_1^2 + N_2 s_2^2}{N_1+N_2-2}}\,,$$
%    donde $\nu=N_1+N_2-2$.
\\
\\
(3) \noindent{\textbf Intervalo de confianza para la diferencia de medias $\mu_1 - \mu_2$}
\\
\\
El teorema central del límite (TCL) para la diferencia de medias muestrales de
dos poblaciones viene dado por

 $$\bar{x}_1-\bar{x}_2\,\sim\,{\rm N}(\mu_{\bar{x}_1-\bar{x}_2},\sigma_{\bar{x}_1-\bar{x}_2})\,,$$
\\
\\
donde
$${\rm Media:}\,\,\mu_{\bar{x}_1-\bar{x}_2}=\mu_{\bar{x}_1}-\mu{\bar{x}_2}=\mu_1-\mu_2$$
$$\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,{\rm Varianza:}\,\,\sigma^2_{\bar{x}_1-\bar{x}_2}=
   \sigma^2{\bar{x}_1}+\sigma^2{\bar{x}_2}=
   \sqrt{  \frac{{\sigma_1}^2}{N_1} + \frac{{\sigma_2}^2}{N_2} }$$
\\
   {\textbf 3.1) Desviaciones estandar poblacionales ($\sigma_1$ y $\sigma_2$)
                  conocidas; $\mu_1$ y $\mu_2$ desconocidas} (estadístico z):

$$z=\frac{\bar{x}_1-\bar{x}_2-(\mu_1-\mu_2)}
         {\sqrt{\frac{{\sigma_1}^2}{N_1}+\frac{{\sigma_2}^2}{N_2}}}$$
\\
\\
   {\textbf 3.2) Desviaciones estandar poblacionales ($\sigma_1$ y $\sigma_2$)
                  y medias poblacionales ($\mu_1$ y $\mu_2$) desconocidas} (estadístico t):

$$t=\frac{\bar{x}_1-\bar{x}_2-(\mu_1-\mu_2)}
              {\sqrt{\frac{{s_1}^2}{N_1}+\frac{{s_2}^2}{N_2}}}$$
\\
\\
   {\textbf 3.3) Desviaciones estandar poblacionales ($\sigma_1$ y $\sigma_2$)
                  desconocidas pero iguales} (estadístico t):
\\
\\
Supongamos dos muestras de tamaño $N_1$ y $N_2$ extraídas
de dos poblaciones Normales con desviaciones estándar iguales
($\sigma_1=\sigma_2$).  Supongamos que conocemos las medias
y desviaciones estándar muestrales $\bar{x}_1$ y $\bar{x}_2$ y
$s_1$ y $s_2$. Para comprobar la hipótesis nula $H_o$ que
las muestran proceden de la misma población ($\mu_1=\mu_2$
y $\sigma_1=\sigma_2$) usamos el estadístico t-score (o pooled t):

$$t=\frac{\bar{x}_1-\bar{x}_2-(\mu_1-\mu_2)}
         {\hat{s}_d\sqrt{\frac{1}{N_1}+\frac{1}{N_2}}}$$
\\
$$\hat{s}_d=\sqrt{\frac{(N_1-1)s^2_1 + (N_2-1)s^2_2}{N_1+N_2-2}}\,,$$
\\
donde $\nu=N_1+N_2-2$ es el número de grados de libertad.
\\

\noindent \textbf {Ejemplo:} 
Un ingeniero que diseña instrumentos oceanográficos está ineteresado en aumentar el tiempo durante el cuál la pintura "antifouling" evita que los microorganismos se peguen y crezcan sobre el instrumento oceanográfico. Se prueban dos fórmulas de pintura:
fórmula 1 estándar y fórmula 2 con un nuevo ingrediente que
aumenta el tiempo de acción. 

De la experiencia se sabe que la desviación
estándar del tiempo de acción de la pintura es de 8 días y ésta
variabilidad no se vé afectada por el nuevo ingrediente. Se pintan 35
instrumentos con la fórmula 1 y otros 35 con la fórmula 2. Los tiempos
promedios de acción del "antifouling" son de 116 días para la fórmula 1
y 112 días para la fórmula 2. ¿A qué conclusión puede llegar el ingeniero
diseñador del instrumento sobre la eficacia del nuevo ingrediente, con un
nivel de significancia de 0.01?\\

$x_1\equiv$ Tiempo de acción "antifouling" fórmula 1\\
$x_2\equiv$ Tiempo de acción "antifouling" fórmula 2\\
$x_1\sim$ Desconocida ($\mu_1,\sigma_1=$ 8 días)\\
$x_2\sim$ Desconocida ($\mu_2,\sigma_2=$ 8 días)\\
$\bar{x}_1-\bar{x}_2\,\sim\,{\rm N}(\mu_1-\mu_2,\sigma_1/\sqrt{N_1} + \sigma_2/\sqrt{N_2})$ (TCL)\\
$\bar{x}_1=116$ días\\
$\bar{x}_2=112$ días\\
$N_1=N_2=35$\\
$\alpha=0.01$\\
$H_0:\,\mu_1 - \mu_2 =0$\\
$H_1:\,\mu_1 - \mu_2 \neq0$\\

El intervalo de confianza de la diferencia de medias $\mu_1-\mu_2$

$$\bar{x}_1-\bar{x}_2-z_{\alpha/2} \left(
   \frac{\sigma_1}{\sqrt{N_1}} + \frac{\sigma_2}{\sqrt{N_2}}\right)<
   \mu_1-\mu_2 <
   \bar{x}_1-\bar{x}_2+z_{\alpha/2} \left(
   \frac{\sigma_1}{\sqrt{N_1}} + \frac{\sigma_2}{\sqrt{N_2}}\right)$$

$$\bar{x}_1-\bar{x}_2-2.33 (1.9124)<
   \mu_1-\mu_2 <
   \bar{x}_1-\bar{x}_2+2.33 (1.9124)$$

$$\bar{x}_1-\bar{x}_2-4.4559<
   \mu_1-\mu_2 <
   \bar{x}_1-\bar{x}_2+4.4559$$

$$4-4.4559<
   \mu_1-\mu_2 <
   4+4.4559\,,$$
\\

y el intervalo de confianza es:
\\
$$-0.4559<\mu_1-\mu_2<8.4559\,\,{\rm al}\,\,99\%\,(\alpha=0.01)$$

$$H_0:\,\mu_1 - \mu_2 = 0\,\,{\rm al}\,\,99\%\,,$$

y aceptamos hipótesis nula $H_0$.\\

(4)\textbf {Intervalo de confianza para la varianza:

Distribución {\it chi-cuadrada}}\\

En ocasiones queremos definir un intervalo de confianza para la varianza muestral.
Para ello podemos usar el estadístico {\it chi-cuadrado}. Definamos

$$\chi^2=(N-1)\frac{s^2}{\sigma^2}\,.$$
\\
\textbf {Propiedades:}

\begin{itemize}
\item No es simétrica.
\item La forma de la distribución depende de los grados de libertad.
\item A medida que los grados de libertad aumentan ($N$ aumenta),
la distribución se parece más a la Normal.
\item $\chi^2\geq0$
\end{itemize}

\vspace{0.25cm}
Para definir el intervalo de confianza sabemos que
hay un $100*(1-\alpha)\%$ que cualquier estadístico
$\chi^2$ caiga en el intervalo

  $$\chi^2_{1-\alpha/2}<(N-1)\frac{s^2}{\sigma^2}<\chi^2_{\alpha/2}\,,$$
  $$\frac{1}{\chi^2_{1-\alpha/2}}>\frac{\sigma^2}{(N-1)s^2}>\frac{1}{\chi^2_{\alpha/2}}\,,$$

y entonces:

  $$\frac{(N-1)s^2}{\chi^2_{\alpha/2}}<\sigma^2<\frac{(N-1)s^2}{\chi^2_{1-\alpha/2}}\,.$$

\begin{center}
%\centering
%\includegraphics[width=10cm,angle=0]{chi2.jpg}
\end{center}

Usamos $1-{\alpha/2}$ porque la $\chi^2$ es positiva. El valor $\chi^2_{\alpha/2}$ es
mayor que el valor $\chi^2_{1-\alpha/2}$. Las tablas dan la probabilidad
a la derecha del valor.
\\
\\
Para una población normalmente distribuida con desviación estandar $\sigma$,
la función de densidad de probabilidad de la {\it chi-cuadrada} es:
$$F_x(\chi)=f_0\chi^{\nu-2}e^{-\frac{1}{2}\chi^2};\,\,\nu=N-1\,.$$

Puesto que la distribución {\it chi} es asimétrica y positiva, si $\alpha=0.05$
(95\% confianza), el intervalo de confianza para la varianza $\sigma^2$ como
$$\frac{(N-1)s^2}{\chi^2_{0.025}}<\sigma^2<\frac{(N-1)s^2}{\chi^2_{0.975}}\,,$$

y si leemos en las tablas para $\nu=9$ grados de libertad:

  $$\frac{(N-1)s^2}{19.023}<\sigma^2<\frac{(N-1)s^2}{2.700}\,,$$

{\textbf Ejemplo}: Supongamos que tenemos $\nu=9$ grados de libertad de nuestra
estimación espectral de la componente meridional de la velocidad de la
corriente. Sabemos que la varianza muestral de un pico espectral es
$s^2=10\,{\rm cm}\,{\rm s}^2$ ?`Cuál es el intervalo de confianza al
95\% para la varianza?
\\

De las tablas estadísticas vemos que para $\nu=N-1=9$ grados de libertad,
$\chi^2_{1-\alpha/2}=\chi^2_{0.095}=19.02$ y
$\chi^2_{\alpha/2}=\chi^2_{0.025}=2.70$. Entonces, el
intervalo es:

$$\frac{(9)10}{19.023}<\sigma^2<\frac{(9)10}{2.700}$$
$$4.7\,{\rm cm}^2\,{\rm s}^{-2}<\sigma^2<33.3\,{\rm cm}^2\,{\rm s}^{-2}$$

\vspace{0.5cm}
{\noindent \textbf Grados de libertad}\\
El número de grados de libertad es el número de muestras independientes
N menos el número de parámetros del estadístico que queremos estimar. Por
ejemplo en el estadístico $t$
$$t=\frac{\bar{x}-\mu}{\frac{s}{\sqrt{N-1}}}=\frac{\bar{x}-\mu}{\frac{\hat{s}}{\sqrt{N}}};
\hat{s}=\sqrt{\frac{N}{N-1}s}\,,$$
calculamos la media muestral y la desviación estandar $s$ a partir de
los datos, pero la verdadera media $\mu$ debe ser estimada, por lo que
$\nu=N-1$. Similarmente en el estadístico {\it chi-cuadrada}
$$\chi^2=(N-1)\frac{s^2}{\sigma^2}\,,$$
conocemos la varianza muestral $s^2$ y el tamaño muestral $N$, pero debemos
estimar la verdadera varianza, y entonces $\nu=N-1$.

\vspace{0.5cm}
 {\noindent \textbf Estadístico $F$}\\
 Otro estadístico útil para tests espectrales es el estadístico $F$. Si $s^2_1$ y
 $s^2_2$ son las varianzas de muestras aleatorias independientes de tamaño $N_1$ y $N_2$,
 tomadas de dos poblaciones Normales con la misma varianza $\sigma^2$, entonces
 $$F=\frac{s^2_1}{s^2_2}\,,$$
 es el valor de una variable aleatria cuya distribución es $F$ con los
 parámetros $\nu_1=N_1-1$ y $\nu_2=N_2-1$. Este estadístico es muy útil en tests
 de significancia para los picos de los espectros frecuenciales de potencia.
 Los dos parámetros son los grados de libertad para las varianzas del
 cociente; $\nu_1$ para $s^2_1$ y $\nu_2$ para $s^2_2$.

\vspace{0.5cm}
 {\noindent \textbf Tests para hipótesis}\\
Para usar los test de significancia estadística debemos de seguir 5 pasos:
\\
(1) Definir el nivel de confianza
\\
(2) Definir la hipótesis nulla $H_0$ y su alternativa $H_1$
\\
(3) Definir el estadístico que usaremos
\\
(4) Definir la región crítica
\\
(5) Evaluar el estadístico y concluir
\\
\\
Es muy importante definir correctamente la hipótesis nula. Es decir,
estar seguro que rechazar la hipótesis nula $H_0$ implica únicamente
la existencia de su alternativa $H_1$. Normalmente la hipótesis nula
y su alternativa son mutualmente excluyentes. Ejemplos:
\\
\\
$H_0$: Las medias de dos muestras son iguales
$H_1$: Las medias de dos muestras no son iguales
\\
\\
$H_0$: El coeficiente de correlación es cero
$H_1$: El coeficiente de correlación no es cero
\\
\\
\vspace{0.5cm}
\noindent{\textbf Ejemplo:}\\
En una muestra de 41 inviernos la temperatura media de Enero es $5.55^\circ{C}$ y
la desviación es de $0.65^\circ{C}$ ?`Cual es el intervalo de confianza al 95\%
de que la verdadera temperatura media sea esa?
\\
(1) Nivel de confianza del 95\%
\\
(2) $H_0$ es que la media verdadera se encuentra en el intervalo
$5.55\pm \Delta{T}$ y su alternativa $H_1$ es que se encuentra fuera
de este intervalo.
\\
(3) Usamos el estadístico $t$.
\\
(4) La región crítica es $|t|<t_{0.025}$, lo cual para $\nu=N-1=40$ es
$|t|<2.26$ (leido de tablas estadísticas). Escrito en términos de
intervalo de confianza para la media poblacional:
$$\bar{x}-2.0211\frac{s}{\sqrt{N-1}}<\mu<\bar{x}+2.0211\frac{s}{\sqrt{N-1}}  $$
\\
(5) Si ponemos en números el intervalo obtenemos $5.06<\mu<6.03$.
Tenemos un 95\% de confianza que la verdadera temperatura media
se encuentra en ese intervalo.

\vspace{0.5cm}
{\noindent \textbf Teorema de Bayes:}\\
Sea $E_i\,,i=1,2,3,...,n$ un conjunto de $n$ eventos que constituyen una
partición del espacio muestral $S$

$$\bigcup^n_{i=1}E_i\in S,$$

{\noindent}cada uno de los cuales tiene
probabilidad positiva de ocurrir $P(Ei)>0$ para $i=1,2,....,n$ y son exclusivos entre si
$$E_i\cap E_j=\o \,\,\,\,i\ne j\,.$$
Entonces dada la ocurrencia previa de un evento cualquiera $B$, la probabilidad de que suceda el
evento $E_j$ es

\begin{equation}
P(E_j|B)=\frac{P(B|E_j)P(E_j)}{\sum^n_{i=1}P(B|E_i)P(E_i)}\,,
\end{equation}

{\noindent}donde $$P(E_j|E_i)=\frac{P(E_i\cap E_j)}{P(E_i)}\,,$$
es la probabilidad condicional, es deicr, la probabilidad que ocurra el evento $E_j$
si previamente ha ocurrido el evento $E_i$ y $P(E_i \cap E_j)$ es la probabilidad
que ambos eventos ocurran, i.e. la intersección de dos eventos. La intersección
es puede escribir
$$P(E_i\cap E_j)=P(E_j|E_i)*P(E_i)=P(E_i|E_j)*P(E_j)\,.$$
Si ambos eventos son independientes (no interseccionan) tal que
$P(E_i|E_j)=P(E_i)$ obtenemos
$$P(E_i\cap E_j)=P(E_i)*P(E_j)\,,$$
es decir, la definición de independencia estadística entre eventos.
\\
\\
{\textbf Ejemplo teorema de Bayes:} Imaginemos que queremos saber si una muestra de agua
contiene diatomeas o no. La probabilidad de que una muestra de agua tomada al azar en la
bahía de Todos Santos tenga diatomeas es de $1/100$ ($P(D)=0.01$). La probabilidad de
que si hay diatomeas el test de negativo es cero ($P(+|D)=1$) y la probabilidad de que
el test de un falso positivo es del $5\%$ ($P(+|noD)=0.05$). Si agarramos una muestra
de agua y da positivo, ?`Cual es la probabilidad de que hayan diatomeas? Intuitivamente,
sabemos que existe un $5\%$ de probabilidad de que el test me de un falso positivo y por
lo tanto, un $95\%$ de que si da positivo tenga diatomeas en la muestra
de agua. Veamos que nos dice el teorema de Bayes. Sabemos que $P(D)=0.01$,
$P(+|D)=1$, y $P(+|noD)=0.05$. Si usamos el teorema
de Bayes:
$$ P(D|+)=\frac{P(+|D)P(D)}{P(+|D)P(D) + P(+|noD)P(noD)}=
          \frac{1*0.01}{1*0.01+0.05*0.99}\sim\frac{1}{6}\,.$$

En verdad solamente existe $1$ probabilidad de $6$ ($\sim16\%$) que si el test es positivo
existan diatomeas. De cada $100$ muestras solamente $1$ tiene diatomeas y
$5\%*100=0.05*100=5$ muestras de las $100$ dan positivo y no tienen diatomeas.
Por ello, sabiendo que la muestra que tiene diatomeas dio positivo,
de $6$ positivos, solamente $1$ es cierto.

\includepdf[pages={18,19,20}]{review_statistics_hartmann_1.pdf}
\includepdf[pages={25,26,27}]{review_statistics_hartmann_1.pdf}

## Repaso de álgebra Lineal

Una matriz es un elemento matemático compuesto por filas y columnas.
Una matriz rectangular de dimensiones $m\times n$ se define como
$${\textbf A}=\left(\begin{array}{cccc}
  a_{11} & a_{12} & .... & a_{1n}\\
  a_{21} & a_{22} & .... & a_{2n}\\
    .    &   .    &      &   . \\
    .    &   .    &      &   . \\
    .    &   .    &      &   . \\
  a_{m1} & a_{m2} & .... & a_{mn}\\
\end{array}
  \right)
\,.$$

Un elemento de la matriz ${\textbf A}$ queda definida como
${\textbf A}=[a_{ij}],\,\,\,i=1,2,...,m; j=1,2,...,n\,.$
El primer índice $i$ denota filas y el segundo $j$
columnas.

\vspace{0.5cm}
{\noindent \textbf Definición} Un vector es una matriz con solo una columna ($m\times1$)
$${\textbf v}=\left( \begin{array}{c}
 v_1 \\ v_2 \\ . \\ . \\ . \\ v_m
       \end{array} \right)
       \,.$$

\vspace{0.5cm}
{\noindent \textbf Vectores ortogonales}

Sean dos vectores ${\textbf u}=[u_1,u_2,...,u_N]$ y ${\textbf v}=[v_1,v_2,...,v_N]$ de
longitud $N$, decimos que son ortogonales si
$${\textbf u}\cdot{\textbf v}=({\textbf u},{\textbf v})=\sum\limits^N_{i=1} u_i v_i={\textbf u}^T{\textbf v}=0\,.$$


 \vspace{0.5cm}
{\noindent \textbf Norma, módulo, longitud de un vector}

Sea el vector ${\textbf u}=[u_1,u_2,...,u_N]$, entonces la norma de dicho vector
se define como
$$||{\textbf u}||=|{\textbf u}|=\sqrt{u^2_1 + u^2_2 + ... + u^2_N}=({\textbf u},{\textbf u})^{1/2}=({\textbf u}^T{\textbf u})^{1/2}\,.$$

En general se puede obtener un {\textbf vector unitario} (de módulo $=1$)
dividiendo el vector por su norma:
$${\textbf u}_I=\frac{{\textbf u}}{||{\textbf u}||}\,.$$

\vspace{0.5cm}
{\noindent \textbf Vectores ortonormales}

Dos vectores ortogonales ${\textbf u}$ y ${\textbf v}$ si tienen módulo la unidad,
entonces se denominan vectores ortonormales.

 \vspace{0.5cm}
{\noindent \textbf Suma de vectores}

La suma de 2 vectores ${\textbf u}=[u_1,u_2,...,u_N]$ y ${\textbf v}=[v_1,v_2,...,v_N]$ de
longitud $N$ se define como
$${\textbf u} + {\textbf v}= [u_1 + v_1, u_2 + v_2,..., u_N + v_N]=\sum\limits^N_{i=1} u_i + v_i\,.$$


 \vspace{0.5cm}
{\noindent \textbf Combinación lineal de vectores}

Un vector ${\textbf y}$ se dice que es combinación lineal de
un conjunto de vectores ${\textbf x}_1, {\textbf x}_2, ...,{\textbf x}_N$ si se puede
expresar como la suma de los $N$ vectores multiplicados
por $N$ coeficientes escalares $a_1,a_2,...,a_N$:

$${\textbf y}=a_1 {\textbf x}_1 + a_2 {\textbf x}_2 +...+a_N {\textbf x}_N=\sum\limits^N_{i=1}a_i {\textbf x}_i\,.$$

 \vspace{0.5cm}
{\noindent \textbf Independencia lineal}

Un conjunto de vectores ${\textbf y}_1, {\textbf y}_2, ...,{\textbf y}_N$ se dice que es linealmente
independiente si existe una combinación
lineal finita de los vectores del conjunto tal que:

$$\sum\limits^N_{i=1} a_i {\textbf y}_i=a_1{\textbf y}_1 + a_2{\textbf y}_2+...+a_N{\textbf y}_N=0\,,$$

que se satisface cuando no todos los coeficientes son cero. En caso contarrio,
se dice que son linealmente dependientes.

\vspace{0.5cm}
{\noindent \textbf Ortonormalización Gram-Schmidt }

Es un método para convertir un conjunto de vectores ${\textbf v}$ en
vectores ortonormales. De forma general el proceso definido
por Gram-Schmidt para ortonormalizar el vector ortogonal ${\textbf w}_k$ a partir
de un conjunto de vectores ortonormales ${\textbf u}_i=[{\textbf u}_1, {\textbf u}_2,...,{\textbf u}_{k-1}]$
se define como:

$${\textbf w}_k={\textbf v}_k-\sum\limits^{k-1}_{i=1} {\textbf u}_i\cdot{\textbf v}_k{\textbf u}_i\,.$$

\vspace{0.25cm}

$${\textbf v}_k\equiv\,\,{\rm vectores}\,\,{\rm originales}$$
$${\textbf u}_i\equiv\,\,{\rm vectores}\,\,{\rm ortonormales}$$
$${\textbf w}_k\equiv\,\,{\rm vectores}\,\,{\rm ortogonales}\,\,{\rm a}\,\,{\textbf u}_j$$

\vspace{0.25cm}

\noindent{\textbf Ejemplo}:
\\
Convertir el conjunto de vectores de la base $A$ en una base ortonormal:

 $${\textbf A}=\left(\begin{array}{cccc}
  1 & 2 & 1\\
  0 & 2 & 0\\
  2 & 3 & 1\\
  1 & 1 & 0\\
\end{array}
  \right)
\,.$$

Primero normalizamos el primer vector columna ${\textbf v}_1$:
$${\textbf u}_1=\frac{{\textbf v}_1}{||{\textbf v}_1||}=\left[\frac{1}{\sqrt{6}},0,\frac{2}{\sqrt{6}},\frac{1}{\sqrt{6}}\right]\,.$$
\\
(1er vector ortonormal)
\\
Ahora usamos la fórmula de arriba para encontrar el vector ${\textbf w}_2$
ortogonal a ${\textbf u}_1$
$${\textbf w}_2={\textbf v}_2- {\textbf u}_1\cdot{\textbf v}_2{\textbf u}_1=[2,2,3,1]-
   \left[\frac{1}{\sqrt{6}},0,\frac{2}{\sqrt{6}},\frac{1}{\sqrt{6}}\right]\cdot
   [2,2,3,1]\left[\frac{1}{\sqrt{6}},0,\frac{2}{\sqrt{6}},\frac{1}{\sqrt{6}}\right]=$$
$$=[2,2,3,1]-\left(\frac{9}{\sqrt{6}}\right)\left[\frac{1}{\sqrt{6}},0,\frac{2}{\sqrt{6}},\frac{1}{\sqrt{6}}\right]=
   [2,2,3,1]-\left[\frac{3}{2},0,3,\frac{3}{2}\right]=\left[ \frac{1}{2},2,0,\frac{-1}{2}\right]\,.$$
(vector ortogonal a $u_1$)

Normalizamos ${\textbf w}_2$ para obtener el primer vector ortonormal a ${\textbf u}_1$
$${\textbf u}_2=\frac{{\textbf w}_2}{||{\textbf w}_2||}=\left[ \frac{\sqrt{2}}{6},\frac{2\sqrt{2}}{3},0,\frac{-\sqrt{2}}{6}\right]$$
\\
\\
(2o vector ortonormal)
\\
Ahora calculamos ${\textbf w}_3$ en términos de ${\textbf u}_1$ y ${\textbf u}_2$
$${\textbf w}_3=  {\textbf v}_3- {\textbf u}_1\cdot{\textbf v}_3{\textbf u}_1 - {\textbf u}_2\cdot{\textbf v}_3{\textbf u}_2=
  \left[ \frac{4}{9},\frac{-2}{9},0,\frac{-4}{9}\right]\,,$$
(vector ortogonal a $u_1$ y $u_2$)
\\
\\
y si normalizamos
\\
\\
$${\textbf u}_3=\left[ \frac{2}{3},\frac{-1}{3},0,\frac{-2}{3}\right]\,.$$
\\
(3er vector ortonormal)
\\
La matriz o conjunto de vectores ortonormal es
$${\textbf A}=\left(\begin{array}{cccc}
  \frac{\sqrt{6}}{6} & \frac{\sqrt{2}}{6} & \frac{2}{3}\\
  0 & \frac{2\sqrt{2}}{3} & \frac{-1}{3}\\
  \frac{\sqrt{6}}{3} & 0 & 0\\
  \frac{\sqrt{6}}{6} & \frac{-\sqrt{2}}{6} & \frac{-2}{3}\\
\end{array}
  \right)
\,$$
(base ortonormal)


 \vspace{0.5cm}
{\noindent \textbf Aplicación lineal}
Sean dos espacios vectoriales $V$ y $W$, decimos que una aplicación $f:V \rightarrow W$
es lineal si la `imagen' de la combincación lineal es la combinación lineal
de las imágenes. Es decir,
$$f(\alpha {\textbf u} + \beta {\textbf v} )=\alpha f({\textbf u}) + \beta f({\textbf v})\,.$$

La imagen de una aplicación es el resultado de aplicar al vector una aplicación.

Ejemplos:

 \vspace{0.5cm}
(1) La aplicación $f$: ${\cal R}^3 \rightarrow {\cal R}^2$ definida por $f(x,y,z)=(x+y,y+2z)$ es lineal.

Demostración: Definimos ${\textbf u}=[u_1,u_2,u_3]$ y ${\textbf v}=[v_1,v_2,v_3]$. Entonces $f$ es lineal si
$$f(\alpha (u_1,u_2,u_3) + \beta(v_1,v_2,v_3))=\alpha f(u_1,u_2,u_3) + \beta f(v_1,v_2,v_3)=$$
$$f(\alpha u_1 + \beta v_1,\alpha u_2 + \beta v_2,\alpha u_3 + \beta v_3 )=\alpha [u_1 + u_2,u_2 + 2u_3] +
\beta [v_1 + v_2,v_2 + 2v_3]$$
$$[\alpha u_1 + \beta v_1 + \alpha u_2 + \beta v_2,\alpha u_2 + \beta v_2 + 2 (\alpha u_3 + \beta v_3)]=
[\alpha u_1 + \alpha u_2 + \beta v_1 + \beta v_2, \alpha u_2 + 2\alpha u_3 + \beta v_2 + 2\beta v_3]$$

 \vspace{0.5cm}
(2) La aplicación $f$: ${\cal R}^3 \rightarrow {\cal R}^2$ definida por $f(x,y,z)=(x+y+1,y+2z)$ no es lineal.

\vspace{0.5cm}
(3) La aplicación ${\textbf R}$: ${\cal R}^2 \rightarrow {\cal R}^2$ definida por
$${\textbf x}'={\textbf R}{\textbf x}\,,$$
$${\textbf R}=\left( \begin{array}{cc}
 cos(\theta) & -sin(\theta)\\
 sin(\theta) & cos(\theta)
       \end{array} \right)\,,$$
rota el vector un ángulo $\theta$ en sentido contrario a las agujas del
reloj. Si queremos cambiar el sentido de la rotación
solo debemos tomar el signo de $\theta$ negativo. Aqui ${\textbf x}'$ es el vector
rotado o la imagen de la aplicación rotación.

\vspace{0.5cm}
{\noindent \textbf Matriz Identidad}

Se define la matriz identidad ${\textbf I}$ como una matriz diagonal
compuesta por unos

$${\textbf I}=\left(\begin{array}{ccc}
  1 & 0 & 0\\
  0 & 1 & 0\\
  0 & 0 & 1
\end{array}\right)\,.$$

El producto matricial de cualquier matriz ${\textbf A}$ por la matriz identidad
${\textbf I}$ es igual a la matriz original
$${\textbf A}{\textbf I}={\textbf A}\,.$$

\vspace{0.5cm}
{\noindent \textbf Matriz transpuesta}

Sea ${\textbf A}$ una matriz $m\times n$ con elementos $[a_{ij}]\,\,{\rm para}\,\,i=1,2,...,m; j=1,2,...,n$.
Se define el elemento de su transpuesta como
$${\textbf A}^T=[a_{ji}]\,.$$

La inversa de la matriz transpuesta es
$$({\textbf A}^T)^{-1}={\textbf A}\,,$$
si ${\textbf A}$ es ortogonal.
\\
{\textbf Demostración}:
$$({\textbf A}^T)^{-1}={\textbf A}$$
$${\textbf A}^T({\textbf A}^T)^{-1}={\textbf A}^T{\textbf A}={\textbf I}$$
$${\textbf A}{\textbf A}^T({\textbf A}^T)^{-1}={\textbf A}{\textbf I}$$

\vspace{0.5cm}
{\noindent \textbf Matriz Ortogonal}

Una matriz ${\textbf A}$ es ortogonal si ${\textbf A}{\textbf A}^T={\textbf A}^T{\textbf A}={\textbf I}\,.$

\vspace{0.5cm}
{\noindent \textbf Matriz Diagonal}

Una matriz es diagonal si únicamente contiene
algunos elementos diferentes de cero en la diagonal
principal (ceros en la matriz triangular superior
e inferior).

$${\textbf D}=\left(\begin{array}{ccc}
  a_{11} & 0 & 0\\
  0 & a_{22} & 0\\
  0 & 0 & a_{33}
\end{array}\right)
$$

\vspace{0.5cm}
{\noindent \textbf Matriz Simétrica}

Una matriz simétrica se define como aquella matriz ${\textbf A}$
tal que sea igual a su traspuesta
$${\textbf A}={\textbf A}^T\,\,(a_{ij}=a_{ji})$$

$${\textbf A}=\left(\begin{array}{cccc}
  a_{11} & a_{12} & a_{31}\\
  a_{12} & a_{22} & a_{32}\\
  a_{31} & a_{32} & a_{33}\\
\end{array}\right)
$$

\vspace{0.5cm}
{\noindent \textbf Matriz Antisimétrica}

Una matriz antisimétrica se define como aquella matriz ${\textbf A}$
tal que sea igual a su traspuesta
$${\textbf A}=-{\textbf A}^T\,\,(a_{ij}=-a_{ji})$$

$${\textbf A}=\left(\begin{array}{cccc}
  0 & a_{12} & a_{13}\\
  -a_{12} & 0 & a_{23}\\
  -a_{13} & -a_{23} & 0\\
\end{array}\right)
$$
\\
NOTA: Cualquier matriz ${\textbf A}$ se puede descomponer en una parte
simétrica y otra antisimétrica:

$$A=\frac{1}{2}({\textbf A}+{\textbf A}^T) + \frac{1}{2}({\textbf A}-{\textbf A}^T)\,.$$

\vspace{0.5cm}
{\noindent \textbf Matriz Singular}

Una matriz singular es aquella matriz cuadrada cuyo determinante
es igual a cero. Las matrices singulares no tienen matriz
inversa.
\\
\\
Ejemplo:

$${\textbf S}=\left(\begin{array}{cc}
  3 & 2\\
  6 & 4
\end{array}\right)
$$

Si nos fijamos las 2 filas de la matriz singular ${\textbf S}$ son
linealmente dependientes, es decir, podemos recuperar la segunda
fila multiplicando la primera fila por 2. Si la matriz tiene columnas
o filas linealmente dependientes, el determinante es cero.

Si ${\textbf A}$ es ortogonal

$${\textbf A}^{-1}={\textbf A}^T$$
$${\textbf A}^{-1}({\textbf A}^T)^{-1}={\textbf I}$$
$${\textbf A}^{-1}\underbrace{({\textbf A}^T)^{-1}({\textbf A}^T)}_{{\textbf I}}={\textbf A}^T$$

Una matriz cualquiera se puede convertir en matriz cuadrada si
es multiplicada por su transpuesta.

\vspace{0.5cm}
{\noindent \textbf Determinante de una matriz}

\noindent El determinante de una matriz ${\textbf A}$ $3\times 3$ se puede
calcular como
$$|{\textbf A}|=\left|\begin{array}{cccc}
  a_{11} & a_{12} & a_{13}\\
  a_{21} & a_{22} & a_{23}\\
  a_{31} & a_{32} & a_{33}\\
\end{array}
  \right|=a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32}-(a_{31}a_{22}a_{13} + a_{32}a_{23}a_{11} +
          a_{33}a_{21}a_{12})\,.$$

Para matrices de orden superior se puede utilizar la formula de
adjuntos. El determinante de una matriz $n\times n$ es el
producto escalar entre cualquier fila o columna con sus
adjuntos
$$|{\textbf A}|=a_{i1}C_{i1} + a_{i2}C_{i2}+...+a_{in}C_{in}\,,$$
donde los adjuntos $C_{ij}$ son subdeterminantes (de orden $n-1$, sin contar
la columna j y fila i) con el signo adecuado
$$C_{ij}=(-1)^{i+j}|{\textbf M}_{ij}|\,.$$

\vspace{0.5cm}
{\noindent \textbf Rango de una matriz}
\\
\\
{\textbf Definición 1}: El rango de una matriz se define como el número de filas o columnas de la
matriz que son linealmente independientes.
\\
\\
{\textbf Definición 2}: El orden de la mayor submatriz cuadrada no nula

Así para calcular el rango debemos de hacer cero todos los elementos posibles
de la matriz hasta obtener una submatriz no nula que nos indicará el rango de
la matriz.

$$\left[\begin{array}{cccc}
  1 & 2 & 1\\
  -2 & -3 & 1\\
  3 & 5 & 0\\
\end{array}\right]\underbrace{\rightarrow}_{2r_1+r_2}
\left[\begin{array}{cccc}
 1 & 2 & 1\\
  0 & 1 & 3\\
  3 & 5 & 0\\
\end{array}\right]\underbrace{\rightarrow}_{-3r_1+r_3}
\left[\begin{array}{cccc}
 1 & 2 & 1\\
  0 & 1 & 3\\
  0 & -1 & -3\\
\end{array}\right]\underbrace{\rightarrow}_{r_2+r_3}
\left[\begin{array}{cccc}
 1 & 2 & 1\\
  0 & 1 & 3\\
  0 & 0 & 0\\
\end{array}\right]\underbrace{\rightarrow}_{-2r_2+r_1}
\left[\begin{array}{cccc}
 1 & 0 & -5\\
  0 & 1 & 3\\
  0 & -1 & -3\\
\end{array}\right]\,,$$
y entonces el rango de la matriz es 2. El rango lo podíamos haber
encontrado simplimente viendo que sustrayendo la segunda fila de la
matriz a la primera obtenemos la tercera fila; solamente hay 2
vectores linealmente independientes.

\vspace{0.5cm}
{\noindent \textbf Matriz inversa}

\noindent La matriz cuadrada ${\textbf A}$ es invertible si existe una matriz ${\textbf A}^{-1}$ tal
que
$${\textbf A}^{-1}{\textbf A}={\textbf I}\,\,\,{\rm and}\,\,\,{\textbf A}{\textbf A}^{-1}={\textbf I}\,.$$

\vspace{0.5cm}
{\noindent \textbf Proposición} Si ${\textbf A}$ es invertible entonces la única
solución de ${\textbf A}{\textbf x}={\textbf b}$ es
$${\textbf x}={\textbf A}^{-1}{\textbf b}\,.$$

\vspace{0.5cm}
{\noindent \textbf Proposición} Si ${\textbf A}$ es una matriz cuadrada de dimensión
$2\times 2$ y $|{\textbf A}|=a_{11}a_{22} - a_{12}a_{21}$ no es cero, entonces
$${\textbf A}^{-1}=\left(\begin{array}{cc}
  a_{11} & a_{12} \\
  a_{21} & a_{22} \\
\end{array}
  \right)^{-1} =
  \frac{1}{|{\textbf A}|}
  \left(\begin{array}{cc}
  a_{22} & -a_{12} \\
  -a_{21} & a_{11} \\
\end{array}
  \right)
\,.$$

\vspace{0.5cm}
{\noindent \textbf Cálculo de matriz inversa con método Gauss-Jordan}

\noindent El método de Gauss-Jordan usa la siguiente identidad matemática
$[{\textbf A}|{\textbf I}]\rightarrow [{\textbf I}|{\textbf A}^{-1}]$.
\\
{\textbf Ejemplo:} Calcula la matriz inversa de
$$\left(\begin{array}{cc}
  2 & 3 \\
  4 & 7 \\
\end{array}
  \right)$$

$$[{\textbf A}  {\textbf I}]=
  \left(
        \begin{array}{ccccc}
  2 & 3 & & 1 & 0\\
  4 & 7 & & 0 & 1\\
        \end{array}
  \right)
  \underbrace{\rightarrow}_{-2r_1+r_2}
  \left(
        \begin{array}{ccccc}
  2 & 3 & & 1 & 0\\
  0 & 1 & & -2 & 1\\
        \end{array}
  \right)
  \underbrace{\rightarrow}_{-3r_2+r_1}
  \left(
        \begin{array}{ccccc}
  2 & 0 & & 7 & -3\\
  0 & 1 & & -2 & 1\\
        \end{array}
  \right)\rightarrow$$
  $$\underbrace{\rightarrow}_{r_1/2}
  \left(
        \begin{array}{ccccc}
  1 & 0 & & 7/2 & -3/2\\
  0 & 1 & & -2 & 1\\
        \end{array}
  \right)
  \,.
  $$

  Entonces

  $${\textbf A}^{-1}=\left(
        \begin{array}{ccccc}
  7/2 & -3/2\\
  -2 & 1\\
        \end{array}
  \right)\,.$$

\noindent {\textbf Ejemplo 2:}
$${\textbf A}=\left[\begin{array}{cccc}
  1 & -1 & 2\\
  2 & 0 & 3\\
  0 & 1 & -1\\
\end{array}\right]\rightarrow
{\textbf A}^{-1}=\left[\begin{array}{cccc}
  3 & -1 & 3\\
  -2 & 1 & -1\\
  -2 & 1 & -2\\
\end{array}\right]\,.$$

\vspace{0.5cm}
{\noindent \textbf Factorización LU}

\noindent Es la descomposición de una matriz en una matriz triangular inferior ${\textbf L}$
y una matriz triangular superior ${\textbf U}$, es decir
$${\textbf A}={\textbf L}{\textbf U}\,,$$
donde
$${\textbf L}=\left(\begin{array}{cccc}
  a_{12} & 0\\
  a_{21} & a_{22}\\
\end{array}\right)\,$$
es una matriz triangular inferior (L; lower) y
$$
{\textbf U}=\left(\begin{array}{cccc}
  a_{12} & a_{21}\\
  0 & a_{12}\\
\end{array}\right)\,,$$
es una matriz triangular superior
(U; superior).\\
   Esta factorización se suele usar para resolver un sistema
de ecuaciones lineal ${\textbf A}{\textbf x}={\textbf b}$ y no es única.
Es decir, pueden existir más de una factorización. Si sustituimos
la definición de arriba
$${\textbf A}{\textbf x}=({\textbf L}{\textbf U}){\textbf x}={\textbf b}\,,$$
lo que implica que
$${\textbf L}({\textbf U}{\textbf x})={\textbf b}\,.$$
\\
Si definimos ${\textbf U}{\textbf x}={\textbf z}$, entonces tenemos que
$${\textbf L}{\textbf z}={\textbf b}\,.$$
Como ${\textbf L}$ es una matriz triangular inferior, podemos resolver para ${\textbf z}$
utilizando sustitución hacia delante. Luego, como ${\textbf U}$ es una matriz
triangular superior, resolvemos ${\textbf U}{\textbf x}={\textbf z}$ por sustitución en reversa.

\vspace{0.5cm}
{\noindent \textbf Ejemplo:}

\noindent Encuentre la descomposición LU de la matriz
$$\left(\begin{array}{ccccc}
  2 & 5\\
 -3 & -4\\
        \end{array}\right)\,.
$$

\noindent Si multiplicamos la primera fila por $L_{21}=3/2$ y le sumamos la segunda fila hacemos
cero el elemento $a_{21}=-3$
$${\textbf U}=\left(\begin{array}{ccccc}
  2 & 5\\
 0 & 7/2\\
        \end{array}\right)\,.
$$
Esta matriz ya es una matriz triangular superior, es decir, la matriz ${\textbf U}$.
Para encontrar la matriz triangular inferior solo debemos de conocer
el valor del elemento $L_{21}$. Ese elemento es el multiplicador con signo
opuesto usado en la eliminación de Gauss-Jordan. Es decir, $L_{21}=-3/2$.
La matriz ${\textbf L}$ es
$${\textbf L}=\left(\begin{array}{ccccc}
  1 & 0\\
 -3/2 & 1\\
        \end{array}\right)\,.$$

\noindent Vamos a comprobar
$$\left(\begin{array}{ccccc}
  1 & 0\\
 -3/2 & 1\\
        \end{array}\right)
	\left(\begin{array}{ccccc}
  2 & 5\\
 0 & 7/2\\
        \end{array}\right)
	=
	\left(\begin{array}{ccccc}
  2 & 5\\
 -3 & -4\\
        \end{array}\right)\,.$$

\vspace{0.5cm}
{\noindent \textbf Ejemplo:}

\noindent Resuelva el siguiente sistema de ecuaciones con la
descomposición LU
$$2x_1+3x_2+4x_3=6$$
$$4x_1+5x_2+10x_3=16$$
$$4x_1+8x_2+2x_3=2$$

\noindent La matriz de coeficientes es
$${\textbf A}=\left(\begin{array}{ccccc}
  2 & 3 & 4\\
  4 & 5 & 10\\
  4 & 8 & 2 \\
        \end{array}\right)\,.
$$

\noindent Si factorizamos
$${\textbf L}=\left(\begin{array}{ccccc}
  1 & 0 & 0\\
  2 & 1 & 0\\
  2 & -2 & 1 \\
        \end{array}\right)\,,
$$

\noindent y

$${\textbf U}=\left(\begin{array}{ccccc}
  2 & 3 & 4\\
  0 & -1 & 2\\
  0 & 0 & 0 \\
        \end{array}\right)\,.$$

\noindent Si utilizamos la identidad ${\textbf L}{\textbf z}={\textbf b}$ obtenemos
$$\left(\begin{array}{ccccc}
  1 & 0 & 0\\
  2 & 1 & 0\\
  2 & -2 & 1 \\
        \end{array}\right)
	\left(\begin{array}{c}
  z_1 \\
  z_2 \\
  z_3 \\
        \end{array}\right)=
	\left(\begin{array}{c}
  6 \\
  16 \\
  2 \\
        \end{array}\right)\,.$$

\noindent Si resolvemos para ${\textbf z}$
$$z_1=6$$
$$z_2=16-2z_1=4$$
$$z_3=2+2z_2-2z_1=-2$$

\noindent Así que
$${\textbf z}=\left(\begin{array}{c}
  6 \\
  4 \\
  -2 \\
        \end{array}\right)\,.$$

\noindent Si utilizamos ahora la definición ${\textbf U}{\textbf x}={\textbf z}\,,$
$$\left(\begin{array}{ccccc}
  2 & 3 & 4\\
  0 & -1 & 2\\
  0 & 0 & 0 \\
        \end{array}\right)
	\left(\begin{array}{c}
  x_1 \\
  x_2 \\
  x_3 \\
        \end{array}\right)=
	\left(\begin{array}{c}
  6 \\
  4 \\
  -2 \\
        \end{array}\right)\,,$$
\noindent y obtenemos
$$x_3=1$$
$$x_2=\frac{4-2x_3}{-1}=-2$$
$$x_1=\frac{6-4x_3-3x_2}{2}=4$$

\noindent Por lo tanto la solución del sistema de ecuaciones es
$${\textbf x}=\left(\begin{array}{c}
  4 \\
  -2\\
  1 \\
        \end{array}\right)\,.$$

\vspace{0.5cm}
{\noindent \textbf Valores propios y vectores propios}

\noindent Sea ${\textbf A}$ una matriz cuadrada, un número real $\lambda$ se dice que es un
valor propio de ${\textbf A}$ si existe un vector, diferente del vector cero, ${\textbf x}$
tal que
$${\textbf A}{\textbf x}=\lambda{\textbf x}\,.$$

\noindent Es decir, ${\textbf x}$ es un vector que al transformarlo mediante la multiplicación por
${\textbf A}$ el vector resultante mantiene la misma dirección; solamente se modifica
su longitud (magnitud) y/o sentido. El valor propio $\lambda$ nos informa si el
vector propio ${\textbf x}$ se acorta o alarga o cambia de signo cuando es multiplicado
por ${\textbf A}$.

\vspace{0.5cm}
{\noindent \textbf Definición} El número $\lambda$ es un valor propio si y solo si
$$|{\textbf A}-\lambda{\textbf I}|=0\,.$$

\vspace{0.5cm}
{\noindent \textbf Ejemplo:}

\noindent Calcula los valores propios y vectores propios de la
matriz
$${\textbf A}=\left(\begin{array}{ccccc}
  1 & 2\\
  2 & 4\\
        \end{array}\right)\,.$$

\noindent Sabemos
$$|{\textbf A}-\lambda{\textbf I}|=\left|\begin{array}{ccccc}
  1-\lambda & 2\\
  2 & 4-\lambda\\
        \end{array}\right|=\lambda^2-5\lambda=0\,.$$

\noindent El polinomio de arriba se llama polinomio característico y es igual a cero cuando
$\lambda$ es un valor propio. Resolviendo obtenemos dos soluciones $\lambda=0$ y
$\lambda=5$. Ahora para encontrar los vectores propios debemos resolver el sistema
$({\textbf A}-\lambda{\textbf I}){\textbf x}=0$ separadamente para las dos $\lambda$:
$$({\textbf A}-0{\textbf I}){\textbf x}=
\left(\begin{array}{ccccc}
  1 & 2\\
  2 & 4\\
      \end{array}\right)
      \left(\begin{array}{c}
  x_1 \\
  x_2 \\
      \end{array}\right)=
      \left(\begin{array}{c}
  0 \\
  0 \\
      \end{array}\right)\,\,\,\rightarrow\,\,\,{\textbf x}=\left(\begin{array}{c}
  2 \\
  -1 \\
      \end{array}\right)\,\,\,{\rm para}\,\,\,\lambda_1=0\,,$$

y

$$({\textbf A}-5{\textbf I}){\textbf x}=
\left(\begin{array}{ccccc}
  -4 & 2\\
  2 & -1\\
      \end{array}\right)
      \left(\begin{array}{c}
  x_1 \\
  x_2 \\
      \end{array}\right)=
      \left(\begin{array}{c}
  0 \\
  0 \\
      \end{array}\right)\,\,\,\rightarrow\,\,\,{\textbf x}=\left(\begin{array}{c}
  1 \\
  2 \\
      \end{array}\right)\,\,\,{\rm para}\,\,\,\lambda_2=5\,.$$

\vspace{0.25cm}
\noindent {\textbf Ejercicio:} Calcule los valores y vectores propios de la matriz de rotación

$${\textbf R}=
\left(\begin{array}{ccccc}
  cos\theta & -sen\theta\\
  sen\theta & cos\theta\\
      \end{array}\right)\,.$$

\vspace{0.5cm}
{\noindent \textbf Multiplicación de matrices}

\noindent El producto matricial de dos matrices ${\textbf A}$ ($m\times n$) y ${\textbf B}$ ($n\times p$)
se define como
$${\textbf C}={\textbf A}{\textbf B}\,,$$
donde ${\textbf C}$ es una matriz $m\times p$, con el elementi $(i,j)$ definido por
$$c_{ij}=\sum^n_{k=1} a_{ik} b_{kj}\,,$$
para todo $i=1,2,...,m; j=1,2,...,p$.

\vspace{0.5cm}
{\noindent \textbf Multiplicación de matrices}

\noindent Sea ${\textbf A}$ una matriz $m\times n$, y ${\textbf v}$ un vector $n\times 1$, entonces
el elemento del producto
$${\textbf z}={\textbf A}{\textbf v}$$
viene dado por
$$z_{i}=\sum^n_{k=1} a_{ik} v_{k}\,,$$
para todo $i=1,2,....,m$. Similarmente, si ${\textbf u}$ es un vector $m\times 1$,
entonces el elemento del producto
$${\textbf z}^T={\textbf u}^T{\textbf A}$$
viene dado por
$$z_{i}=\sum^n_{k=1} a_{ki} u_{k}\,,$$
para todo $i=1,2,....,n$. Finalmente, el escalar que resulta del
producto
$\alpha={\textbf u}^T{\textbf A}{\textbf v}\,,$
viene dado por
$$\alpha=\sum^m_{j=1}\sum^n_{k=1}a_{jk} u_j v_k\,.$$

\vspace{0.5cm}
{\noindent \textbf Proposición:}

\noindent Sea ${\textbf C}={\textbf A}{\textbf B}$ una matriz $m\times p$, entonces
$${\textbf C}^T={\textbf B}^T {\textbf A}^T\,.$$

\vspace{0.5cm}
{\noindent \textbf Proposición} Sea ${\textbf A}$ y ${\textbf B}$ matrices cuadradas $n\times n$ invertibles.
\noindent Sea el producto matricial ${\textbf C}={\textbf A}{\textbf B}$, entonces
$${\textbf C}^{-1}={\textbf B}^{-1} {\textbf A}^{-1}\,.$$

\vspace{0.5cm}
{\noindent \textbf Derivada de matrices}

\vspace{0.5cm}
{\noindent \textbf Proposición:}

\noindent Sea
$${\textbf y}={\textbf A}{\textbf x}\,,$$
donde ${\textbf y}$ es $m\times 1$, ${\textbf x}$ es $n\times 1$, ${\textbf A}$ es $m\times n$, y ${\textbf A}$
no depende de ${\textbf x}$, entonces la derivada de ${\textbf y}$ es
$$\frac{\partial{\textbf y}}{\partial{\textbf x}}={\textbf A}\,.$$

\vspace{0.5cm}
{\noindent \textbf Proposición:}

Sea
$${\textbf y}={\textbf A}{\textbf x}\,,$$
donde ${\textbf y}$ es $m\times 1$, ${\textbf x}$ es $n\times 1$, ${\textbf A}$ es $m\times n$, y ${\textbf A}$
no depende de ${\textbf x}$. Supongamos que ${\textbf x}$ es una función del vector ${\textbf z}$,
mientras que ${\textbf A}$ es independiente de ${\textbf z}$. Entonces
$$\frac{\partial{\textbf y}}{\partial{\textbf z}}={\textbf A}\frac{\partial{\textbf x}}{\partial{\textbf z}}\,.$$

\vspace{0.5cm}
{\noindent \textbf Proposición:}

 \noindent Sea el escalar $\alpha$ definido como
$$\alpha={\textbf x}^T{\textbf A}{\textbf x}\,,$$
donde ${\textbf x}$ es $n\times 1$, ${\textbf A}$ es $n\times n$,
y ${\textbf A}$ es independiente de ${\textbf x}$, entonces
$$\frac{\partial{\alpha}}{\partial{{\textbf x}}}={\textbf x}^T({\textbf A}+{\textbf A}^T)={\textbf x}^T{\textbf A}^T + {\textbf x}^T{\textbf A}={\textbf x}({\textbf A}^T+{\textbf A})$$

\vspace{0.5cm}
{\noindent \textbf Proposición:}

\noindent Sea el escalar $\alpha$ definido como
$$\alpha={\textbf y}^T{\textbf A}{\textbf x}\,,$$
donde ${\textbf y}$ es $m\times 1$, ${\textbf x}$ es $n\times 1$, y ${\textbf A}$ es $m\times n$,
y ${\textbf A}$ es independiente de ${\textbf x}$ y ${\textbf y}$, entonces
$$\frac{\partial{\alpha}}{\partial{{\textbf x}}}={\textbf y}^T{\textbf A}$$
y
$$\frac{\partial{\alpha}}{\partial{{\textbf y}}}={\textbf x}^T{\textbf A}\,.$$

\vspace{0.5cm}
{\noindent \textbf Definición} Sea ${\textbf A}$ una matriz $m\times n$ cuyos elementos son funciones
de un escalar $\alpha$. Entonces la derivada de ${\textbf A}$ con respecto $\alpha$
es una matriz $m\times n$ compuesta por las derivadas de elemento por elemento:
$$\frac{\partial{\textbf A}}{\partial{\alpha}}=\left(\begin{array}{cccc}
  \frac{\partial{a_{11}}}{\partial{\alpha}} & \frac{\partial{a_{12}}}{\partial{\alpha}} & .... &
   \frac{\partial{a_{1n}}}{\partial{\alpha}}\\
  \frac{\partial{a_{21}}}{\partial{\alpha}} & \frac{\partial{a_{22}}}{\partial{\alpha}} & .... &
   \frac{\partial{a_{2n}}}{\partial{\alpha}}\\
    .    &   .    &      &   . \\
    .    &   .    &      &   . \\
    .    &   .    &      &   . \\
  \frac{\partial{a_{m1}}}{\partial{\alpha}} & \frac{\partial{a_{m2}}}{\partial{\alpha}} & .... &
   \frac{\partial{a_{mn}}}{\partial{\alpha}}\\
\end{array}
  \right)
\,.$$

\section {Cuadrados mínimos  y regresión}

\noindent En esta sección se van a introducir algunos modelos lineales estadísticos o modelos de regresión. Aqui se incluyen ajustes lineales por mínimos cuadrados, coeficientes de correlación, regresión múltiple, etc.


\subsection{Métodos de cuadrados mínimos}

\noindent Estos métodos son utilizados para ajustar un modelo dependiente de un
conjunto compuesto por $k$ variables independientes $x_k;i=1,2,...,k$.

\vspace{0.5cm}
{\noindent \textbf (A) \large Mínimos cuadrados lineales}
\\
Empezamos aplicando el método en términos de estimación lineal.
Nos referimos a lineal en cuanto a los coeficientes $b_0, b_1,...,b_k$,
es decir, $y=b_0 + b_1x_1 + \epsilon$ es lineal, pero $y=b_0+sin(b_1x_1)$
no lo es.
\\
\\
(1) {\textbf Ajuste de una recta a un conjunto de datos}
\\
\noindent Queremos usar los mejores coeficientes $b_0$ y $b_1$ en el sentido que
se reduzca la desviación estandar de la recta ajustada versus los
datos. Sea $i=1,2,...,N$ observaciones, entonces
$$y_i=\hat{y_i} + \epsilon\,,$$
donde
$$\hat{y_i}=b_0+b_1 x_i$$
es el estimador estadístico y $\epsilon$ es el residuo (medida de la diferencia
de la recta ajustada versus conjunto de puntos). Para encontrar $b_0, \, b_1$ debemos
de minimizar la suma de los errores cuadrados (SEC), donde SEC es la varianza
total que no es explicada por nuestro modelo de regresión lineal
$$SEC=\sum^N_{i=1} \epsilon^2_i=\sum^N_{i=1} (y_i - \hat{y_i})^2=\sum^N_{i=1} [y_i - (b_0+b_1 x_i)]^2\,.$$
\\
\\
La suma de los cuadrados totales se define como
$$SCT=\sum^N_{i=1} (y_i - \bar{y})^2\,,$$
y la suma de los cuadrados residuales
$$SCR=\sum^N_{i=1} (\hat{y_i} - \bar{y})^2\,.$$
\\
La SCT es proporcional a la varianza de los datos y SCR es proporcional a la cantidad de
varianza explicada por nuestra regresión. La varianza total (o SCT) se puede descomponer
en función de SCR y SEC como:
$$SCT=\sum^N_{i=1}(y_i-\bar{y})^2 = \sum^N_{i=1}(\hat{y_i} + \epsilon_i - \bar{y})^2
=\sum^N_{i=1}((\hat{y_i}- \bar{y}) + \epsilon_i)^2=\sum^N_{i=1}(\hat{y_i}- \bar{y})^2 +$$
$$+\sum^N_{i=1}\epsilon_i^2 + 2\sum^N_{i=1}(\hat{y_i}- \bar{y})\epsilon_i\,,$$
y por lo tanto
$$SCT=SCR + SEC\,$$
\\
ya que por las ecuaciones normales $2\sum^N_{i=1}(\hat{y_i}- \bar{y})\epsilon_i=0$.
\\

\noindent El problema de mínimos cuadrados consiste en minimizar la
SEC con respecto los coeficientes, cuyas condiciones son
$$\frac{\partial{SEC}}{\partial{b_0}}=0; \frac{\partial{SEC}}{\partial{b_1}}=0\,.$$
\\

\noindent Substituyendo obtenemos para $b_0$
$$\frac{\partial{SEC}}{\partial{b_0}} =\frac{\partial}{\partial{b_0}}
  \left\{
    \sum^N_{i=1} [y_i - (b_0+b_1 x_i)]^2
  \right\}=
  -2\sum^N_{i=1} [y_i - (b_0+b_1 x_i)]=$$
$$=-2\left(\sum^N_{i=1}y_i -N b_0 - b_1\sum^N_{i=1} x_i  \right)=0\,,$$
y para $b_1$
 $$\frac{\partial{SEC}}{\partial{b_1}} =\frac{\partial{}}{\partial{b_1}}
  \left\{
    \sum^N_{i=1} [y_i - (b_0+b_1 x_i)]^2
  \right\}=
  -2\sum^N_{i=1} x_i [y_i - (b_0+b_1 x_i)]=$$
$$=-2\left(\sum^N_{i=1}x_i y_i - b_0\sum^N_{i=1}x_i - b_1\sum^N_{i=1} x^2_i \right)=0\,.$$
\\
\\
\noindent Si resolvemos para $b_0$
$$b_0=\bar{y}-b_1\bar{x}\,,$$
y sustituimos en la segunda ecuación, obtenemos:
$$b_1=\frac{N \sum\limits^N_{i=1} x_i y_i - \sum\limits^N_{i=1}x_i \sum\limits^N_{i=1}y_i}
           {N\sum\limits^N_{i=1}x^2_i-\left(\sum\limits^N_{i=1}x_i \right)^2}=
      \frac{\sum\limits^N_{i=1} x_i y_i - \frac{1}{N}\sum\limits^N_{i=1}x_i \sum\limits^N_{i=1}y_i}
           {\sum\limits^N_{i=1}x^2_i-\frac{1}{N}\left(\sum\limits^N_{i=1}x_i \right)^2}=$$
$$=\frac{(N-1)C_{xy}}{(N-1)s^2_x}=\frac{\sum\limits^N_{i=1} (x_i-\bar{x})(y_i-\bar{y})}
                                      {\sum\limits^N_{i=1} (x_i-\bar{x})^2}=\frac{<x'y'>}{<x'^2>}\,,$$
donde $C_{xy}=<x'y'>$ es la covarianza entre $x$ e $y$, y $s^2_x=<x'^2>$ es la varianza de $x$.
Así, una vez hemos calculado las medias de las variables $x$ e $y$, podemos encontrar el
coeficiente $b_1$ y, a partir de este, calcular el segundo coeficiente $b_0$.
\\
\\
\noindent Si sustituimos
$b_0=\bar{y}-b_1\bar{x}$ en la ecuación de la regresión lineal $\hat{y_i}=b_0+b_1 x_i$
obtenemos
$$\hat{y_i}=\bar{y} +b_1(x_i-\bar{x})\,,$$
o
$$\hat{y_i}=\bar{y} +b_1{x'_i}\,,$$
o finalmente
$$\hat{y'_i}=b_1{x'_i}\,,$$
que nos informa que cuando $x'_i=0\,(x_i=\bar{x})$ entonces $\hat{y'_i}=0
\,(\hat{y}=\bar{y})$, es decir, la recta
pasa por el punto $(\bar{x},\bar{y})$, de tal forma, que puesto que $\partial{SEC}/{\partial{b_0}}=0$
minimiza la suma del error, $\sum \epsilon_i=0$, los puntos estan dispersos respecto a la recta ajustada
de tal forma que los residuos positivos ($\epsilon>0$) siempre se cancelan con los residuos negativos
($\epsilon<0$). El parámetro $b_0$ se interpreta como la intersección (corte con el eje $y$) y $b_1$
es la pendiente de la recta ajustada.
\\
\\
\noindent El cociente
$$100(SCR/SCT)\,,$$
es el porcentaje de varianza explicada por nuestra regresión lineal
(varianza explicada/varianza total) y nos informa de la bondad del
ajuste denominado {\it coeficiente de correlación}, $r^2$. Si la
regresión se ajusta perfectamente a todos los datos, todos los
residuos son cero y por lo tanto $SEC=0$ y $SCR/SCT=r^2=1$. A medida
que el ajuste empeora el coeficiente $r^2$ disminuye hasta un mínimo
posible de $r^2=0$.

\vspace{0.5cm}
{\noindent \textbf Error estandar de la estimación}

\noindent Una medida de la magnitud absoluta de la bondad del ajuste es el error
estandar del estimado, $s_{\epsilon}$, definido como
$$s_{\epsilon}=[SEC/(N-2)]^{1/2}=\left[ \frac{1}{N-2}\sum\limits^N_{i=1} (y-\hat{y})^2\right]^{1/2}\,.$$
El número de grados de libertad, $N-2$, se debe a que necesitamos estimar dos
parámetros para encontrar realizar la regresión lineal.
Si ${\epsilon}$ es una variable aleatoria que sigue una distribución Normal de media cero y
desviación estandar $s_{\epsilon}$, entonces, el $68.3\%$ de las observaciones caen dentro del
intervalo $\pm 1s_{\epsilon}$ unidades de la recta ajustada, $95.4\%$ caerá dentro del intervalo
$\pm 2 s_{\epsilon}$ unidades de la recta, y $99.7\%$ caerán en el intervalo  $\pm 3 s_{\epsilon}$
unidades de la recta. $s_{\epsilon}$ es la desviación estandar de $y$ alrededor de su media,
i.e., la recta ajustada $b_0 + b_1 x$.


\vspace{0.5cm}
{\noindent \textbf Generalización de mínimos cuadrados en notación matricial}

\noindent Supongamos el modelo dependiente de $k$ variables independientes $X_k$
$$Y=b_0 + b_1 X_1 + b_2 X_2 + .... + b_k X_k + \epsilon\,,$$
y supongamos que hacemos N observaciones independientes $y_1, y_2,....,y_N$ de
$Y$. Por lo tanto podemos escribir el modelo como
$$y_i=b_0 + b_1 x_{i1} + b_2 x_{i2} + ... + b_k x_{ik} + \epsilon_i\,,$$
donde $x_{ij}$ es la observación $i$ del de la variable independiente $j$.
Es decir,
$$N;\,\,{\rm observaciones}$$
$$k;\,\,{\rm variables\,\,independientes}$$
$$k+1;\,\,{\rm coeficientes}$$

\noindent Si escribimos en notación matricial
$${\textbf Y}=\left(\begin{array}{c}
  y_1\\
  y_2\\
  ...\\
  ...\\
  ...\\
  y_N
      \end{array}\right)\,\,\,{\textbf X}=\left(\begin{array}{cccc}
  1 & x_{11} & ... & x_{1k}\\
  1 & x_{21} & ... & x_{2k}\\
  ... & ... & ... & ...\\
  ... & ... & ... & ...\\
  1 & x_{N1} & ... & x_{Nk}\\
      \end{array}\right)$$
$${\textbf B}=\left(\begin{array}{c}
  b_0\\
  b_1\\
  ...\\
  ...\\
  ...\\
  b_k
      \end{array}\right)\,\,\,{\textbf E}=\left(\begin{array}{c}
  \epsilon_1\\
  \epsilon_2\\
  ...\\
  ...\\
  ...\\
  \epsilon_N
      \end{array}\right)\,.$$
\\
\\
\noindent De esta forma, podemos escribir nuestro modelo en notación matricial como
$${\textbf Y}={\textbf X}{\textbf B} + {\textbf E}\,.$$
\\
\\
\noindent Si restringimos el modelo a una variable independiente ($k=1$), i.e. dos coeficientes $(b_0, b_1)$,
entonces
$${\textbf B}=\left(\begin{array}{c}
  b_0\\
  b_1\\
      \end{array}\right)\,,$$
es la matriz de coeficientes, e
$${\textbf X}=\left(\begin{array}{cc}
  1& x_{11}\\
  1& x_{21}\\
  ... & ...\\
  ... & ...\\
   ... & ...\\
 1 & x_{N1}
 \end{array}\right)\,,$$
es la matriz de variables independientes.
Si utilizamos la definición de los residuos como
$${\textbf E}={\textbf Y}-{\textbf X}{\textbf B}\,,$$
podemos encontrar la suma de los residuos al
cuadrado como
$$SEC=\sum^N_{i=1} \epsilon^2_i=\sum^N_{i=1} \epsilon_i \epsilon_i=
{\textbf E}^T{\textbf E}=({\textbf Y}-{\textbf X}{\textbf B})^T({\textbf Y}-{\textbf X}{\textbf B})=$$
$${\textbf Y}^T{\textbf Y} - {\textbf Y}^T{\textbf X}{\textbf B}-{\textbf B}^T{\textbf X}^T{\textbf Y}+{\textbf B}^T
{\textbf X}^T{\textbf X}{\textbf B}\,.$$

\noindent Si queremos minimizar esa suma de errores entonces
$$\frac{\partial{SEC}}{\partial{\textbf B}}=0\,.$$
Para calcular las derivadas de ${SEC}$ respecto de los coeficientes ${\textbf b}$
vamos a asumir las siguientes consideraraciones
\\
\\
(i)  ${\textbf Y}^T{\textbf X}{\textbf B}={\textbf B}^T{\textbf X}^T{\textbf Y}$ ya
que son matrices elemento ($1\times1$) y siempre son simétricas.
Por lo tanto
$-{\textbf Y}^T{\textbf X}{\textbf B}-{\textbf B}^T{\textbf X}^T{\textbf Y}=-2{\textbf B}^T{\textbf X}^T{\textbf Y}$,
y su derivada
$$-2{\textbf X}^T{\textbf Y}\,.$$
\\
\\
(ii) $\partial{{\textbf B}^T{\textbf X}^T{\textbf X}{\textbf B}}/\partial{\textbf B}=2{\textbf X}^T{\textbf X}{\textbf B}$. Para
demostrar esto tomemos el caso para el ajuste lineal (2 parámetros).
Definamos los elementos de ${\textbf X}^T{\textbf X}$ como $c_{ij}\,\,i,j=1,2$ y
$c_{12}=c_{21}$ por ser simétrica. Entonces
$${\textbf B}^T{\textbf X}^T{\textbf X}{\textbf B}=c_{11}b_0^2+c_{22}b_1^2 + 2c_{12} b_0 b_1\,,$$
y su derivada respecto $b_0$ es
$$2c_{11}b_0 + 2c_{12}b_1$$
y respecto a $b_1$ es
$$2c_{12}b_0 + 2c_{22}b_1\,.$$
Si acomodamos las derivadas en un vector columna $2\times1$
obtenemos
$$\left(\begin{array}{c}
  2c_{11}b_0 + 2c_{12}b_1 \\
  2c_{12}b_0 + 2c_{22}b_1 \\
        \end{array}\right)=2{\textbf X}^T{\textbf X}{\textbf B}\,.$$
y por lo tanto
$$\frac{\partial{S}}{\partial{\textbf B}}=-2{\textbf X}^T{\textbf Y}+2{\textbf X}^T{\textbf X}{\textbf
B}\,,$$
y las ecuaciones normales quedan
$$-2{\textbf X}^T{\textbf Y}+2{\textbf X}^T{\textbf X}{\textbf
B}=0\,.$$

\noindent Finalmente, el problema de mínimos cuadrados es
$$({\textbf X}^T{\textbf X}){\textbf B}={\textbf X}^T{\textbf Y}\,.$$
Y resolviendo para ${\textbf B}$ la forma general del método
de regresión por mínimos cuadrados es
$${\textbf B}=({\textbf X}^T{\textbf X})^{-1}{\textbf X}^T{\textbf Y}\,,$$
o equivalentemente
$${\textbf B}^T={\textbf Y}^T{\textbf X}({\textbf X}^T{\textbf X})^{-1}\,.$$


\vspace{0.5cm}
\noindent {\textbf Propiedades estadísticas}
\\
\noindent Algunas de las consideraciones de un modelo
de regresión múltiple son:
\begin{itemize}
\item[1.]Distribuciones aleatorias con media cero. El vector
$N\times1$ de errores, $\epsilon$, sigue una distribución aleatoria de
media cero tal que $E[\epsilon]=0$, es ecir, $E[\epsilon_i]=
0\,(i=1,2,....,N)$.
\item[2.]Homoscedasticidad. La matriz de covarianzas de los errores
$E[\epsilon \epsilon']$ existe y los elementos de la diagonal
principal son igual a $\sigma^2$, es decir,
$E[\epsilon^2_i]=\sigma^2\,(i=1,2,....,N)$.
\item[3.]No correlación. Los elementos fuera de la diagonal principal
de la matriz de errores $E[\epsilon \epsilon']$ son iguales a cero,
es decir, $E[\epsilon_i\epsilon_j]=0$ for todo $i\neq j$.
\item[4.]Parámetros constantes. Los elementos del vector $k\times1$,
$\beta$, y el escalar $\sigma$ son números fijos desconocidos con
$\sigma>0$.
\item[5.]Modelo lineal. Los valores del vector dependiente $y$ han sido
generados a partir de el modelo de regresión lineal multivariado:
$${\rm y}={\textbf X}\beta + \epsilon$$
\item[6.]Normalidad. Las distribuciones aleatorias de los errores siguen
una distribución Normal. Las consideraciones 2 y 3 pueden expresarse en
notación matricial
$$E[\epsilon \epsilon']=\sigma^2\,{\textbf I}\,,$$
donde ${\textbf I}$ es la matrix identidad $N\times N$. Si además la
consideración 1 se cumple, entonces $\epsilon$ sigue una distribución Normal
$$\epsilon\sim N(0,\sigma^2{\textbf I})\,.$$
\\
\noindent Las consideraciones 3 y 6 implican que las distribuciones aleatorias $\epsilon_i,\,
i=1,2,....,N$ son linealmente independientes.
\end{itemize}

\vspace{0.5cm}
\noindent {\textbf Mínimos cuadrados no tienen sesgo}
\\
\noindent La esperanza del vector de coeficientes $k\times 1$, $b$, se obtiene a partir de
las consideraciones 1,4, y 5. La consideración 5 implica que el estimador de
los coeficientes $b=({\textbf X}^T{\textbf X})^{-1}{\textbf X}^T {\rm y}$ se puede escribir cómo

$$b=({\textbf X}^T{\textbf X})^{-1}{\textbf X}^T({\textbf X} \beta + \epsilon)={\textbf I}\beta +
    ({\textbf X}^T{\textbf X})^{-1}{\textbf X}^T \epsilon\,.$$
\\
\\
\noindent De las consideraciones 1, y 4 obtenemos
$$E[b]=E[\beta + ({\textbf X}^T{\textbf X})^{-1}{\textbf X}^T\epsilon]=
  \beta + ({\textbf X}^T{\textbf X})^{-1}{\textbf X}^T \underbrace{E[\epsilon]}_{0}=\beta\,,$$
lo que implica que $b$ es insesgado.

\vspace{0.5cm}
\noindent {\textbf Matriz de varianza de $b$}
\\
\noindent Usando el resultado de arriba, bajo las consideraciones 1-6, la matriz
de varianza de los estimadores de los coeficientes $b$ (o momento centrado
de orden 2) viene dada por
$${\rm var(b)}=E[(b-\beta)(b-\beta)^T]=E[({\textbf X}^T{\textbf X})^{-1}{\textbf X}^T\epsilon \epsilon^T{\textbf X}
    ({\textbf X}^T{\textbf X})^{-1}]=$$
    $$=({\textbf X}^T{\textbf X})^{-1}{\textbf X}^T E[\epsilon \epsilon^T]{\textbf X}({\textbf X}^T{\textbf X})^{-1}=
    ({\textbf X}^T{\textbf X})^{-1}{\textbf X}^T(\sigma^2{\textbf I}){\textbf X}({\textbf X}^T{\textbf X})^{-1}=
    \sigma^2({\textbf X}^T{\textbf X})^{-1}\,.$$
\\
\\
\noindent Los elementos de la diagonal principal de esta matriz son las
varianzas asociadas a los estimadores de los coeficientes $b$,
y los elementos fuera de la diagonal principal representan la
covarianza entre esos estimadores.
\\
Si asumimos que las variables del problema han sido
estandarizadas, es decir la media ha sido extraida de las
variables $x_{ij}$ e $y_i$ y hemos dividido por las
desviaciones estandar
$${\textbf X}^T{\textbf X}=\left(\begin{array}{cccc}
   \sum\limits_{i=1}^N{x_{i1}x_{i1}} & \sum\limits_{i=1}^N{x_{i1}x_{i2}} & ... & \sum\limits_{i=1}^N{x_{i1}x_{ik}}\\
   \sum\limits_{i=1}^N{x_{i2}x_{i1}} & \sum\limits_{i=1}^N{x_{i2}x_{i2}} & ... & \sum\limits_{i=1}^N{x_{i2}x_{ik}}\\
                . & . & . \\
		. & . & . \\
		. & . & . \\
   \sum\limits_{i=1}^N{x_{ik}x_{i1}} & \sum\limits_{i=1}^N{x_{ik}x_{i2}} & ... & \sum\limits_{i=1}^N{x_{ik}x_{ik}}\\
        \end{array}\right)\,,$$
es una matriz $k\times k$ y en notación índice se puede escribir como
$$[{\textbf X}^T{\textbf X}]_{nm}=(N-1)\rho_{x_{in} x_{im}}\,\,;(i=1,2,...,N)\,,$$
que se puede interpretar como la matriz de correlaciones
entre las variables independientes.
La matriz
$${\textbf X}^T{\textbf Y}=\left(\begin{array}{c}
  \sum\limits_{i=1}^N y_i x_{i1}\\
  \sum\limits_{i=1}^N y_i x_{i2}\\
                   . \\
		   . \\
		   . \\
  \sum\limits_{i=1}^N y_i x_{ik}\\
        \end{array}\right)\,,$$
es una matriz $k\times 1$ y en notación índice es
$$[{\textbf Y}^T{\textbf X}]_n=(N-1)\rho_{y_i x_{in}}\,\,;(i=1,2,...,N)\,.$$
Esta matriz se puede interpretar como la matriz de correlación entre las variables
independientes y dependientes.
\\
\noindent El modelo multivariado en notación índice es
$$(N-1)\rho_{x_{in} x_{im}}b_m=(N-1)\rho_{y_i x_{in}}$$
o
$$\rho_{x_{in} x_{im}}b_m=\rho_{y_i x_{in}}\,;m=n=1,2,....,k; i=1,2,...,N$$
y para una única observación $i$ dada obtenemos
$$\rho_{x_{n} x_{m}}b_m=\rho_{y x_{n}}\,$$
Ahora supongamos, por simplicidad, que solo tenemos dos variables independientes.
Entonces:
$$\rho_{x_{1} x_{1}}b_1+\rho_{x_{1} x_{2}}b_2=\rho_{y x_{1}}$$
$$\rho_{x_{2} x_{1}}b_1+\rho_{x_{2} x_{2}}b_2=\rho_{y x_{2}}\,,$$
y puesto que $\rho_{x_{1} x_{1}}=\rho_{x_{2} x_{1}}=1$, y $\rho_{x_{1} x_{2}}=\rho_{x_{2} x_{1}}$,
podemos escribir el sistema como:
$$\left(\begin{array}{cc}
  1 & \rho_{x_{1} x_{2}} \\
  \rho_{x_{1} x_{2}} & 1 \\
        \end{array}\right)
  \left(\begin{array}{c}
  b_1 \\
  b_2 \\
        \end{array}\right)=
	 \left(\begin{array}{c}
  \rho_{y x_{1}} \\
  \rho_{y x_{2}} \\
        \end{array}\right)\,.$$
De forma que los coeficientes quedan como
$$\left(\begin{array}{c}
  b_1 \\
  b_2 \\
        \end{array}\right)=\left(\begin{array}{cc}
  1 & \rho_{x_{1} x_{2}} \\
  \rho_{x_{1} x_{2}} & 1 \\
        \end{array}\right)^{-1}
	 \left(\begin{array}{c}
  \rho_{y x_{1}} \\
  \rho_{y x_{2}} \\
        \end{array}\right)=\frac{1}{1-\rho^2_{x_{1} x_{2}}}
	\left(\begin{array}{cc}
  1 & -\rho_{x_{1} x_{2}} \\
  -\rho_{x_{1} x_{2}} & 1 \\
        \end{array}\right)\left(\begin{array}{c}
  \rho_{y x_{1}} \\
  \rho_{y x_{2}} \\
        \end{array}\right)\,.$$
$$b_1=\frac{1}{1-\rho^2_{x_{1} x_{2}}}({\rho_{y x_{1}} - \rho_{x_{1} x_{2}} \rho_{y x_{2}}})$$
$$b_2=\frac{1}{1-\rho^2_{x_{1} x_{2}}}({\rho_{y x_{2}} - \rho_{x_{1} x_{2}} \rho_{y x_{1}}})\,.$$

\noindent Finalmente puntualizar que el problema de mínimos cuadrados se puede
resolver utilizando la descomposición $LU$. Para ello solo es necesario
un cambio de variable en la ecuación
$${\textbf X}^T{\textbf X}{\textbf B}={\textbf X}^T{\textbf Y}$$
para obtener un sistema de ecuaciones tipo ${\textbf A}{\textbf x}={\textbf b}$,
donde ahora ${\textbf A}={\textbf X}^T{\textbf X}$, ${\textbf x}={\textbf B}$, y
${\textbf b}={\textbf X}^T{\textbf Y}$.

\vspace{0.5cm}
{\noindent \textbf (2) Mínimos cuadrados con restricciones}

\vspace{0.5cm}
{\textbf \noindent NOTA: Multiplicadores de Lagrange}
\\
\noindent Dada la función $f(x)=f(x_1,x_2,...,x_N)$ que depende de $N$ variables y
$p$ restricciones $g_1(x)=d_1, g_2(x)=d_2,....,g_p(x)=d_p$ entonces el
teorema de Lagrange nos dice que para minimizar la función $f(x)$ bajo
esas $p$ restricciones debemos resolver el sistema de ecuaciones
$$\frac{\partial}{\partial{x_i}}\left[ f(x) +\sum\limits_{j=1}^p \lambda_j g_j(x)\right]=0\,\,\,; i=1,2,...,N$$
$$g_j(x)=d_j\,\,\,; j=1,2,...,p$$

\noindent Vamos a usar los multiplicadores de Lagrange para resolver el
problema de mínimos cuadrados
$${\textbf Y}={\textbf X}{\textbf B}\,,$$
pero incluyendo $p$ restricciones de la forma
$${\textbf G}{\textbf B}={\textbf d}\,.$$

\noindent Queremos minimizar la función:
$${\textbf \cal L}=({\textbf Y}-{\textbf X}{\textbf B})^T({\textbf Y}-{\textbf X}{\textbf B}) +{\textbf \lambda}^T({\textbf G}{\textbf B}-{\textbf d})\,.$$
Aqui hemos introducido $p$ incógnitas pero
también tenemos $p$ nuevas ecuaciones ${\textbf G}{\textbf B}={\textbf d}$.
Derivando ${\textbf \cal L}$ e igualando a cero
$$\frac{\partial{\textbf \cal L}}{\partial{\textbf B}}=-2{\textbf X}^T{\textbf Y}+2{\textbf X}^T{\textbf X}{\textbf B} + {\textbf G}^T{\textbf \lambda}=0\,,$$
lo cual tiene la solución
$${\textbf B}=( {\textbf X}^T {\textbf X} )^{-1} ({\textbf X}^T {\textbf Y} -\frac{1}{2}{\textbf G}^T{\textbf \lambda})\,,$$
que para ${\textbf \lambda}=0$ se reduce a la expresión de los
mínimos cuadrados sin restricciones
$${\textbf B}=( {\textbf X}^T {\textbf X} )^{-1} ({\textbf X}^T {\textbf Y})\,.$$

\noindent Si sutituimos en la ecuación de las restricciones
$${\textbf G}( {\textbf X}^T {\textbf X} )^{-1} ({\textbf X}^T {\textbf Y} -\frac{1}{2}{\textbf G}^T{\textbf \lambda})={\textbf d}\,,$$
y resolvemos para ${\textbf \lambda}$, obtenemos
$$\frac{1}{2}{\textbf \lambda}=\left[ {\textbf G}({\textbf X}^T {\textbf X})^{-1}{\textbf G}^T\right]^{-1} \left[ {\textbf G}({\textbf X}^T {\textbf X})^{-1}{\textbf X}^T{\textbf Y}-{\textbf d}\right]\,.$$

\noindent Finalmente, si sustituimos en la solución para la matriz de coeficientes ${\textbf B}$
obtenemos
$${\textbf B}=({\textbf X}^T {\textbf X})^{-1}\left( {\textbf X}^T {\textbf Y} - {\textbf G}^T\left[{\textbf G}({\textbf X}^T{\textbf X})^{-1}{\textbf G}^T \right]^{-1}
           \left[{\textbf G}({\textbf X}^T{\textbf X})^{-1}{\textbf X}^T {\textbf Y}-{\textbf d}\right] \right)\,.$$

\noindent Un ejemplo sería ajustar una recta a un conjunto de
observaciones pero imponiendo que la recta pase por
algún punto determinado del espacio $xy$.

\vspace{0.5cm}
{\noindent \textbf (3) Mínimos cuadrados pesados}
\\
\noindent En ocasiones debido a las incertidumbres asociadas a la medidas de las
variables independientes es conveniente asignarles un peso diferencial
en el problema de mínimos cuadrados. Supongamos que unas variables
independientes son conocidas con mayor precisión que otras. Entonces
el modelo de mínimos cuadrados pesados es:
$$SEC=({\textbf Y}-{\textbf X}{\textbf B})^T{\textbf W}_{\epsilon}({\textbf Y}-{\textbf X}{\textbf B})\,,$$
donde
${\textbf W}_{\epsilon}$
es una matriz diagonal con los elementos $\sigma_j^{-2}$, el inverso
de la varianza de cada variable independiente.
\\
\noindent Sin embargo, en general, los errores en los datos estan correlacionados, y
 ${\textbf W}_{\epsilon}$ no es diagonal. Una elección razonable para ${\textbf W}_{\epsilon}$ es
 el inverso de la matriz de covarianzas. Si asumimos que ${\textbf Y}$ se compone de un
 valor medio mas un error o fluctuación respecto la media
 $${\textbf Y}=\bar{\textbf Y}+{\textbf Y}'\,,$$
entonces la matriz de covarianzas es $<{\textbf Y}'{\textbf Y}'^T>$ y
 $${\textbf W}_{\epsilon}=<{\textbf Y}'{\textbf Y}'^T>^{-1}\,.$$


\vspace{0.5cm}
{\noindent \textbf Ejemplo (1):} Emery and Thompson
(sección 3.12.4).
\\
\noindent En la tabla adjunta se muestran 5 observaciones
de la variable independiente ($x_i$) y dependiente ($y_i$). Se pide
ajustar una recta al conjunto de datos y calcular las medidas de
error (varianza $s^2$ y coeficiente de correlación al cuadrado $r^2$) asociadas al ajuste lineal.
<!--
your comment goes here
\begin{center}
`` \includegraphics[width=1\textwidth]{regresion_example} ``
\end{center}
-->
%
%insert table
\begin{center}
\begin{tabular}{c c c c}
\hline\hline
$x_i$ & $y_i$ & $x_i\,y_i$ & $x_i^2$ \\ [0.5ex] % inserts table
\hline
-2 & 0 & 0 & 4 \\
-1 & 0 & 0 & 1 \\
0 & 1 & 0 & 0 \\
1 & 1 & 1  & 1 \\
2 & 3 & 6 & 4 \\ [1ex] % [1ex] adds vertical space
\hline
\end{tabular}
\end{center}

Los coeficientes del modelo lineal se pueden calcular con las expresiones:
$$\hat{b}_1=\frac{\left[N \sum\limits^N_{i=1} x_i y_i - \sum\limits^N_{i=1}x_i \sum\limits^N_{i=1}y_i\right]}
           {N\sum\limits^N_{i=1}x^2_i-\left(\sum\limits^N_{i=1}x_i \right)^2}=$$
	   $$\frac{[(5)(7)-(0)(5)]}{[(5)(10)-10^2]}=0.7$$

$$b_0=\bar{y}-b_1\bar{x}=5/5-(0.7)(0)=1$$

En notación matricial obtenemos el mismo resultado:

$${\textbf Y}=\left(\begin{array}{c}
  0 \\ 0 \\ 1 \\ 1 \\ 3\\
        \end{array}\right)$$

$${\textbf X}=\left(\begin{array}{cc}
  1 & -2 \\
  1 & -1 \\
  1 & 0 \\
  1 & 1 \\
  1 & 2\\
        \end{array}\right)$$

$${\textbf X}^T{\textbf X}= \left(\begin{array}{ccccc}
   5 &  0 \\
  0 & 10 \\
        \end{array}\right)$$

$${\textbf X}^T{\textbf Y}= \left(\begin{array}{ccccc}
   5  \\
  7  \\
        \end{array}\right)$$

$$({\textbf X}^T{\textbf X})^{-1}= \left(\begin{array}{ccccc}
   1/5 &  0 \\
   0 & 1/10 \\
        \end{array}\right)$$

$$ ({\textbf X}^T{\textbf X})^{-1}{\textbf X}^T{\textbf Y}=
	\left(\begin{array}{ccccc}
   1/5 &  0 \\
   0 & 1/10 \\
        \end{array}\right)\left(\begin{array}{ccccc}
   5  \\
  7  \\
        \end{array}\right)=
	\left(\begin{array}{ccccc}
   1  \\
  0.7  \\
        \end{array}\right)\,.$$

\vspace{0.5cm}
\noindent Como ya vimos, para calcular la varianza de nuestro ajuste usamos
$$s^2=\frac{1}{N-2}\sum\limits^N_{i=1}(y_i-\hat{y_i})^2=\frac{1}{N-2}SEC\,,$$
donde $SEC$ es la suma de los errores cuadrados y $N-2$ resulta debido
a que en la regresión lineal se requiere la estimación de
dos parámetros. En notación matricial
$$SEC={\textbf Y}^T{\textbf Y} - {\textbf Y}^T{\textbf X}{\textbf B}-{\textbf B}^T{\textbf X}^T{\textbf Y}+{\textbf B}^T
{\textbf X}^T{\textbf X}{\textbf B}=
{\textbf Y}^T{\textbf Y}-2{\textbf B}^T{\textbf X}^T{\textbf Y}+{\textbf B}^T{\textbf X}^T{\textbf X}{\textbf B}=$$
$$={\textbf Y}^T{\textbf Y}-2{\textbf B}^T{\textbf X}^T{\textbf Y}+{\textbf B}^T{\textbf X}^T{\textbf Y}=
{\textbf Y}^T{\textbf Y}-{\textbf B}^T{\textbf X}^T{\textbf Y}\,,$$
donde hemos usado la identidad ${\textbf X}^T{\textbf X}{\textbf B}={\textbf X}^T{\textbf Y}$.
Si sustituimos las matrices de nuestro ejemplo, obtenemos
$$SEC=\left(\begin{array}{ccccc}
  0 & 0 & 1 & 1 & 3\\
        \end{array}\right)
      \left(\begin{array}{c}
  0 \\ 0 \\ 1 \\ 1 \\ 3\\
        \end{array}\right)-
	\left(\begin{array}{ccccc}
  1 & 0.7\\
        \end{array}\right)
	\left(\begin{array}{ccccc}
   1 &  1 & 1 & 1 & 1\\
  -2 & -1 & 0 & 1 & 2 \\
        \end{array}\right)
	\left(\begin{array}{c}
  0 \\ 0 \\ 1 \\ 1 \\ 3
        \end{array}\right)=$$
$$=11-\left(\begin{array}{ccccc}
  1 & 0.7\\
        \end{array}\right)
	\left(\begin{array}{c}
        5 \\ 7\\
        \end{array}\right)=11-9.9=1.1$$

\noindent $SEC$ puede ser calculado directamente con la expresión:
$$SEC=\sum \limits^N_{i=1}(\hat{y_i}-y_i)^2=
   (-0.4)^2 + (-0.3)^2+(0)^2 + (0.7)^2 + (0.6)^2=1.1$$


\noindent Y entonces la desviación estandar de nuestro
ajuste lineal es
$$s=\sqrt{\frac{1}{N-2}SEC}=\sqrt{\frac{1}{5-2}1.1}=\sqrt{1.1/3}\simeq0.366\,.$$

\noindent El coeficiente de correlación se puede escribir como
$$r^2=\frac{SCR}{SCT}=\frac{\sum\limits^N_{i=1}(\hat{y_i}-\bar{y})^2}{\sum\limits^N_{i=1}
({y_i}-\bar{y})^2}=\frac{4.9}{6}\simeq0.8167$$

\vspace{0.5cm}
{\noindent \textbf (4) Ajuste de curvas con mínimos cuadrados}
\\
\noindent En general, podemos escribir nuestro modelo lineal como
$$Y=b_0 + b_1x + b_2 x^2 + ... + b_k x^k + \epsilon\,.$$
\\
\\
\noindent El procedimiento es el mismo que para el caso de la línea
recta, pero ahora la matriz ${\textbf X}$ tiene una columna mas.
Es decir, para $k=2$ y para $N$ observaciones independientes
las ecuaciones independientes son
$$y_1=b_0 + b_1x_1 + b_2 x_1^2 + \epsilon_1$$
$$y_2=b_0 + b_1x_2 + b_2 x_2^2 + \epsilon_2$$
$$...$$
$$...$$
$$...$$
$$y_N=b_0 + b_1x_N + b_2 x_N^2 + \epsilon_N\,$$
y puden resolverse matricialmente para ${\textbf B}$
como
$${\textbf B}=({\textbf X}^T{\textbf X})^{-1}{\textbf X}^T{\textbf Y}\,,$$
donde ${\textbf X}$ tiene una columna mas que el caso
del ajuste de una recta, i.e. $k+1$ columnas.

\vspace{0.5cm}
{\noindent \textbf Ejemplo (2):} Ajustes por mínimos cuadrados
con restricciones.
\\
\noindent Supongamos que queremos ajustar dos polinomios
$f(x)$ y $g(x)$ de orden d-1 a dos conjunto de datos contínuos
de $M$ y $N$ observaciones, respectivamente,
$x_1, x_2, ....,x_M\leq a$ y $x_{M+1}, x_{M+2}, ....,x_N>a$, tal
que, queremos minimizar:

$$\sum\limits^M_{i=1}(f(x_i)-y_i)^2 + \sum\limits^N_{i=M+1}(g(x_i)-y_i)^2\,,$$

sujeto a las restricciones:

$$f(a)=g(a)\,\,y\,\,f'(a)=g'(a)\,.$$\\

Primero debemos de construir las matrices del problema lineal
de cuadrados mínimos:

$$ {\textbf X}=\left(\begin{array}{cccccccc}
  1 & x_1   & .  .  . & x_1^{d-1}  & 0 & 0 & .  .  . & 0 \\
  . & .         &  & .                  & . & . &  & . \\
  . & .         &  & .                  & . & . &  & . \\
  . & .         &  & .                  & . & . &  & . \\
  1 & x_M & .  .  . & x_M^{d-1} & 0 & 0 & .  .  . & 0 \\
  0 & 0       & .  .  . & 0                 & 1 & x_{M+1} & .  .  . & x^{d-1}_{M+1} \\
  . & .         && .                  & . & . &  & . \\
  . & .         & & .                  & . & . &  & . \\
  . & .         &  & .                  & . & . &  & . \\
  0 & 0       & .  .  . & 0                & 1  & x_N & .  .  . & x_N^{d-1} \\
  \end{array}\right)
  $$

$${\textbf Y}=\left(\begin{array}{c}
    y_1 \\ y_2 \\ . . . \\ y_M \\ y_{M+1}\\ y_{M+2}\\...\\y_N
      \end{array}\right)\,,$$

$${\textbf G}=\left(\begin{array}{cccccccc}
        1 & a & .  .  .&a^{d-1}&-1&-a& .  .  .&-a^{d-1}\\
        0 & 1 & .  .  .&(d-1)a^{d-2}&0&-1& .  .  .&-(d-1)a^{d-2}\\
          \end{array}\right)\,,
          {\textbf d}=\left(\begin{array}{c}
            0\\
            0\\
              \end{array}\right)\,.$$ 

Segundo debemos de calcular los coeficientes del ajuste ${\textbf B}$
con la expresión para mínimos cuadrados con restricciones.

$${\textbf B}=({\textbf X}^T {\textbf X})^{-1}\left( {\textbf X}^T {\textbf Y} - {\textbf G}^T\left[{\textbf G}({\textbf X}^T{\textbf X})^{-1}{\textbf G}^T \right]^{-1}
           \left[{\textbf G}({\textbf X}^T{\textbf X})^{-1}{\textbf X}^T {\textbf Y}-{\textbf d}\right] \right)\,.$$
\\
Apliquemos el caso particular del ajuste de dos rectas
$f(x)=b_1 + b_2 x$ y $g(x)=b_3 + b_4 x$:
\\
\begin{itemize}
\item Minimizar:  $\sum\limits^M_{i=1}(f(x_i)-y_i)^2 + \sum\limits^N_{i=M+1}(g(x_i)-y_i)^2$
\item Restricciones:
      $$f(h)=g(h)\rightarrow b_1+b_2 h - b_3 -b_4 h=0$$
      $$f'(h)=g'(h)\rightarrow b_1          - b_3           =0\,,$$
\end{itemize}
donde $h$ es el valor donde ambas
rectas interseccionan. Por ello exigimos que ambas rectas tomen el mismo
valor y sus derivadas sean iguales en $x=h$. Se resuleve el sistema de
ecuaciones:
$${\textbf X}\,{\textbf B}={\textbf Y}$$
$$({\it 1})b_1+({\it x_1})b_2 + ({\it 0})b_3 + ({\it 0})b_4=y_1$$
$$({\it 1})b_1+({\it x_2})b_2 + ({\it 0})b_3 + ({\it 0})b_4=y_2$$
$$({\it 1})b_1+({\it x_3})b_2 + ({\it 0})b_3 + ({\it 0})b_4=y_3$$
$$({\it 1})b_1+({\it x_4})b_2 + ({\it 0})b_3 + ({\it 0})b_4=y_4$$
$$({\it 0})b_1+({\it 0})b_2 + ({\it 1})b_3 + ({\it x_5})b_4=y_5$$
$$({\it 0})b_1+({\it 0})b_2 + ({\it 1})b_3 + ({\it x_6})b_4=y_6$$
$$({\it 0})b_1+({\it 0})b_2 + ({\it 1})b_3 + ({\it x_7})b_4=y_7$$
$$({\it 0})b_1+({\it 0})b_2 + ({\it 1})b_3 + ({\it x_8})b_4=y_8\,,$$
\\
bajo el sistema de ecuaciones de restricciones:
$${\textbf G}\,{\textbf B}={\textbf d}$$
$$({\it 1})b_1+({\it h})b_2 + (-{\it 1})b_3 + (-{\it 1})b_4=0$$
$$({\it 1})b_1+({\it 0})b_2 + (-{\it 1})b_3 + ({\it 0})b_4=0$$
\\
En notación matricial:
\\
$${\textbf X}=\left(\begin{array}{cccccccc}
  1 & x_1  & 0 & 0 \\
  1 & x_2   & 0 & 0 \\
  1 & x_3   & 0 & 0\\
  1 & x_4   & 0 & 0 \\
  0 & 0  & 1 & x_5 \\
  0 & 0   & 1 & x_6 \\
  0 & 0   & 1 & x_7\\
  0 & 0   & 1 & x_8 \\
  \end{array}\right)\,,
{\textbf Y}=\left(\begin{array}{c}
    y_1 \\ y_2 \\ y_3 \\ y_4 \\ y_5\\ y_6\\y_7\\y_8
      \end{array}\right)\,,
{\textbf G}=\left(\begin{array}{cccccccc}
        1 & h & -1&-h\\
        0 & 1& 0&-1\\
          \end{array}\right)\,,
{\textbf d}=\left(\begin{array}{c}
            0\\
            0\\
              \end{array}\right)\,.
  $$

  \vspace{0.5cm}
  {\noindent \textbf (B) \large Mínimos cuadrados no-lineales}
\\
  \noindent Los mínimos cuadrados no-lineales se aplican cuando
  queremos ajustar un conjunto de observaciones a un modelo que
  no es lineal en cuanto a los coeficientes.

\vspace{0.5cm}
{\noindent   \textbf  (1) Linealización del problema no lineal}
En ocasiones se puede linealizar el problema y resolverlo
utilizando la solución de mínimos cuadrados lineales.
Por ejemplo, supongamos que queremos ajustar un
conjunto de $N$ observaciones a una función no-lineal
exponencial:
$$\hat{y}=a e^{bx}\,.$$

Este ejemplo se puede linealizar:
$$\hat{y}=a e^{bx}\rightarrow ln(\hat{y})=ln(a) + bx\,,$$
y resolver el problema lineal:
$${\cal Y}={\cal B}_1 + {\cal B}_2 {\cal X}\,,$$
donde hemos hecho el cambio de variables:
$${\cal Y}=ln(\hat{y})\,,{\cal B}_1=ln(a)\,,{\cal B}_2=b\,\,\,{\rm y}\,{\cal X}=x\,.$$

Resolvemos para ${\cal B}_1$ y ${\cal B}_2$:

$${\cal B}=\left(\begin{array}{c}
            {\cal B}_1\\
            {\cal B}_2\\
              \end{array}\right)=({\cal X}^T {\cal X})^{-1}\,{\cal X}^T{\cal Y}
\,,$$
y calculamos los coeficientes originales $a$ y $b$ con el cambio de variables.

\vspace{0.5cm}
{\noindent   \textbf  (2) Resolución numérica del problema no lineal}
\\
Cuando no es posible linealizar la función no-lineal que
queremos ajustar, debemos minimizar la suma de los errores cuadráticos
medios y resolver numéricamente. La $SEC$ del modelo exponencial
$$SEC=\sum\limits^N_{i=1}(y_i-\hat{y_i})^2=\sum\limits^N_{i=1}
              {\underbrace{\left(y_i - a e^{bx_i}\right)}_{\epsilon_i}}^2\,$$
es minimizada derivando con respecto los coeficientes:
$$\frac{\partial{SEC}}{\partial{a}}=
      2\sum\limits^N_{i=1}\left(y_i - a e^{bx_i}\right)\frac{\partial{\epsilon_i}}{\partial{a}}=0$$
$$\frac{\partial{SEC}}{\partial{b}}=
     2\sum\limits^N_{i=1}\left(y_i - a e^{bx_i}\right)\frac{\partial{\epsilon_i}}{\partial{b}}=0\,.$$
Debemos resolver el sistema de ecuaciones de arriba, lo cual se puede hacer
con descomposiciones algebráicas tipo ${\textbf L}{\textbf U}$,
minimizando $SEC$ con la función de Matlab {\it fminsearch.m}
(o implementando un método numérico manual), o
directamente utilizando la función de mínimos cuadrados
no-lineales de Matlab {\it nlinfit.m}.
\\
Otro ejemplo no-lineal es ajustar la siguiente función trigonométrica:
$$\hat{y}=\phi_1 e^{\phi_2 x} cos(\phi_3 x + \phi_4)\,,$$
donde la suma de los errores cuadráticos es:
$$SEC=\sum\limits^N_{i=1}(y_i-\hat{y_i})^2=\sum\limits^N_{i=1}
              {\underbrace{\left(y_i - \phi_1 e^{\phi_2 x_i} cos(\phi_3 x_i + \phi_4)\right)}_{\epsilon_i}}^2\,,$$
y sus derivadas respecto los coeficientes $\phi_1$, $\phi_2$, $\phi_3$, y
$\phi_4$ son:
$$\frac{\partial{SEC}}{\partial{\phi_1}}=
     2\sum\limits^N_{i=1}\left(y_i - \phi_1 e^{\phi_2 x_i} cos(\phi_3 x_i + \phi_4)\right)
     \frac{\partial{\epsilon_i}}{\partial{\phi_1}}=0$$
$$\frac{\partial{SEC}}{\partial{\phi_2}}=
    2\sum\limits^N_{i=1}\left(y_i - \phi_1 e^{\phi_2 x_i} cos(\phi_3 x_i + \phi_4)\right)
    \frac{\partial{\epsilon_i}}{\partial{\phi_2}}=0$$
$$\frac{\partial{SEC}}{\partial{\phi_3}}=
    2\sum\limits^N_{i=1}\left(y_i - \phi_1 e^{\phi_2 x_i} cos(\phi_3 x_i + \phi_4)\right)
    \frac{\partial{\epsilon_i}}{\partial{\phi_3}}=0$$
$$\frac{\partial{SEC}}{\partial{\phi_4}}=
    2\sum\limits^N_{i=1}\left(y_i - \phi_1 e^{\phi_2 x_i} cos(\phi_3 x_i + \phi_4)\right)
    \frac{\partial{\epsilon_i}}{\partial{\phi_4}}=0$$

\vspace{0.5cm}
{\noindent \textbf Relación entre regresión y correlación}
\\
\noindent El coeficiente de correlación, $r$, nos informa de que tanto dos
(o mas) variables covarian en el espacio-tiempo. Para dos variables aleatorias
$x$ e $y$, el coeficiente de correlación es
$$r=\rho_{xy}=\frac{\frac{1}{N-1}\sum\limits^N_{i=1}(x_i-\bar{x})(y_i-\bar{y})}
     {\sqrt{\frac{1}{N-1}\sum\limits^N_{i=1}(x_i-\bar{x})}
     \sqrt{\frac{1}{N-1}\sum\limits^N_{i=1}(y_i-\bar{y})}}=\frac{C_{xy}}{s_x s_y}\,,$$
donde $C_{xy}$ es la covarianza de $x$ e $y$, y $s_x$ y $s_y$ son las correspondientes
desviaciones estandar.
\\
\\
{\textbf Propiedades del coeficiente de correlación}
\\
(1) $r$ es adimensional.
\\
(2) la magnitud de $r$ se encuantra acotada entre $-1$ y $1$, ya que es una
normalización de la covarianza por el producto de la desviación
estandar de las dos variables aleatorias.
\\
\\
\noindent Si $r=\pm1$ entonces el ajuste es perfecto. Para $r=0$ los puntos
estan dispersos aleatoriamente y no existe relación alguna entre
las variables. Normalmente encontramos el estadístico $r^2$
en lugar de $r$. $r^2$ se puede reescribir como
$$r^2=SCR/SCT=\frac{SCT-SEC}{SCT}=1-\frac{SEC}{SCT}=\frac{C^2_{xy}}{(s_x s_y)^2}\,,$$
lo que nos informa del porcentaje de
la varianza explicada ($r^2={\rm varianza\,explicada}/{\rm varianza\,total}$) como
vimos anteriormente. Un valor de
$r=0.75$ significa que la regresión lineal de $y$ sobre $x$, es decir $\hat{y}$,
explica $100*r^2=56.25\%$ de la varianza total de la muestra.
\\
\noindent Finalmente puntualizar que también podemos calcular el coeficiente de
correlación utilizando los estimados de los coeficientes de
regresión. Para el caso de una línea recta sabemos
$$\hat{b}_1=\frac{C_{xy}}{s^2_x}\,.$$
Si sustituimos en la definición de $r$ obtenemos

$$r=\hat{b}_1\frac{s^2_x}{s_x s_y}=\hat{b}_1\frac{s_x}{s_y}\,.$$
{\textbf Coeficiente de correlación ajustado}
\\
Con el fin de considerar los grados de libertad del
modelo de regresión lineal, el coeficiente de correlación
debe ser ajustado en función del número de variables
independientes $k$. Para ello hay que considerar la
verdadera varianza de los errores
$$Var(SEC)=\frac{SEC}{N-k-1}\,,$$
y de la variable dependiente
$$Var(SCT)=\frac{SCT}{N-1}\,.$$
Puesto que los grados de libertad no son iguales ($N-1$ vs $N-k-1$), el coeficiente de
correlación ajustado al cuadrado ($\tilde{r}^2$) se define
como
$$\tilde{r}^2=1-\frac{Var(SEC)}{Var(SCT)}=1-\frac{SEC/N-k-1}{SCT/N-1}=
1-\frac{N-1}{N-k-1}(1-r^2)=$$
$$=1-(1-r^2)\frac{N-1}{N-k-1}\,,$$
donde para $k=0$ obtenemos la definición clásica del coeficiente de
correlación.
\\
\\
{\textbf Intervalo de confianza para el coeficiente de correlación}
\\
Podemos calcular intervalos de confianza para el coeficiente
de correlación $r$ por medio de la denominada transformación Z de
Fisher. Básicamente transforma $r$ en
una variable normal estandar $Z$
$$Z=\frac{1}{2}\frac{ln(1+r)}{ln(1-r)}\,,$$
con desviación estandar
$$\sigma(Z)=\frac{1}{\sqrt{(N-3)}}\,,$$
y media
$$\mu(Z)=\frac{1}{2}\frac{ln(1+\rho_0)}{ln(1-\rho_0)}\,,$$
donde $\mu(Z)$ es la media esperada (media poblacional) del
estadístico $Z$.

%El intervalo de confianza se escribe entonces como
%$$Z-Z_{\alpha/2}<Z<Z+Z_{\alpha/2}$$

\vspace{0.5cm}
\noindent {\textbf Ejemplo:}
\\
\noindent Supongamos $N=21$ y $r=0.8$. Encuentra el intervalo de confianza
al $95\%$ para el coeficiente de correlación poblacional $\rho_0$.

$$Z=\frac{1}{2}\frac{ln(1+0.8)}{ln(1-0.8)}=1.0986$$

\noindent Puesto que $Z$ esta Normalmente distribuida, entonces todos los
valores deben de caer dentro de 1.96 desviaciones estandar de $Z$.
Entonces, al $95\%$, la verdadera media $\mu_Z$ esta contenida en
$$Z-1.96\sigma(Z) < \mu(Z) < Z + 1.96\sigma(Z)$$
$$Z-1.96\frac{1}{\sqrt{(21-3)}} < \mu(Z) < Z + 1.96\frac{1}{\sqrt{(21-3)}}$$
$$0.6366<\mu(Z)<1.5606$$

\noindent Los límites encontrados para $\mu(Z)$ los podemos transformar
en términos de la verdadera correlación
$$\mu(Z)=0.6366=\frac{1}{2} \left\{ \frac{1+\rho}{1-\rho} \right\} \rightarrow  \rho=0.56\,,$$
donde hemos usado
$$e^{2\mu}=\frac{1+\rho}{1-\rho}$$
$$e^{2\mu}-1=\rho(1+e^{2\mu})$$
$$\rho=\frac{(e^{2\mu(Z)}-1)}{(e^{2\mu(Z)}+1)}\,.$$

\noindent Podemos afirmar con un 95\% de confianza que la verdadera correlación
$\rho$ esta en el intervalo $0.56<\rho,0.92$, dado un tamaño muestral
$N=21$ y una correlación muestral $r=0.8$.

\vspace{0.5cm}
{\noindent \textbf Verdaderos grados de libertad}
\\
\noindent Ya hemos definido anteriormente los grados de libertad se define como el número de
muestras independientes $N$ menos el número de
parámetros que se quieren estimar. Esta definición es un tanto
incorrecta puesto que debemos de también asegurar que las
$N$ muestras son efectivamente independientes, es decir, no estan
autocorreladas en el espacio-tiempo. Para considerar esto $N$ debe reesecribirse como
$$N^{*}=\frac{N}{\left[ \sum\limits^{\infty}_{\tau=-\infty}
C_{xx}(\tau)C_{yy}(\tau) + C_{xy}(\tau)C_{yx}(\tau)
\right]/\left[ C_{xx}(0)C_{yy}(0)\right]}=$$
$$\frac{N}{\left[\sum\limits^{\infty}_{\tau=-\infty}
               \rho_{xx}(\tau)\rho_{yy}(\tau) +
	       \rho_{xy}(\tau)\rho_{yx}(\tau)\right]}\,.$$
\\
\\
\noindent En general, series de datos suelen estar correlacionados en el espacio-tiempo
y $N^{*}<<N$. Cuanto mayores las escalas de correlación espaciales-temporales,
menores los $N^*$. Esto nos hace pensar que es muy importante
la selección de las escalas espaciales-temporales sobre las que
queremos calcular un estadístico. Para extraer las escalas de interés
podemos usar métodos espectrales y filtros. El proceso de filtrado
se encarga de eliminar aquellas escalas que esperamos no contribuyen a la
verdadera correlación pero pueden adicionar correlación artificial debido
a errores instrumentales y de muestreo.

\section{Propagación de errores}
\vspace{0.5cm}
\noindent \textbf {Regla 1} \\
\noindent Si $x$ e $y$ tienen errores aleatorios independientes 
$\delta{x}$ y $\delta{y}$,nentonces el error en la suma $z=x+y$ es

$$\delta{z}=\sqrt{\delta{x}^2 + \delta{y}^2}\,.$$

\vspace{0.5cm}
\noindent \textbf {Regla 2}
\\
\noindent Si $x$ e $y$ tienen errores aleatorios independientes $\delta{x}$ y $\delta{y}$,
entonces el error en la multiplicación $z=xy$ es
$$\frac{\delta{z}}{z}=\sqrt{\left(\frac{\delta{x}^2}{x}\right) + \left(\frac{\delta{y}^2}{y}\right)}\,.$$

\vspace{0.5cm}
\noindent \textbf {Regla 3}\\

\noindent Si $z=f(x)$, donde $f()$ es una función dada, entonces
$$\delta{z}=|f'(x)|\delta{x}\,.$$

\vspace{0.5cm}

\noindent \textbf {Formula general para propagación del error}

\noindent Sea $x_1, x_2,....,x_N$ medidas con incertidumbres $\delta{x_1},\delta{x_2},....,\delta{x_3}$.
Supongamos que queremos determinar $q$, el cual es una función de $x_1,x_2,...,x_N$:
$$q=f(x_1, x_2,...,x_N)\,.$$
El error asociado a $q$ es entonces
$$\delta{q}=\sqrt{\left( \frac{\partial{q}}{\partial{x_1}}\delta{x_1}\right)^2 + ... +
                  \left( \frac{\partial{q}}{\partial{x_N}}\delta{x_N}\right)^2}$$
\\
\\
\noindent Si $q=x_1+x_2$ entonces obtenemos la regla 1:
$$\frac{\partial{q}}{\partial{x_1}}=1\,,$$
$$\frac{\partial{q}}{\partial{x_2}}=1\,,$$
$$\delta{q}=\sqrt{\delta{x_1}^2 + \delta{x_2}^2}\,.$$
\\
\\
\noindent Si $q=x_1x_2$ entonces obtenemos la regla 2:
$$\frac{\partial{q}}{\partial{x_1}}=x_2\,,$$
$$\frac{\partial{q}}{\partial{x_2}}=x_1\,,$$
$$\delta{q}=\sqrt{x^2_2\delta{x_1}^2 + x^2_1\delta{x_2}^2}=\sqrt{q^2\left[ \left(\frac{\partial{x_1}}{x_1}\right)^2 + \left(\frac{\partial{x_2}}{x_2}\right)^2\right]}\,.$$
$$\frac{\delta{q}}{q}=\sqrt{\left(\frac{\delta{x_1}^2}{x_1}\right) + \left(\frac{\delta{x_2}^2}{x_2}\right)}\,.$$

\vspace{0.5cm}
\noindent \textbf {Demstración:}
\\
\noindent Queremos calcular la desviación estándar de la
función
$$q=q(x_1, x_2,....,x_N)\,,$$
que depende de $N$ variables independientes
$x_1$, $x_2$, ....,$x_N$. El desarrollo de
Taylor de la función $q$ alrededor de la media
$\bar{q}$ se puede escribir:
$$q - \bar{q}=\left(x_1 - \bar{x}_1\right)\frac{\partial{q}}{\partial{x_1}} +
                          \left(x_2 - \bar{x}_2\right)\frac{\partial{q}}{\partial{x_2}}+  \,.\,.\,.\,
                          + \left(x_N - \bar{x}_N\right)\frac{\partial{q}}{\partial{x_N}}\,.$$
La varianza de la función $q$ es:
$$s^2_q=\frac{1}{N-1}\sum\limits^N_{i=1}\left( q_i-\bar{q} \right)^2=
               \frac{1}{N-1}\left[
               \left(x_1 - \bar{x}_1\right)\frac{\partial{q}}{\partial{x_1}} +
               \left(x_2 - \bar{x}_2\right)\frac{\partial{q}}{\partial{x_2}} +
              \,.\,.\,.\, +
               \left(x_N - \bar{x}_N\right)\frac{\partial{q}}{\partial{x_N}}
               \right]^2=$$
$$=\frac{1}{N-1}\left[
\left(x_1 - \bar{x}_1\right)^2\left(\frac{\partial{q}}{\partial{x_1}}\right)^2 +
\left(x_2 - \bar{x}_2\right)^2\left(\frac{\partial{q}}{\partial{x_2}}\right)^2 +
2\left(x_1 - \bar{x}_1\right)\left(x_2 - \bar{x}_2\right)
\frac{\partial{q}}{\partial{x_1}}\frac{\partial{q}}{\partial{x_2}}+\,.\,.\,.
\right]=$$
$$=s^2_{x_1}\left(\frac{\partial{q}}{\partial{x_1}}\right)^2 +
   s^2_{x_2}\left(\frac{\partial{q}}{\partial{x_2}}\right)^2+
   2 s_{{x_1}{x_2}}
   \frac{\partial{q}}{\partial{x_1}}\frac{\partial{q}}{\partial{x_2}}+\,.\,.\,.$$
y finalmente la expresión general para la propagación del
error es:
$$s_{q}=\sqrt{s^2_{x_1}\left(\frac{\partial{q}}{\partial{x_1}}\right)^2 +
               s^2_{x_2}\left(\frac{\partial{q}}{\partial{x_2}}\right)^2+
                2 s_{{x_1}{x_2}}
                \frac{\partial{q}}{\partial{x_1}}\frac{\partial{q}}{\partial{x_2}}+\,.\,.\,.}$$

\vspace{0.5cm}
{\noindent \textbf Ejemplo:}
\\
\noindent La ecuación para el cálculo de la salinidad a partir de la
conductividad (C) y temperatura (T) es (Unesco EOS-80)
$$S=a_0+a_1 R_T^{1/2}+a_2 R_T+a_3 R_T^{3/2}+a_4 R_T^{2}+a_5 R_T^{5/2}+\Delta{S}\,,$$
donde
$$R_T=\frac{R}{R_p r_t}\,\,\,,\,\,R=\frac{C(S,T,0)}{C(35,15,0)}\,,$$
$C(35,15,0)$ es la conductividad de un agua de salinidad práctica
35 a los $15\,^\circ{\rm C}$,
$$r_t=c_0 + c_1 T + c_2 T^2 + c_3T^3 + c_4T^4\,,$$
$$R_p=1+\frac{P(e_1+e_2P+e_3P^2)}{(1+d_1T+d_2T^2+(d_3 + d_4T)R)}\,,$$
y
$$\Delta{S}=\frac{T-15}{1+k(T-15)}(b_0+b_1 R_T^{1/2}+b_2 R_T+b_3 R_T^{3/2}+b_4 R_T^{2}+b_5 R_T^{5/2})\,,$$
con los coeficientes $a_i\,,b_i\,,c_i\,,d_i\,,\,\,y\,e_i$\\
$a_0=0.0080\,\,\,\,\,\,\,\,\,\,\,\,\,\,b_0=0.0005\,\,\,\,\,\,\,\,\,\,\,\,\,\,c_0=0.6766097\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,d_1=3.426\,e^{-2}\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,e_1=2.070\,e^{-5}$ \\
$a_1=-0.1692\,\,\,\,\,\,\,\,\,\,b_1=-0.0056\,\,\,\,\,\,\,\,\,c_1=2.00564\,e^{-2}\,\,\,\,\,\,\,\,\,\,\,d_2=4.464\,e^{-4}\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,e_2=-6.370\,e^{-10}$ \\
$a_2=25.3851\,\,\,\,\,\,\,\,\,\,\,\,b=-0.0066\,\,\,\,\,\,\,\,\,\,\,c_2=1.104259\,e^{-4}\,\,\,\,\,\,\,\,\,d_3=4.215\,e^{-1}\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,e_3=3.989\,e^{-15}$ \\
$a_3=14.0941\,\,\,\,\,\,\,\,\,\,\,\,b=-0.0375\,\,\,\,\,\,\,\,\,\,\,c_3=-6.9698\,e^{-7}\,\,\,\,\,\,\,\,\,\,\,d_4=-3.107\,e^{-3}$ \\
$a_4=-7.0261\,\,\,\,\,\,\,\,\,\,b_4=0.0636\,\,\,\,\,\,\,\,\,\,\,\,\,\,c_4=1.0031\,e^{-9}$ \\
$a_5=2.7081\,\,\,\,\,\,\,\,\,\,\,\,\,\,b_5=-0.0144\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,$\\
$\sum a_i=35.0000\,\,\,\,\sum b_i=0.0000$ \\
$k=0.0162$
\\
\\
\noindent Si el error de precisión del termistor y de la celda de
conductividad del CTD es $\delta{T}=0.001\,^\circ{\rm C}$ y
$\delta{C}=0.001\,{\rm S}{\rm m}^{-1}$, respectivamente,
calcule la incertidumbre asociada al cálculo de la salinidad
$S$ a partir de la formula general de propagación del error
para $C=5\,{\rm S}\,{\rm m}^{-1}$,
$T=28\,^\circ\,{\rm C}$, y
$P=50\,{\rm dbar}$. Suponga que $\delta{P}=0$, es decir,
el altímetro no tiene errores de precisión.
\\
\\
{\textbf Resultado:}
\\
\\
\noindent El error estándar asociado a la salinidad es
$$\delta{S}=\sqrt{\left( \frac{\partial{S}}{\partial{T}}\delta{T} \right)^2 +
                  \left( \frac{\partial{S}}{\partial{C}}\delta{C}\right)^2}\,,
		  $$
donde
$$\frac{\delta{S}}{\delta{T}}=
      \left[(1+k(T-15))^{-1} - (k(T-15))(1+k(T-15))^{-2}\right]$$
      $$\left (b_0+b_1 R_T^{1/2}+b_2 R_T+b_3 R_T^{3/2}+b_4 R_T^{2}+b_5 R_T^{5/2}\right)\,,$$
y
$$\frac{\delta{S}}{\delta{C}}=
                 \frac{1}{2} a_1 \frac{R_T^{-1/2}}{C(35,T,0)} +
		 a_2 \frac{1}{C(35,T,0)} +
	         \frac{3}{2}a_3 \frac{R_T^{1/2}}{C(35,T,0)}+ $$
               $$+2 a_4 \frac{R_T}{C(35,T,0)} +\frac{5}{2} a_5 \frac{R_T^{3/2}}{C(35,T,0)}
     +\frac{T-15}{(1+k(T-15))}$$
     $$\left(
        b_1 \frac{R_T^{-1/2}}{C(35,T,0)} +
		 b_2 \frac{1}{C(35,T,0)} +
	         \frac{3}{2}b_3 \frac{R_T^{1/2}}{C(35,T,0)}+
               2 b_4 \frac{R_T}{C(35,T,0)} +
     +\frac{5}{2} b_5 \frac{R_T^{3/2}}{C(35,T,0)}
     \right)\,.$$
\\
\\
\noindent Por lo tanto, dada una medida de conductividad $C$ y temperatura
$T$ podemos calcular ${\delta{S}}/{\delta{T}}$ y ${\delta{S}}/{\delta{C}}$
y obtener la desviación estándar asociado al cálculo de la salinidad
$\delta{S}$.
\\
\\
{\textbf Ejercicio:}
\\
\\
\noindent Calcule el error estándar asociado a la densidad referenciada a la superfície
que se obtiene al usar el algoritmo del estado del agua de mar (Unesco EOS-80)
$$\rho(S,T,0)=\rho_w + \left(b_0+b_1 T+b_2 T^2+b_3 T^{3}+b_4 T^{4}\right) S +$$
$$+\left(c_0+c_1 T+c_2 T^2\right) S^{3/2} + d_0 S^2\,,$$
con los coeficientes $b_i$, $c_i$, y $d_0$\\
$b_0=8.24493\,e^{-1}\,\,\,\,\,\,\,\,\,\,\,\,\,\,c_0=-5.72466\,e^{-3}$ \\
$b_1=-4.0899\,e^{-3}\,\,\,\,\,\,\,\,\,\,\,\,c_1=1.0227\,e^{-4}$ \\
$b_2=7.6438\,e^{-5}\,\,\,\,\,\,\,\,\,\,\,\,c_2=-1.6546\,e^{-6}$ \\
$b_3=-8.2467\,e^{-7}\,\,\,\,\,\,\,\,\,\,\,\,$ \\
$b_4=5.3875\,e^{-9}\,\,\,\,\,\,\,\,\,\,\,\,d_0=4.8314\,e^{-4}$\,,\\
\\
y la densidad de referencia para agua pura ($\rho_w$) definida como:
$$\rho_w=a_0+a_1 T+a_2 T^2+a_3 T^{3}+a_4 T^{4}+a_5 T^{5}\,,$$
donde los coeficientes $a_i$ son:
\\
$a_0=999.842594$ \\
$a_1=6.793952\,e^{-2}$ \\
$a_2=-9.095290\,e^{-3}$ \\
$a_3=1.001685\,e^{-4}$ \\
$a_4=-1.120083\,e^{-6}$\\
$a_5=6.536332\,e^{-9}$.

\section{Métodos de Interpolación}
Interpolación es el procedimiento para el cual obtenemos
valores de propiedades en posiciones o tiempos que nunca
fueron muestreados a partir de observaciones existentes
en otras localizaciones o tiempos. En oceanografía necesitamos
interpolar (i) para rellenar huecos cuando el instrumento dejo de
medir, (ii) para obtener mapas espaciales de contornos (2D), (iii)
para calcular alguna propiedad derivada en un punto concreto

\subsection{Interpolación Lineal}

La interpolación mas sencilla, pero no por ello peor, es la interpolación lineal.
Esta interpolación se basa en ajustar una línea recta entre los puntos conocidos,
e interpolar cualquier punto intermedio como un punto a lo largo de la recta. Este
tipo de interpolación puede ser usado para rellenar huecos en nuestras series
temporales. La interpolación lineal de unas serie y(t) se puede escribir como
$$y(t_i)=y(t_0)+\frac{y(t_1)-y(t_0)}{t_1 - t_0}(t_i-t_0)\,.$$


\subsection{Interpolación polinómica}

En el caso que queramos interpolar entre mas de dos puntos simultaneamente, debemos
de usar polinomios de orden superior a la recta (orden 1). Es decir,
$$y(x)=a_0 + a_1 x + a_2 x^2 + ... + a_{n-1}x^{n-1} + a_n x_n\,.$$
Si tomamos $N$ observaciones ($y_i(x)\,\,;i=0,2,...,N-1$) obtenemos
un sistema de $N$ ecuaciones
$$\left(\begin{array}{ccccc}
  1 & x_0 & ... & x_0^{N-1} & x_0^N\\
  1 & x_1 & ... & x_1^{N-1} & x_1^N\\
  . & . & . & . & .\\
  . & . & . & . & .\\
  . & . & . & . & .\\
  1 & x_N & ... & x_N^{N-1} & x_N^N\\
     \end{array}\right)
    \left(\begin{array}{c}
  a_0 \\
  a_1 \\
  . \\
  . \\
  . \\
  a_N \\
     \end{array}\right)=
     \left(\begin{array}{c}
  y_0 \\
  y_1 \\
  . \\
  . \\
  . \\
  y_N \\
     \end{array}\right)\,,$$
lo cual se puede resolver con eliminación Gauss-Jordan. Este método es
muy lento y por ello se usa el método de Lagrange.


\vspace{0.5cm}
{\noindent \textbf Método de Lagrange}

De nuevo asumimos
$$p(x)=a_0 + a_1 x + a_2 x^2 + ... + a_{n-1}x^{n-1} + a_n x_n=\sum\limits^{N}_{k=0} a_k x^k
=\sum^{N+1}_{i=1} y_i {\cal L}_i(x)\,,$$
donde
$${\cal L}_i(x)=\prod^{N+1}_{\substack{k=1\\ k\ne i}} \frac{x-x_k}{x_i-x_k}\,,$$
son los denominados polinomios de Lagrange, y $\prod$ es el operador producto.
Puesto que este operador cuando $k\ne i$ no incluye el producto, a pesar que varíe de
$1$ a $N+1$, obtendremos un polinomio de orden $N$.

Esta suma de polinomios de Lagrange es el
polinomio de menor grado que interpola un conjunto de datos,
es decir,
$$p(x_j)=\sum^{N+1}_{i=1} y_i {\cal L}_i(x_j)=y_j\,.$$
{\textbf Demostración:}

Los polinomios de Lagrange para $i\ne j$ son iguales a
cero, y para $i=j$ son iguales a 1. Veamos esto:

$${\rm (1)}\,\,\,\,\,{\rm Para}\,\,\,{i \ne j}:\,\,\,\,\,{\cal L}_i(x_j)=\prod^{N+1}_{\substack{k=1\\ k\ne i}} \frac{x_j-x_k}{x_i-x_k}=$$
$$=\frac{x_j-x_1}{x_i-x_1} \frac{x_j-x_2}{x_i-x_2} ...\frac{x_j-x_j}{x_i-x_j}....\frac{x_j-x_{N+1}}{x_i-x_{N+1}}$$

$${\rm (2)}\,\,\,\,\,{\rm Para}\,\,\,{i = j}:\,\,\,\,\,{\cal L}_i(x_j)=\prod^{N+1}_{\substack{k=1\\ k\ne i}} \frac{x_j-x_k}{x_j-x_k}=1$$

De modo que
$${\cal L}_i(x_j)=\delta_{ij}=\begin{cases}
\begin{array}{c}
   1 \,\,\,\,\text{si}\,\,\,\,i= j\\
   0 \,\,\,\,\text{si}\,\,\,\,i\ne j\\
\end{array}
\end{cases}\,.$$

Finalmente podemos concluir entonces que
$$p(x_j)=\sum^{N+1}_{i=1} y_i {\cal L}_i(x_j)=\sum^{N+1}_{i=1} y_i \delta_{ij}=y_j\,.$$
es un polinomio de grado no mayor a $N$ y que $p(x_j)=y_j$.

El polinomio se puede reescribir en terminos de la función $Q_i$ como
$$p(x)=\sum^{N+1}_{i=1} y_i[Q_i(x)/Q_i(x_i)]\,,$$
donde
$$Q_i(x)=(x-x_1)(x-x_2)....(x-x_{i-1})(x-x_{i+1})...(x-x_{N-1})\,,$$
es el producto de todas las diferencias excepto la posición $i$ (i.e., $x-x_{i}$).
Si expandemos $p(x)$
$$p(x)=y_1\frac{(x-x_2)(x-x_3)...(x-x_{N+1})}{(x_1-x_2)(x_1-x_3)...(x_1-x_{N+1})} +
       y_2\frac{(x-x_1)(x-x_3)...(x-x_{N+1})}{(x_2-x_1)(x_2-x_3)...(x_2-x_{N+1})} +$$
$$+...+y_{N+1}\frac{(x-x_1)(x-x_2)...(x-x_{N})}{(x_{N+1}-x_1)(x_{N+1}-x_2)...(x_{N+1}-x_{N})}\,.$$

{\noindent \textbf Ejemplo:}

Considere los puntos (0,2), (1,2), (2,0) y (3,0) para los cuales
queremos ajustar un polinomio de orden 3
$$y(x)=2\frac{(x-1)(x-2)(x-3)}{(0-1)(0-2)(0-3)} + 2\frac{(x-0)(x-2)(x-3)}{(1-0)(1-2)(1-3)} + 0
+ 0=$$
$$=\frac{2}{3}x^3 -3x^2 +\frac{7}{3}x +2\,.$$
<!--
your comment goes here
\begin{center}
%\includegraphics[width=0.7\textwidth]{Lagrange_interpolation.pdf}
\end{center}
-->

\subsection{Spline cúbico}

La interpolación por spline cúbicos es un método de
ajuste de polinomios de orden 3 por segmentos. Veamos este
método con un ejemplo.

Supongamos que queremos interpolar los siguientes datos
$(x_i,y_i)=[(2,-1), (3,2), (5,-7)]$ con una spline cúbica.
Primero definimos un polinomio cúbico para cada intervalo:
$$s(x)=a_1x^3 + b_1 x^2 +c_1x +d_1\,\,\,\,\,{\rm si}\,\,\,\,\,x\in[2,3]$$
$$s(x)=a_2x^3 + b_2 x^2 +c_2x +d_2\,\,\,\,\,{\rm si}\,\,\,\,\,x\in[3,5]$$

A continuación debemos de asegurar que los polinomios pasan por
los puntos del problema:
$$s(2)=8a_1 + 4b_1  +2c_1 +d_1=-1$$
$$s(3)=27a_1 + 9b_1  +3c_1 +d_1=2$$
$$s(3)=27a_2 + 9b_2  +3c_2 +d_2=2$$
$$s(5)=125a_2 + 25b_2  +5c_2 +d_2=-7$$

Ahora calculamos la primera y segundas derivadas para cada intervalo
$$s'(x)=3a_1x^2+2b_1x+c_1\,\,\,\,\,{\rm si}\,\,\,\,\,x\in[2,3]$$
$$s'(x)=3a_2x^2+2b_2x+c_2\,\,\,\,\,{\rm si}\,\,\,\,\,x\in[3,5]$$
$$s''(x)=6a_1x+2b_1\,\,\,\,\,{\rm si}\,\,\,\,\,x\in[2,3]$$
$$s''(x)=6a_2x+2b_2\,\,\,\,\,{\rm si}\,\,\,\,\,x\in[3,5]$$
y aseguramos que sean contínuas. Para ello debemos de igualar las
derivadas entre intervalos de manera que no hayan discontinuidades
$$3a_1(3)^2+2b_1(3)+c_1=3a_2(3)^2+2b_2(3)+c_2\rightarrow 27a_1+6b_1+c_1=27a_2+6b_2+c_2$$
$$6a_1(3)+2b_1=6a_2(3)+2b_2\rightarrow 18a_1+2_b1=18a_2+2_b2$$

En este momento tenemos 6 equaciones y 8 incógnitas. Debemos
por lo tanto encontrar dos ecuaciones mas. Para ello vamos a
forzar que en los extremos la segunda derivada sea nula, es decir,
no haya curvatura
$$s''(x_0)=s''(2)=0\rightarrow 6a_1(2)+2b_1=0 \rightarrow 12a_1 + 2b_1=0$$
$$s''(x_N)=s''(5)=0\rightarrow 6a_2(5)+2b_2=0 \rightarrow 30a_2+2b_2=0$$

Ahora ya tenemos un sistema determinado, es decir, 8 ecuaciones y
8 incógnitas
$$8a_1 + 4b_1  +2c_1 +d_1=-1$$
$$27a_1 + 9b_1  +3c_1 +d_1=2$$
$$27a_2 + 9b_2  +3c_2 +d_2=2$$
$$125a_2 + 25b_2  +5c_2 +d_2=-7$$
$$27a_1+6b_1+c_1=27a_2+6b_2+c_2$$
$$18a_1+2b_1=18a_2+2b_2$$
$$12a_1 + 2b_1=0$$
$$30a_2+2b_2=0$$

Lo cual en notación matricial se puede escribir

$$\left(\begin{array}{cccccccc}
  8 & 4 & 2 & 1 & 0 & 0 & 0 & 0\\
  27 & 9 & 3 & 1 & 0 & 0 & 0 & 0\\
  0 & 0 & 0 & 0 & 27 & 9 & 3 & 1\\
  0 & 0 & 0 & 0 & 125 & 25 & 5 & 1\\
  27 & 6 & 1 & 0 & -27 & -6 & -1 & 0\\
  18 & 2 & 0 & 0 & -18 & -2 & 0 & 0\\
  12 & 2 & 0 & 0 & 0 & 0 & 0 & 0\\
  0 & 0 & 0 & 0 & 30 & 2 & 0 & 0\\
    \end{array}\right)\left(\begin{array}{c}
  a_1 \\
  b_1 \\
  c_1 \\
  d_1 \\
  a_2 \\
  b_2 \\
  c_2 \\
  d_2 \\
    \end{array}\right)=
    \left(\begin{array}{c}
  -1 \\
  2 \\
  2 \\
  -7 \\
  0 \\
  0 \\
  0 \\
  0 \\
    \end{array}\right)$$

Este sistema se puede resolver facilmente si la matriz de
datos es invertible. Los coeficientes resultantes son
$$a_1=-1.25\,\,\,\,\,b_1=7.5\,\,\,\,c_1=-10.75\,\,\,\,d_1=0.5$$
$$a_1=0.625\,\,\,\,\,b_1=-9.375\,\,\,\,c_1=39.875\,\,\,\,d_1=-50.125\,,$$
y los polinomios de nuestra spline cúbica es
$$s(x)=-1.25x^3+7.5x^2-10.75x+0.5\,\,\,\,\,{\rm si}\,\,\,\,\,x\in[2,3]$$
$$s(x)=0.625x^3+-9.375x^2-39.875x+50.125\,\,\,\,\,{\rm si}\,\,\,\,\,x\in[3,5]$$

<!--
your comment goes here
\begin{center}
%\centering
%\includegraphics[width=10cm,angle=0]{cubic_spline.pdf}
\end{center}
-->

<!--
your comment goes here
\begin{center}
%\centering
%\includegraphics[width=15cm,angle=0]{poly_Lagrange_Spline.pdf}
\end{center}
-->
\centerline{Figura. Comparación entre ajuste de un polinomio de orden 6
por método de}
\centerline{cuadrados mínimos, interpolación de Lagrange, e interpolación
spline cúbica.}


\subsection{Interpolación Objetiva}

Un mapa objetivo se obtiene como una regresión múltiple
(donde el error cuadrático medio es mínimo)
de un conjunto de observaciones discretas. Se utiliza en oceanografía
para obtener mapas continuos (mallas regulares) a partir de datos
discretos distribuidos irregularmente en el espacio. Las
variables representadas en el mapa objetivo pueden modificarse
de una realización a otra, con lo que deben de considerarse
las anomalías de las variables en lugar de las variables en si
mismas. Se debe de definir un ensemble (promedio),
climatología, o candidato y extraerlo a cada variable para obtener
anomalías. Este proceso de elección de la media es una parte delicada
de la interpolación objetiva. En general, para el océano, la media
es desconocida ya que solamente tenemos pocas realizaciones de
nuestro muestreo. Una forma de operar es estimar la media ajustando
un polinomio de bajo orden a nuestros datos discretos,
extraer esta a los datos y proceder con la interpolación objetiva.
La media se añade de nuevo despues de calcular el mapa objetivo de
las fluctuaciones.
\\
\\
Típicamente se debe asumir dos condiciones:
\\
\\
(1) El error (o ruido) asociado a la interpolación
no esta correlacionado con nuestra señal (variable a interpolar)
$$<\phi_i\epsilon_i>=0$$
\\
(2) El error no esta correlacionado de una estación a la otra
$$<\epsilon_i \epsilon_j>=
\begin{cases}
\begin{array}{c}
   <\epsilon^2> \,\,\,\,\text{si} \,\,\,\,i=j\\
   0    \,\,\,\,        \text{si} \,\,\,\,i\ne j\\
\end{array}
\end{cases}$$

Supongamos entonces fluctuaciones respecto un estado climatológico
o media
$$\phi_i'=\phi_i-\bar{\phi}\,; i=1,2,...,N$$
Ahora vamos a intentar aproximar el valor de $\phi'$ en
un punto de una malla, $\phi_g$, en términos de una
combinación lineal de los valores en estaciones vecinas
$\phi_i$ (señal). Entonces el problema de
mínimos cuadrados es
$$\phi'_g=\sum\limits^N_{i=1}b_i\phi_i'$$
donde $\phi'_g$ son anonmalías en la malla regular y $\phi_i'$ son
anomalías en las estaciones. Los mejores coeficientes son aquellos que
minimizan el error cuadrático medio, es decir,
$$SEC=\sum\limits^N_{i=1}\left(\phi'_g-\sum\limits_{i=1}^N b_i\phi'_i \right)^2
     =\sum\limits^N_{i=1}\left(\phi'_g-\sum\limits_{i=1}^N b_i\phi'_i \right)\left(\phi'_g-\sum\limits_{i=1}^N b_i\phi'_i \right)=$$
$$\sum\limits^N_{i=1}\left[\phi'_g \phi'_g -2\sum\limits_{i=1}^N b_i\phi'_g\phi'_i
      +\sum\limits^N_{i=1}\sum\limits^N_{j=1} b_i b_j \phi'_i \phi'_j \right]=$$
$$=\sum\limits^N_{i=1}\phi'_g \phi'_g -2 \sum\limits^N_{i=1} b_i \sum\limits^N_{i=1} \phi'_g\phi'_i + \sum\limits^N_{i=1}\sum\limits^N_{j=1}
b_i b_j \sum\limits^N_{i=1} \phi'_i \phi'_j\,.$$
El error normalizado se puede escribir como
$$\epsilon=\frac{SEC}{\sum\limits^N_{i=1}\phi'_g \phi'_g}=1-2 \sum\limits^N_{i=1} b_i \frac{\sum\limits^N_{i=1} \phi'_g\phi'_i}{\sum\limits^N_{i=1}\phi'_g \phi'_g} + \sum\limits^N_{i=1}\sum\limits^N_{j=1} b_i b_j \frac{\sum\limits^N_{i=1} \phi'_i \phi'_j}{\sum\limits^N_{i=1}\phi'_g \phi'_g}=$$
$$=1-2 \sum\limits^N_{i=1} b_i r_{gi} + \sum\limits^N_{i=1}\sum\limits^N_{j=1} b_i b_j r_{ij}\,,$$
donde
$$r_{gi}=\frac{\sum\limits^N_{i=1} \phi'_g\phi'_i}{\sum\limits^N_{i=1}\phi'_g \phi'_g}\,\,\,\,\,{\rm y}\,\,\,\,\,r_{ij}=\frac{\sum\limits^N_{i=1} \phi'_i \phi'_j}{\sum\limits^N_{i=1}\phi'_g \phi'_g}\,,$$
son matrices de correlación entre el punto de malla y las estaciones, y entre las estaciones,
respectivamente.

Si derivamos $\epsilon$ respecto los coeficientes, obtenemos la condición de
minimización:
$$\frac{\partial{\epsilon}}{\partial{b_i}}=-2 r_{gi} + 2\sum\limits^N_{j=1}b_j r_{ij}=0\,\,\,\,\,\,\,\,\,i=1,2,...,N\,.$$
$$r_{gi}=\sum\limits^N_{j=1}b_j r_{ij}\,,$$
y los coeficientes en notación índice son
$$b_j=(r_{ij})^{-1}r_{gi}\,$$

Finalmente, el valor de la medida en el punto de malla es

$$\phi'_g=r_{gi}(r_{ij})^{-1}\phi_i'$$
o en notación matricial para un único punto de malla
$$\phi'_g=r_{gs}(r_{ss})^{-1}\phi'\,,$$
donde $\phi'_g$ es el valor de la anomalía
en un punto de malla (matriz elemento, $1\times 1$),
$r_{gs}$ es un
vector fila compuesto por las correlaciones
entre el punto de malla y las estaciones de medida,
$r_{ss}$ es una matriz de correlaciones entre todas las
estaciones, y $\phi'$ es un vector columna con las
anomalías en las estaciones.

En el caso de que tengamos $N$ puntos de malla y $k$ estaciones de
medida, las ecuaciones básicas de la interpolación objetiva se
pueden escribir en notación matricial como
$$\left(\begin{array}{c}
  \phi'_{g_1} \\
  \phi'_{g_2} \\
  . \\
  . \\
  . \\
  \phi'_{g_N} \\
     \end{array}\right)=
\left(\begin{array}{ccccc}
  r_{g_11} & r_{g_12} &... &  r_{g_1k}\\
  r_{g_21} & r_{g_22} &... &  r_{g_2k}\\
  . & . & . & . \\
  . & . & . & . \\
  . & . & . & . \\
  r_{g_N1} & r_{g_N2} & ... & r_{g_Nk}\\
     \end{array}\right)
 \left(\begin{array}{ccccc}
  r_{11} & r_{12} &... &  r_{1k}\\
  r_{21} & r_{22} &... &  r_{2k}\\
  . & . & . & . \\
  . & . & . & . \\
  . & . & . & . \\
  r_{k1} & r_{k2} & ... & r_{kk}\\
     \end{array}\right)^{-1}       \left(\begin{array}{c}
  \phi'_1 \\
  \phi'_2 \\
  . \\
  . \\
  . \\
  \phi'_k \\
     \end{array}\right)
     \,.$$
$$\left( N \times 1 \right)\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
  \left( N \times k \right)\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
  \,\,\,\,\,\,\,\,\left( k \times k \right)\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
  \left( k \times 1 \right)\,\,\,\,\,$$
\\

El error normalizado $\epsilon$ asociado al mapa interpolado es
$$\epsilon=1-2 \sum\limits^N_{j=1} b_j r_{gi} + \sum\limits^N_{i=1}\sum\limits^N_{j=1} b_i b_j r_{ij}=
           1-2 \sum\limits^N_{j=1} b_j \sum\limits^N_{j=1} b_j r_{ij} + \sum\limits^N_{i=1}\sum\limits^N_{j=1} b_i b_j r_{ij}=$$
$$=1-2\sum\limits^N_{i=1}\sum\limits^N_{j=1} b_i b_j r_{ij} + \sum\limits^N_{i=1}\sum\limits^N_{j=1} b_i b_j r_{ij} =1-\sum\limits^N_{i=1}\sum\limits^N_{j=1} b_i b_j r_{ij}=$$
$$= 1- \sum\limits^N_{j=1} b_j \sum\limits^N_{j=1} b_j r_{ij}  =1-\sum\limits^N_{i=1}r_{gi}b_i\,,$$

o en notación matricial para un único punto de malla
$$\epsilon=1-r_{gs}(r_{ss})^{-1}r^T_{gs}\,.$$

En el caso de $N$ puntos de malla y $k$ estaciones
$$\left(\begin{array}{ccc}
  \epsilon_{1} \\
  \epsilon_{2} \\
  . \\
  . \\
  . \\
  \epsilon_{N} \\
     \end{array}\right)=\text{Diag}\left[
     {\textbf I}-
\left(\begin{array}{ccccc}
  r_{g_11} & r_{g_12} &... &  r_{g_1k}\\
  r_{g_21} & r_{g_22} &... &  r_{g_2k}\\
  . & . & . & . \\
  . & . & . & . \\
  . & . & . & . \\
  r_{g_N1} & r_{g_N2} & ... & r_{g_Nk}\\
     \end{array}\right)
 \left(\begin{array}{ccccc}
  r_{11} & r_{12} &... &  r_{1k}\\
  r_{21} & r_{22} &... &  r_{2k}\\
  . & . & . & . \\
  . & . & . & . \\
  . & . & . & . \\
  r_{k1} & r_{k2} & ... & r_{kk}\\
     \end{array}\right)^{-1}
\left(\begin{array}{ccccc}
  r_{g_11} & r_{g_21} &... &  r_{g_N1}\\
  r_{g_12} & r_{g_22} &... &  r_{g_N2}\\
  . & . & . & . \\
  . & . & . & . \\
  . & . & . & . \\
  r_{g_1k} & r_{g_2k} & ... & r_{g_Nk}\\
     \end{array}\right) \right]
     \,.$$
$$\left( N \times 1 \right)\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
  \left( N \times k \right)\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
  \,\,\,\,\,\,\,\,\left( k \times k \right)\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
  \left( k \times N \right)\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,$$

Este error es tal vez la característica mas importante de la
interpolación objetiva. En general solo se muestra el mapa
interpolado en las regiones donde $\epsilon$ emenor que un
cierto valor. El error solo depende en las localizaciones
de las estaciones y no en un valor particular de la medida. Es por ello
que esta técnica puede ser usada para el diseño de experimentos.
Es decir, nos sirve para saber cual debe ser la distribución óptima de
las estaciones de medida para obtener el error mínimo en el mapa.
\\

{\textbf Nota:} Si un punto de observación, $i=k$, coincide con un punto de
malla, entonces $r_{gk}=r_{kk}=1$, y esperamos que el método de regresión
nos de $b_k=1$ y todos los demas pesos sean igual a cero. En este
caso el valor interpolado en el punto de malla es igual al valor medido
en la estación $\phi'_g = r_{gk} (r_{kk})^{-1} \phi'_k =1(1)^{-1}\phi'_k=\phi'_k $ .
En este caso el error es cero, $\epsilon=1-r_{gk}(r_{kk})^{-1}r^T_{gk}=1-1(1)^{-1}1=0$, ya que hemos asumido que los
datos son perfectos. Si los puntos de las estaciones estan decorrelacionados
con el punto de malla en cuestión (i.e., muy lejos de las estaciones),
entonces $b_i=0$ y $\epsilon=1$, y recuperamos la media o climatología
$$\phi'_g=\sum\limits^N_{i=1}b_i\phi'_i=0\,\,\rightarrow\,\,\phi'_g=\phi_g-\bar{\phi}=0\,\,\,\,\,\text{and}\,\,\,\,\,\phi_g=\bar{\phi}$$

{\noindent \textbf Error observacional:}

Vamos asumir ahora que las mediciones en las estaciones no son
perfectas, es decir,
$$\phi'_i=E[\phi'_i] + \delta_i\,.$$
Igual que anteriormente asumimos que el error de instrumentación
$\delta_i$ no esta correlacionado con la señal verdadera
$$<E[\phi_i]\delta_i>=0\,,$$
y que el error instrumental entre estaciones tampoco esta correlacionado
$$<\epsilon_i \epsilon_j>=
\begin{cases}
\begin{array}{c}
   <\delta^2> \,\,\,\,\text{si}\,\,\,\,i=j\\
        0     \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\text{si}\,\,\,\,i\ne j\\
\end{array}
\end{cases}$$

En este caso obtenemos
$$\epsilon=1-2 \sum\limits^N_{i=1} b_i r_{gi} + \sum\limits^N_{i=1}\sum\limits^N_{j=1} b_i b_j r_{ij} + \eta\sum\limits_{i=1}^N b_i^2\,,$$
donde $\eta$ es el cociente entre la varianza del error (ruido) y la varianza de las medidas, es decir
el cociente ruido-señal
$$\eta=\frac{<\delta^2>}{<\phi'_g\phi'_g>}\,.$$

La minimización del error nos da la condición
$$r_{gi}=\sum\limits^N_{j=1}b_j r_{ij}+\eta b_i\,,$$
y los coeficientes en notación índice son
$$b_j=(r_{ij} + \eta I_{ij})^{-1}r_{gi}\,,$$
donde $I_{ij}$ es la matriz identidad. De esta expresión se
deduce que cuando $\eta$ es grande (mucho ruido en la medida)
entonces los coeficientes son mas pequeños respecto al
caso de observaciones perfectas. Consecuentemente
nuestro mapa interpolado se acerca mas a la climatología
ya que la anomalía es menor para coeficientes mas
pequeños
$$\downarrow \sum\limits^N_{i=1}b_i\phi'_i \,\,\,\,\,\rightarrow \,\,\,\,\,\downarrow\phi'_g\,\,\,\,\,\,\text{y}\,\,\,\,\,\phi_g\rightarrow\bar{\phi}\,.$$
Incluyendo errores observacionales, la interpolación objetiva
tendera a la climatología o media y las nuevas observaciones serán
incluidas pero con menos pesos. También podemos añadir diferentes
errores ruido-señal en la diagonal principal para darle menos
peso a las estaciones de medida que tienen mas incertidumbre
asociada. Es conveniente entonces añadir errores observacionales
al esquema de interpolación objetiva. Otra razón para ello es
el caso en que existan dos estaciones que coincidan
exactamente con un punto de malla. En este caso, si no hemos añadido
error observacional la matriz de correlaciones entre las estaciones
se convierte singular y el esquema de interpolación objetiva no se
puede resolver.  Por ejemplo para la interpolación en un único
punto de malla a partir de dos estaciones de medida, la matriz de
correlaciones sería singular
$$r_{ss}=
       \left(\begin{array}{cc}
  1 & 1 \\
  1 & 1
     \end{array}\right)\,.$$

La anomalía interpolada en notación matricial en un punto de malla es ahora
$$\phi'_g=r_{gs}(r_{ss}+\eta I)^{-1}\phi'\,,$$
y el error asociado al mapa interpolado en notación matricial en un
único punto de malla es
$$\epsilon=1-r_{gs}(r_{ss}+\eta I)^{-1}r^T_{gs}\,.$$

Para el caso de $N$ puntos de malla y $k$ estaciones obtenemos el mismo sistema que
para el caso de medidas perfectas pero añadiendo el error ruido-señal en la
diagonal principal de la matriz de correlaciones entre estaciones

\begingroup\makeatletter\def\f@size{8}\check@mathfonts$$\left(\begin{array}{ccc}
  \epsilon_{1} \\
  \epsilon_{2} \\
  . \\
  . \\
  . \\
  \epsilon_{N} \\
     \end{array}\right)=\text{Diag}\left[
     {\textbf I}-
\left(\begin{array}{ccccc}
  r_{g_11} & r_{g_12} &... &  r_{g_1k}\\
  r_{g_21} & r_{g_22} &... &  r_{g_2k}\\
  . & . & . & . \\
  . & . & . & . \\
  . & . & . & . \\
  r_{g_N1} & r_{g_N2} & ... & r_{g_Nk}\\
     \end{array}\right)
 \left(\begin{array}{ccccc}
  r_{11}+\eta & r_{12} &... &  r_{1k}\\
  r_{21} & r_{22}+\eta &... &  r_{2k}\\
  . & . & . & . \\
  . & . & . & . \\
  . & . & . & . \\
  r_{k1} & r_{k2} & ... & r_{kk}+\eta\\
     \end{array}\right)^{-1}
\left(\begin{array}{ccccc}
  r_{g_11} & r_{g_21} &... &  r_{g_N1}\\
  r_{g_12} & r_{g_22} &... &  r_{g_N2}\\
  . & . & . & . \\
  . & . & . & . \\
  . & . & . & . \\
  r_{g_1k} & r_{g_2k} & ... & r_{g_Nk}\\
     \end{array}\right) \right]
     \,.$$\endgroup
$$\left( N \times 1 \right)\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
  \left( N \times k \right)\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
  \,\,\,\,\,\,\,\,\left( k \times k \right)\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
  \left( k \times N \right)\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,$$


\section{Funciones Empíricas Ortogonales (FEOs)}
\vspace{0.25cm}
{\textbf \noindent Interpretación de los sistemas `propios'}

Antes de entrar en la teoría para las FEOs, vamos a ver que los vectores propios
son equivalentes a modos de oscilación de sistemas físicos. Imaginemos oscilaciones
verticales de bolas en una cuerda. Las bolas tienen masa $m$ y estan separadas por
cuerdas elásticas de longitud ${\cal d}$ en el equilibrio. Supongamos que los
desplazamientos $y_n$ son tan pequeños que la tensión de la cuerda ${\textbf T}$
se puede considerar constante. El ángulo de cada cuerda es $\theta$ como se ilustra
en la figura. Entonces, la ecuación del movimiento para la bola $n$ es
$$m\frac{d^2y_n}{dt^2}=-T sen\theta_{n-1} - T sen\theta_{n}\,.$$

\vspace{0.5cm}
\begin{center}
%\includegraphics[width=0.5\textwidth]{Modos_fisicos.pdf}
\end{center}

Bajo la asunción que los desplazamientos son pequeños, el $sen\theta_n=tan\theta_n$,
es decir, $sen\theta_{n-1}=tan\theta_{n-1}=y_n-y_{n-1}/d$ y $sen\theta_n=tan\theta_n=(y_n-y_{n+1})/d$.
Entonces la ecuación queda
$$m\frac{d^2y_n}{dt^2}=-T\frac{y_n-y_{n-1}}{d} - T\frac{y_n-y_{n+1}}{d}\,,$$
y si reagrupamos
$$m\frac{d^2y_n}{dt^2}=\frac{T}{d}(y_{n-1}-2y_n+y_{n+1})\,.$$
Vamos ahora a substituir una solución oscilatoria del tipo
$$y_n=Y_n e^{i\omega t}$$
en la ecuación del movimiento
$$-m\omega^2 Y_n e^{i\omega t}=\frac{T}{d}(Y_{n-1} e^{i\omega t}-2Y_n e^{i\omega t}+Y_{n+1} e^{i\omega t})$$
$$\frac{-m\omega^2 d}{T} Y_n e^{i\omega t}=\frac{T}{d}(Y_{n-1} e^{i\omega t}-2Y_n e^{i\omega t}+Y_{n+1} e^{i\omega t})\,.$$
Si definimos
$$\lambda=\frac{m\omega^2 d}{T}\,,$$
el sistema de ecuaciones a resolver es
$$-\lambda Y_n=(Y_{n-1}-2Y_n +Y_{n+1})$$
$$Y_n(2-\lambda)-Y_{n-1}-Y_{n+1}=0\,,$$
con las condiciones de frontera $Y_0=Y_{n+1}=0$ en las paredes.

Vamos a suponer ahora el caso de dos bolas.
Para la primera bola $n=1$
$$Y_1(2-\lambda)-Y_0-Y_2=0\,,$$
y para la bola $n=2$
$$Y_2(2-\lambda)-Y_1-Y_3=0\,.$$
Si aplicamos las condiciones de frontera $Y_0=Y_3=0$,
nos queda uns sistema "propio", donde
$\lambda$ son los autovalores.

$$\left|\begin{array}{cc}
  2-\lambda & -1 \\
   -1 & 2-\lambda\\
        \end{array}\right|=0\,.$$

El polinomio característico es 

$$\lambda^2-4\lambda+3=0\,,$$

y tienes las raíces $\lambda_1=1$ y $\lambda_2=3$.

Resolvamos para los vectores propios:\\

(1) $\lambda_1=1$ entonces

$$Y_1-Y_2=0$$ 

y el vector propio es

$${\textbf y}=\frac{1}{\sqrt{2}}\left(\begin{array}{c}
   1 \\
   1 \\
        \end{array}\right)$$
\\

(2) $\lambda_2=3$ entonces

$$-Y_1-Y_2=0\,,$$

y el vector propio es
$${\textbf y}=\frac{1}{\sqrt{2}}\left(\begin{array}{c}
   1 \\
   -1 \\
        \end{array}\right)\,.$$

Estas soluciones representan los modos oscilatorios
de un sistema físico de dos bolas oscilando verticalmente.
Los modos de oscilación se muestran en la figura. Los
modos oscilan independientemente uno de otro, y la evolución
del sistema es una combinación lineal de los dos modos.
De esta forma, lo que estamos haciendo al resolver el
problema de las bolas en una cuerda elástica es precisamente
la solución de un sistema "propio".

\vspace{0.5cm}
\begin{center}
%\includegraphics[width=0.5\textwidth]{Modos_2bolas.png}
\end{center}


\vspace{0.25cm}
{\textbf \noindent Definición de FEOs}

Un análisis en FEOs busca estructuras en los datos que explican
la mayor cantidad de varianza de un conjunto de datos bidimensional.
La primera dimensión es la dimension en la que deseamos encontrar una
estructura, y la otra dimension es la dimensión en la que se
muestrean las diferentes realizaciones. Por ejemplo, un conjunto de
series temporales de datos distribuidos espacialmente (i.e. arreglo
de anclajes). La primera dimensión es espacio y la segunda es
tiempo. Las estructuras en la dimensión espacial son las FEOs,
mientras que las estructuras en la dimensión de muestreo se
denominan Componentes Principales (CPs).
\\
Tanto las FEOs como las PCs son ortogonales en sus dimensiones.
Las FEOs/PCs pueden entenderse de diferentes formas:
\\
(i) Transforma variables correlacionadas en un conjunto de variables no correlacionadas
que expresan mejor la relación dinámica entre los datos originales.
\\
(ii) Identifica y ordena los vectores ortogonales (o dimensiones) a lo largo de los cuales nuestro
conjunto de datos presenta la mayor varianza.
\\
(iii) Una vez definidas las FEOs, es posible encontrar la mejor aproximación de los datos
originales con el mínimo número de vectores ortogonales.

\vspace{0.25cm}
{\noindent En general, aplicaremos el análisis en FEOs para describir de manera
mas sencilla conjuntos de datos organizados en matrices $M\times N$:}
\\
(1) Una matriz espacio-tiempo: Medidas de una variable en $M$ localizaciones
y $N$ tiempos.
\\
(2) Una matriz parámetro-tiempo: Medidas de $M$ variables en una
localización y $N$ tiempos.
\\
(3) Una matriz parámetro-espacio: Medidas de $M$ variables tomadas en $N$
localizaciones al mismo tiempo.
\\
\textbf {Nota:} Un error comun es considerar que las FEOs se corresponden con modos de físicos oceánicos. Eso no es cierto!. Los modos físicos en el océano son modos de oscilación que se obtienen considerando las ecuaciones que rigen el movimiento y condiciones de frontera; las FEOs son simplemente funciones ortogonales que explican la mayor cantidad de varianza de un conjunto de datos.

Aunque los procesos físicos dominantes son representados por los
primeros modos de oscilación, no existe una correspondencia uno a uno entre modos
físicos y FEOs.


\vspace{0.25cm}
\textbf {Teoría}\\

Supongamos M localizaciones de medición con series temporales de temperatura de N elementos. Queremos descomponer la serie temporal 
de temperatura en una localización dada $k$ como una combinación 
lineal de $M$ funciones ortogonales ${\textbf b}_i$ cuyas amplitudes
son pesadas con $M$ coeficientes dependientes del tiempo, es decir,

$${\textbf T}(t)=\sum\limits^M_{i=1}[\alpha_i(t){\textbf b}_{i}]\,,$$

donde $\alpha_i(t)$ es la amplitud del modo ortogonal $i$ al tiempo
$t=t_n(1\le n\le N)$. Los coeficientes $\alpha_i(t)$ nos informan de
como varian los modos ${\textbf b}_{i}$ con el tiempo. Necesitamos tantas
funciones ortogonales como estaciones con series temporales tenemos
para poder describir la varianza total de los datos originales de
temperatura a cada tiempo. Sin mebargo, en términos prácticos
podemos explicar una gran cantidad de varianza de los datos originales
con las primeras FEOs. Podemos ver el problema al revés, es
decir, tenemos $N$ funciones temporales cuyas amplitudes son pesadas
por $M$ coeficientes que varian en el espacio. En este caso hablamos
de PCs. Ya sea la reducción de los datos en funciones ortogonales
espaciales (FEOs) o temporales (PCs), obtenemos los mismos resultados.

Puesto que queremos ${\textbf b}_{i}$ ser ortogonal, requerimos
$${\textbf b}^T_i {\textbf b}_{j}=\delta_{ij}\,,$$
y los coeficientes temporales $\alpha_i={\textbf b}^T_{i}{\textbf T}(t)$.
Son precisamente estos coeficientes temporales las CPs, es
decir, la proyección de los datos originales sobre las
FEOs o la expresión de los datos originales en la nueva
base de vectores ortogonales (el nuevo sistema de coordenadas).

El objetivo del análisis es encontrar una base de
vectores ortogonales tal que las funciones $\alpha_i(t)$
no esten correlacionadas
$$<\alpha_i \alpha_j>=<{\textbf b}^T_{i}{\textbf T}{\textbf b}^T_{j}{\textbf T}>=<{\textbf b}^T_{i}{\textbf T}{\textbf T}^T{\textbf b}_{j}>={\textbf b}^T_{i}<{\textbf T} {\textbf T}^T>{\textbf b}_{j}=\delta_{ij}<\alpha^2_i>\,,$$
donde
$$<\alpha_i^2>=\frac{1}{N}\sum\limits^N_{n=1}=\alpha^2_i(t_n)\,.$$
Es decir, la matriz de covarianza de $\alpha_i(t)$ será una
matriz diagonal ${\textbf D}$. Para $M$ posiciones se puede reescribir como

$${\textbf B}^T<{\textbf T}{\textbf T}^T>{\textbf B}={\textbf D}\,\,\,\,\,\,\,\,\,{\rm o}\,\,\,\,\,\,\,\,{\textbf B}^T{\textbf T}{\textbf T}^T{\textbf B}=N{\textbf D}$$

donde ${\textbf B}$ es una matriz ortogonal cuyas columnas son
los vectores ortogonales ${\textbf b}_i$ y ${\textbf D}$ es una matriz
diagonal compuesta por las varianzas de las funciones
temporales $\alpha_i(t)$. Si multiplicamos por ${\textbf B}$
llegamos a un sistema propio

$$<{\textbf T}{\textbf T}^T>{\textbf B}={\textbf B}{\textbf D}\,.$$

Este tipo de sistema propio es conocido. La diagonal de ${\textbf D}$ esta
compuesta por valores propios y las columnas de ${\textbf B}$ son los
vectores propios. Los vectores propios son denominados FEOs, y los valores
propios son las varianzas de las amplitudes $\alpha_i$. Básicamente hemos realizado una transformación de coordenadas de tal forma que los vectores propios ${\textbf b}_i$

indican combinaciones lineales de datos que no estan correlacionados (i.e., $<\alpha_i \alpha_j>=\delta_{ij}<\alpha^2_i>$).
Esta descomposición de los datos en FEOs es óptima en el sentido
de mínimos cuadrados. Imaginemos que queremos un conjunto de $K$
vectores que mejor aproxima ${\textbf T}$

$$<(\hat{\textbf T}-{\textbf T})^T(\hat{\textbf T}-{\textbf T})>=<{\textbf T}^T{\textbf T}>
  -\sum\limits^K_{i=1}<\alpha^2_i>\,.$$

El problema en mínimos cuadrados, bajo las restricciones que las
funciones ${\textbf b}_i$ sean ortogonales, se puede escribir a partir de
los multiplicadores de Lagrange

$${\textbf \cal L}=\sum\limits^K_{i=1}\left[{\textbf b}^T_i<{\textbf T}{\textbf T}^T>{\textbf b}_i-
            \lambda_i({\textbf b}_i^T{\textbf b}_i-1)\right]\,.$$

Si derivamos ${\textbf \cal L}$ respecto de ${\textbf b}_i$ obtenemos el
sistema propio

$$<{\textbf T}{\textbf T}^T>{\textbf b}_i=\lambda_i{\textbf b}_i\,.$$

De este análisis deducimos que las primeras $K$
funciones ortogonales o FEOs son las mejores funciones que
explican la máxima varianza de los datos originales, donde
los valores porpios estan ordenados de mayor a menor. Es decir,
no existe un subset de datos mas pequeño que $K$ funciones
ortogonales que produce el error cuadrático medio menor. En
este sentido las FEOs son los mejores `descriptores' de la
varianza de los datos.

Una matriz de datos de $M$ localizaciones y $N$ tiempos se puede
descomponer

$${\textbf C}{\textbf B}={\textbf B}{\textbf \Lambda}\,,$$

donde

$${\textbf C}={\textbf T}{\textbf T}^T=\left(\begin{array}{cccc}
   \sum\limits_{i=1}^N{T_{1}(t_i)T_{1}(t_i)} & \sum\limits_{i=1}^N{T_{1}(t_i)T_{2}(t_i)} & ... & \sum\limits_{i=1}^N{T_{1}(t_i)T_{M}(t_i)}\\
   \sum\limits_{i=1}^N{T_{2}(t_i)T_{1}(t_i)} & \sum\limits_{i=1}^N{T_{2}(t_i)T_{2}(t_i)} & ... & \sum\limits_{i=1}^N{T_{2}(t_i)T_{M}(t_i)}\\
                . & . & . \\
		. & . & . \\
		. & . & . \\
   \sum\limits_{i=1}^N{T_{M}(t_i)T_{1}(t_)} & \sum\limits_{i=1}^N{T_{M}(t_i)T_{2}(t_i)} & ... & \sum\limits_{i=1}^N{T_{M}(t_i)T_{M}(t_i)}\\
        \end{array}\right)\,,$$

es la matriz de covarianza entre las series temporales de
temperatura en localizaciones espaciales,
unas con otras;

$${\textbf T}=\left(\begin{array}{cccc}
   T_{1}(t_1) & T_{1}(t_2) & ... & T_{1}(t_N)\\
   T_{2}(t_1) & T_{2}(t_2) & ... & T_{2}(t_N)\\
                . & . & . \\
		. & . & . \\
		. & . & . \\
   T_{M}(t_1) & T_{M}(t_2) & ... & T_{M}(t_N)\\
        \end{array}\right)$$ es la

matriz de datos de temperatura;

$${\textbf B}=\left(\begin{array}{cccc}
   b_{11} & b_{12} & ... & b_{1M}\\
   b_{21} & b_{22} & ... & b_{2M}\\
                . & . & . \\
		. & . & . \\
		. & . & . \\
   b_{M1} & b_{M2} & ... & b_{MM}\\
        \end{array}\right)\,,$$
es la matriz con los vectores
propios ${\textbf b}_i$ como columnas, y
$${\textbf \Lambda}=\left(\begin{array}{cccc}
   \lambda_1 & 0 & ... & 0\\
  0 & \lambda_2 & ... &0\\
                . & . & . \\
		. & . & . \\
		. & . & . \\
   0 & 0 & ... & \lambda_M\\
        \end{array}\right)\,,$$
es la matriz diagonal compuesta por los valores propios
$\lambda_i=<\alpha_i^2>$
\\
\\
Otra forma de entender las FEOs (PCs) consiste en un análisis en valores
propios de las matrices de dispersión de nuestras matrices de datos. La
matriz de dispersión es el producto matricial de la matriz con su transpuesta,
o a la inversa. La primera matriz de dispersión es
$${\textbf C}={\textbf T}{\textbf T}^T=\left(\begin{array}{cccc}
   T_{1}(t_1) & T_{1}(t_2) & ... & T_{1}(t_N)\\
   T_{2}(t_1) & T_{2}(t_2) & ... & T_{2}(t_N)\\
                . & . & . \\
		. & . & . \\
		. & . & . \\
   T_{M}(t_1) & T_{M}(t_2) & ... & T_{M}(t_N)\\
        \end{array}\right)
\left(\begin{array}{cccc}
   T_{1}(t_1) & T_{2}(t_1) & ... & T_{M}(t_1)\\
   T_{1}(t_2) & T_{2}(t_2) & ... & T_{M}(t_2)\\
                . & . & . \\
		. & . & . \\
		. & . & . \\
   T_{1}(t_N) & T_{2}(t_N) & ... & T_{M}(t_N)\\
        \end{array}\right)=$$
$$=\left(\begin{array}{cccc}
   \sum\limits_{i=1}^N{T_{1}(t_i)T_{1}(t_i)} & \sum\limits_{i=1}^N{T_{1}(t_i)T_{2}(t_i)} & ... & \sum\limits_{i=1}^N{T_{1}(t_i)T_{M}(t_i)}\\
   \sum\limits_{i=1}^N{T_{2}(t_i)T_{1}(t_i)} & \sum\limits_{i=1}^N{T_{2}(t_i)T_{2}(t_i)} & ... & \sum\limits_{i=1}^N{T_{2}(t_i)T_{M}(t_i)}\\
                . & . & . \\
		. & . & . \\
		. & . & . \\
   \sum\limits_{i=1}^N{T_{M}(t_i)T_{1}(t_i)} & \sum\limits_{i=1}^N{T_{M}(t_i)T_{2}(t_i)} & ... & \sum\limits_{i=1}^N{T_{M}(t_i)T_{M}(t_i)}\\
        \end{array}\right)\,,$$
$$\left( M \times M \right)$$
y la segunda es
$${\textbf C}={\textbf T}^T{\textbf T}=
\left(\begin{array}{cccc}
   T_{1}(t_1) & T_{2}(t_1) & ... & T_{M}(t_1)\\
   T_{1}(t_2) & T_{2}(t_2) & ... & T_{M}(t_2)\\
                . & . & . \\
		. & . & . \\
		. & . & . \\
   T_{1}(t_N) & T_{2}(t_N) & ... & T_{M}(t_N)\\
        \end{array}\right)\left(\begin{array}{cccc}
   T_{1}(t_1) & T_{1}(t_2) & ... & T_{1}(t_N)\\
   T_{2}(t_1) & T_{2}(t_2) & ... & T_{2}(t_N)\\
                . & . & . \\
		. & . & . \\
		. & . & . \\
   T_{M}(t_1) & T_{M}(t_2) & ... & T_{M}(t_N)\\
        \end{array}\right)=$$
$$=\left(\begin{array}{cccc}
   \sum\limits_{i=1}^M{T_{i}(t_1)T_{i}(t_1)} & \sum\limits_{i=1}^M{T_{i}(t_1)T_{i}(t_2)} & ... & \sum\limits_{i=1}^i{T_{1}(t_1)T_{i}(t_N)}\\
   \sum\limits_{i=1}^M{T_{i}(t_2)T_{i}(t_1)} & \sum\limits_{i=1}^M{T_{i}(t_2)T_{i}(t_2)} & ... & \sum\limits_{i=1}^M{T_{i}(t_2)T_{i}(t_N)}\\
                . & . & . \\
		. & . & . \\
		. & . & . \\
   \sum\limits_{i=1}^M{T_{i}(t_N)T_{i}(t_1)} & \sum\limits_{i=1}^M{T_{i}(t_N)T_{i}(t_2)} & ... & \sum\limits_{i=1}^M{T_{i}(t_N)T_{i}(t_N)}\\
        \end{array}\right)\,.$$
$$\left( N \times N \right)$$
Ambas matrices de dispersión obtenidas del producto de la matriz de datos
consigo misma son matrices de covarianza simétricas. La primera matriz de
dispersión ${\textbf T}{\textbf T}^T$ es una matriz $M \times M$ con lo que hemos
eliminado la dimensión temporal o dimensión de muestreo. En este caso,
la matriz de dispersión es una matriz de covarianzas temporales (determinadas
por sus variaciones temporales) de las estaciones unas con otras. En la segunda
matriz ${\textbf T}^T{\textbf T}$ se invierten las dimensiones, la matriz resultante es
$N\times N$, y es una matriz de covarianzas espaciales (determinadas
por sus variaciones espaciales) entre los diferenetes tiempos.
\\
\\
{\textbf Ejemplo:} Un ejmplo sencillo viene dado por el diagrama de dispersión
de la figura. La primera EOF o vector propio que explica la mayor
varianza sería la recta que se ajusta al conjunto de puntos y
la segunda EOF sería la línea perpendicular a la ajustada.

\begin{center}
%\includegraphics[width=0.5\textwidth]{EOF_example_plot.png}
\end{center}

\vspace{0.25cm}
{\textbf \noindent Relación entre FEOs y CPs}

Como ya hemos visto nuestro sistema propio es
$${\textbf B}^T<{\textbf T}{\textbf T}^T>{\textbf B}={\textbf D}\,\,\,\,\,\,\,\,\,{\rm o}\,\,\,\,\,\,\,\,{\textbf B}^T{\textbf T}{\textbf T}^T{\textbf B}=N{\textbf D}\,,$$
donde ${\textbf B}$ es una matriz cuyas columnas son los vectores
propios y ${\textbf D}$ es una matriz cuadrada con los $M$ valores
propios en la diagonal principal.
Si queremos expresar los datos originales en términos
de los vectores propios, entonces debemos usar la definición
$$\alpha_i(t)={\textbf b}^T_{i}{\textbf T}(t)\,,$$
que en notación matricial se puede expresar como
$${\textbf Z}={\textbf B}^T{\textbf T}\,,$$
y finalmente para recuperar los datos originales a partir de la
base de FEOs usamos
$${\textbf T}={\textbf B}{\textbf Z}\,,$$
ya que ${\textbf B}{\textbf B}^T={\textbf I}$.
La matriz ${\textbf Z}$ contiene los vectores de las
CPs, que no son mas que las
amplitudes por las cuales multiplicamos las FEOs
para obtener los datos originales de vuelta. De
esta forma podemos ir de un espacio (vectores propios) al
otro (datos originales) con la matriz de FEOs.
Supongamos que tenemos un conjunto de vectores propios ortogonales
y normalizados. El primero de ellos por ejemplo sería
$${\textbf e}=\left(\begin{array}{c}
   e_{11} \\
   e_{21} \\
         .\\
         .\\
	 .\\
   e_{M1} \\
        \end{array}\right)\,.$$
Si ponemos todos los vectores propios en columna obtenemos
la matriz cuadrada ${\textbf B}$, la cual es ortonormal
${\textbf B}^T{\textbf B}={\textbf I}$.
Si queremos proyectar un vector propio sobre los datos originales
y obtener la amplitud de este vector propio en cada tiempo,
debemos de hacer
$${\textbf e}^T{\textbf T}=\left(\begin{array}{ccccccc}
   e_{11} &
   e_{21} & . & . & . & e_{M1} \\
        \end{array}\right)
	\left(\begin{array}{cccc}
   T_{1}(t_1) & T_{1}(t_2) & ... & T_{1}(t_N)\\
   T_{2}(t_1) & T_{2}(t_2) & ... & T_{2}(t_N)\\
                . & . & . \\
		. & . & . \\
		. & . & . \\
   T_{M}(t_1) & T_{M}(t_2) & ... & T_{M}(t_N)\\
        \end{array}\right)=
	\left(\begin{array}{ccccccc}
   z_{11} &
   z_{12} & . & . & . & z_{1N} \\
        \end{array}\right)\,,$$
donde, por ejemplo, $z_{11}=e_{11}T_1(t_1)+ e_{21}T_2(t_1) + ... + e_{M1}T_M(t_1)\,.$
Si hacemos lo mismo para todos los otros vectores propios, obtenemos series
temporales de longitud $N$ para cada FEO, lo cual
se denominan componentes principales de cada EOF
$${\textbf Z}={\textbf B}^T{\textbf T}\,.$$
$$M\times N$$

Las CPs también son ortogonales. Si substituimos ${\textbf T}={\textbf B}{\textbf Z}$, en
${\textbf B}^T{\textbf T}{\textbf T}^T{\textbf B}=N{\textbf D}$ obtenemos
$${\textbf B}^T{\textbf B}{\textbf Z}({\textbf B}{\textbf Z})^T{\textbf B}=N{\textbf D}$$
$${\textbf I}{\textbf Z}{\textbf Z}^T{\textbf B}^T{\textbf B}=N{\textbf D}$$
$${\textbf I}{\textbf Z}{\textbf Z}^T{\textbf I}=N{\textbf D}$$
$${\textbf Z}{\textbf Z}^T=N{\textbf D}\,\,\,\,\,\,{\rm o}\,\,\,\,\,\,<{\textbf Z}{\textbf Z}^T>={\textbf D}\,.$$
Por lo tanto no solo las FEOs sino también las CPs son
ortogonales.

\vspace{0.25cm}
{\textbf \noindent Equivalencia con descomposición en valores singulares (SVD)}

\begin{framed}
{\noindent \textbf Definición de SVD:}
\\
La descomposición en valores singulares es básicamente un método de
reducción dimensional de un conjunto de datos inicialmente definidos sobre
un espacio multidimensional. Esta puede ser usada para encontrar simultáneamente
las FEOs y PCs de una matriz de datos.
\\

Una matriz ${\textbf A}$ puede ser descompuesta en un producto de
matrices: (i) una matriz ortogonal ${\textbf U}$ (i.e., ${\textbf U}^T{\textbf U}={\textbf I}$), una matriz
diagonal ${\textbf S}$, y la transpuesta de una matriz ortogonal
${\textbf V}$ (i.e., ${\textbf V}^T{\textbf V}={\textbf I}$)

$${\textbf A}={\textbf U}{\textbf S}{\textbf V}^T\,.$$

Las columnas de ${\textbf U}$ son vectores propios ortogonales de
${\textbf A}{\textbf A}^T$ o los vectores singulares izquierdos de ${\textbf A}$,
las columnas de ${\textbf V}$ son vectores propios ortogonales de
${\textbf A}^T{\textbf A}$ o los vectores singulares derechos de ${\textbf A}$,
y ${\textbf S}$ es una matriz diagonal que contiene las raices cuadradas
de los valores propios positivos de las matrices  ${\textbf U}$ o ${\textbf V}$
en orden descendente (o los valores singulares no-nulos de ${\textbf A}$). con orden
descendente nos referimos a que la primera
columna esta asociada al valor propio mas grande, la segunda columna al
segundo valor propio mas grande, y así sucesivamente.
\\
{\textbf Interpretación geométrica:} La SVD descompone la matriz ${\textbf A}$ en tres transformaciones
algebraicas sucesivas: (i) una rotación inicial
${\textbf U}$, un escalamiento ${\textbf S}$ a lo largo de los ejes coordenados, una rotación final
${\textbf V}$. Las longitudes $\sigma_1$ y $\sigma_2$ de la elipse son los valores
singulares de ${\textbf A}$ (ver script interpreta\_svd.m).

\end{framed}

Supongamos la matriz de datos compuesta por $M$ series temporales de
temperatura de longitud $N$
$${\textbf T}=\left(\begin{array}{cccc}
   T_{1}(t_1) & T_{1}(t_2) & ... & T_{1}(t_N)\\
   T_{2}(t_1) & T_{2}(t_2) & ... & T_{2}(t_N)\\
                . & . & . \\
		. & . & . \\
		. & . & . \\
   T_{M}(t_1) & T_{M}(t_2) & ... & T_{M}(t_N)\\
        \end{array}\right)\,.$$
La matriz de dispersión es
$$<{\textbf T}{\textbf T}^T>=\frac{1}{N} {\textbf T}{\textbf T}^T\,.$$
Sabemos que la matriz ${\textbf T}$ puede descomponerse en una matriz
ortogonal ${\textbf U}$, una matriz diagonal ${\textbf S}$, y la transpuesta
de una matriz ortogonal ${\textbf V}$. Esto es
$${\textbf T}={\textbf U}{\textbf S}{\textbf V}^T\,,$$
donde el número de valores singulares no nulos
indican el rango de ${\textbf T}$. Si $K<N$ y el número
de filas (i.e., los datos) son linealmente independientes
entonces el rango será $K$. Ahora la matriz de covarianza es
$$\frac{1}{N} {\textbf T}{\textbf T}^T = \frac{1}{N} ({\textbf U}{\textbf S}{\textbf V}^T)({\textbf U}{\textbf S}{\textbf V}^T)^T=
\frac{1}{N}{\textbf U}{\textbf S}{\textbf V}^T{\textbf V}({\textbf U}{\textbf S})^T=\frac{1}{N} {\textbf U}{\textbf S}{\textbf V}^T{\textbf V}{\textbf S}^T{\textbf U}^T=\frac{1}{N} {\textbf U}{\textbf S}{\textbf S}^T{\textbf U}^T\,.$$
La derecha del igual es la descomposición en valores propios de la
matriz de covarianza, donde ${\textbf S}{\textbf S}^T$ es cuadrada y diagonal con los
elementos igual a $N\lambda_i$; y las columnas de ${\textbf U}$ son
las FEOs. Las amplitudes de las FEOs vienen dadas por las
filas de la matriz
$${\textbf U}^T{\textbf T}={\textbf S}{\textbf V}\,,$$
asociado con valores singulares no nulos.

\vspace{0.25cm}
{\textbf \noindent Interpretación de las FEOs}

Como comentario final decir que las FEOs no son muy faciles de interpretar. Matematicamente
son estructuras que representan la mayor cantidad de varianza de los datos originales y que son
ortogonales entre ellas. En ocasiones estas estructuras nos dan estructuras con sentido físico
en un conjunto de datos, u otras no. Las estructuras particulares encontradas dependeran de como
hemos acomodado nuestra matriz bidimensional de datos. Algunas sugerencias para detectar si
las FEOs tienen sentido físico son las siguientes:
\\
(1) ?`La varianza de tu FEO es mas grande que lo que esperabas si los datos originales no tenían
estructura alguna?
\\
(2) ?`Existe una explicación apriori para las estructuras que has encontrado?
?`Se pueden explicar las estructuras en términos de alguna teoría? ?`Las
estructuras se comportan consistentemente con la teoría apriori?
\\
(3) ?`Cuán robustas son las estructuras a la elección del dominio de la estructura? ?`Si
cambias el dominio del análisis, esas estructuras cambian significantemente?
?`Si las estructuras estan definidas en un espacio geográfico, y cambias el tamaño de la
región, las estructuras cambian significativamente? ?`Si las estructuras estan definidas en el
espacio de parámetros y añades o eliminas un parámetros, los resultados cambian
de forma suave o aleatoriamente?
\\
(4)?`Cuán robustas son las estructuras a los datos usados? ?`Si divides los datos originales en
fracciones menores y haces el analisis de cada fracción, obtienes las mismas estructuras?

\vspace{0.5cm}
\noindent{\textbf Ejemplo de descomposición en valores singulares}
\\
 \noindent Supongamos la matriz
$${\textbf A}=\left(\begin{array}{ccc}
  3 & 1 & 1\\
  -1 & 3 & 1\\
      \end{array}\right)\,.$$

Para encontrar ${\textbf U}$ debemos resolver el problema en vectores y valores propios
de la matriz ${\textbf A}{\textbf A}^T$
$${\textbf A}{\textbf A}^T=\left(\begin{array}{ccc}
  3 & 1 & 1\\
  -1 & 3 & 1\\
      \end{array}\right)
      \left(\begin{array}{ccc}
  3 & -1 \\
  1 & 3 \\
  1 & 1 \\
      \end{array}\right)=\left(\begin{array}{cc}
  11 & 1 \\
  1 & 11 \\
      \end{array}\right)$$

El sistema de ecuaciones `propio' es

 $$\left(\begin{array}{cc}
  11 & 1 \\
  1 & 11 \\
      \end{array}\right)\left(\begin{array}{cc}
  x_1 \\
  x_2 \\
      \end{array}\right)=\lambda\left(\begin{array}{cc}
  x_1 \\
  x_2 \\
      \end{array}\right)$$

Si resolvemos para $\lambda$

$$\left|\begin{array}{cc}
  11-\lambda & 1 \\
  1 & 11-\lambda \\
      \end{array}\right|=0\,,$$

lo que deja el polinomio característico
$$(\lambda-10)(\lambda-12)=0\,,$$
con raices (valores propios) $\lambda_1=12$ y $\lambda_2=10$.

Si sustituimos en el sistema de ecuaciones `propio' el primer valor propio $\lambda_1=12$
$$(11-12)x_1 + x_2=0$$
$$x_1=x_2$$
Para $x_1=1$ obtenemos que $x_2=1$. Entonces obtenemos el vector propio
${\textbf v}_1=[1,1]$.

Si sustituimos en el sistema de ecuaciones `propio' el primer valor propio $\lambda_1=10$
$$(11-10)x_1 + x_2=0$$
$$x_1=-x_2$$
Para $x_1=1$ obtenemos que $x_2=-1$. Entonces obtenemos el vector propio
${\textbf v}_2=[1,-1]$.

Si organizamos la matriz con columnas correspondientes a los
vectores propios asociados a los valores propios de mayor a
menor, obtenemos
$$\left(\begin{array}{cc}
  1 & 1 \\
  1 & -1 \\
      \end{array}\right)$$

Finalmente, sabemos que ${\textbf U}$ tiene que ser ortonormal. Vamos
a usar el proceso de Gram-Schmidt para ortonormalizar las columnas
de ${\textbf U}$. Empezamos normalizando la primera columna
$${\textbf u}_1=\frac{{\textbf v}_1}{|{\textbf v}_1|}=\frac{[1,1]}{\sqrt{2}}=\left[ \frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}} \right]\,,$$
y calculamos el vector ortonormal como
$${\textbf w}_2={\textbf v}_2-{\textbf u}_1\cdot{\textbf v}_2{\textbf u}_1=[1,-1]-\left[ \frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}} \right]
\cdot[1,-1]\left[ \frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}} \right]=[1,-1]-[0,0]=[1,-1]\,.$$

Si lo normalizamos
$${\textbf u}_2=\frac{{\textbf w}_2}{|{\textbf w}_2|}=\left[ \frac{1}{\sqrt{2}}, \frac{-1}{\sqrt{2}} \right]\,,$$
dejando la matriz
$${\textbf U}=\left(\begin{array}{cc}
  \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\
  \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\
      \end{array}\right)\,.$$

Similarmente, para calcular la matriz ${\textbf V}$ debemos resolver el problema en vectores
y valores propios de la matriz ${\textbf A}^T{\textbf A}$
$${\textbf A}^T{\textbf A}=\left(\begin{array}{ccc}
  3 & -1 \\
  1 & 3 \\
  1 & 1 \\
      \end{array}\right)\left(\begin{array}{ccc}
  3 & 1 & 1\\
  -1 & 3 & 1\\
      \end{array}\right)=
      \left(\begin{array}{ccc}
  10 & 0 & 2 \\
  0 & 10 & 4\\
  2 & 4 & 2\\
      \end{array}\right)
      $$

El sistema de ecuaciones `propio' es

 $$\left(\begin{array}{ccc}
  10 & 0 & 2 \\
  0 & 10 & 4\\
  2 & 4 & 2\\
      \end{array}\right)\left(\begin{array}{c}
  x_1 \\
  x_2 \\
  x_3 \\
      \end{array}\right)=\lambda \left(\begin{array}{c}
  x_1 \\
  x_2 \\
  x_3 \\
      \end{array}\right)$$

$$(10-\lambda)x_1 + 2x_3=0$$
$$(10-\lambda)x_2 + 4x_3=0$$
$$2x_1 + 4x_2 + (2-\lambda)x_3=0$$

Si resolvemos para $\lambda$
$$\left|\begin{array}{ccc}
  10-\lambda & 0 & 2 \\
  0 & 10-\lambda & 4\\
  2 & 4 & 2-\lambda\\
      \end{array}\right|=0$$

lo que deja el polinomio característico
$$\lambda (\lambda-10)(\lambda-12)=0\,,$$
con raices (valores propios) $\lambda_1=12$, $\lambda_2=10$, y $\lambda_3=0$.
Substituyendo en el sistema `propio' para $\lambda_1=12$
$$(10-12)x_1 + 2x_3 = -2x_1+2x_3=0$$
$$x_1=1;\,\,x_3=1$$
$$(10-12)x_2 + 4x_3 = -2x_2+4x_3=0$$
$$x_2=2x_3$$
$$x_2=2$$
Entonces, para $\lambda_1=12$ obtenemos el vector propio ${\textbf v}_1=[1,2,1]$.
\\
Para $\lambda_2=10$
$$(10-10)x_1 + 2x_3=2x_3=0$$
$$x_3=0$$
$$2x_1+4x_2=0$$
$$x_1=-2x_2$$
$$x_1=2;\,\,x_2=-1$$
y obtenemos ${\textbf v}_2=[2,-1,0]$ para $\lambda_2=10$.
\\
Finalmente, para $\lambda_3=0$
$$10x_1+2x_3=0$$
$$x_3=-5$$
$$10x_1-20=0$$
$$x_2=2$$
$$2x_1+8-10=0$$
$$x_1=1$$
lo que implica que para $\lambda_3=0$ ${\textbf v}_3=[1,2,-5]$.
\\
\\
Si organizamos los vectores de acuerdo con el valor de los
valores propios (de mayor a menor) obtenemos la matriz
$$\left(\begin{array}{ccc}
  1 & 2 & 1 \\
  2 & -1 & 2\\
  1 & 0 & -5\\
      \end{array}\right)$$

Ahora vamos a ortonormalizarla con el proceso de Gram-schmidt
   $${\textbf u}_1=\frac{{\textbf v}_1}{|{\textbf v}_1|}=\frac{[1,2,1]}{\sqrt{6}}\,,$$
y calculamos el vector ortonormal como
$${\textbf w}_2={\textbf v}_2-{\textbf u}_1\cdot{\textbf v}_2{\textbf u}_1=[2,-1,0]$$

Si lo normalizamos
$${\textbf u}_2=\frac{{\textbf w}_2}{|{\textbf w}_2|}=\left[ \frac{2}{\sqrt{5}}, \frac{-1}{\sqrt{5}} , 0 \right]$$

El último vector ortonormal a calcular es
$${\textbf w}_3={\textbf v}_3-{\textbf u}_1\cdot{\textbf v}_3{\textbf u}_1 - {\textbf u}_2\cdot{\textbf v}_3{\textbf u}_2=[\frac{-2}{3},\frac{-4}{3},\frac{10}{3}]$$

Si lo normalizamos
$${\textbf u}_3=\frac{{\textbf w}_3}{|{\textbf w}_3|}=\left[ \frac{1}{\sqrt{30}}, \frac{2}{\sqrt{30}} , \frac{-5}{\sqrt{30}} \right]$$

dejando la matriz
$${\textbf V}=\left(\begin{array}{ccc}
  \frac{1}{\sqrt{6}} & \frac{2}{\sqrt{5}} & \frac{1}{\sqrt{30}}\\
  \frac{2}{\sqrt{6}} & -\frac{1}{\sqrt{5}} & \frac{2}{\sqrt{30}}\\
  \frac{1}{\sqrt{6}} & 0 & \frac{-5}{\sqrt{30}}
      \end{array}\right)\,.$$

Finalmente, lo que realmente queremos es
$${\textbf V}^T=\left(\begin{array}{ccc}
  \frac{1}{\sqrt{6}} & \frac{2}{\sqrt{6}} & \frac{1}{\sqrt{6}}\\
  \frac{2}{\sqrt{5}} & -\frac{1}{\sqrt{5}} & 0\\
  \frac{1}{\sqrt{30}} & \frac{2}{\sqrt{30}} & \frac{-5}{\sqrt{30}}\\
      \end{array}\right)\,.$$

Para calcular ${\textbf S}$ debemos tomar las raices cuadradas de los
valores propios diferentes de cero ($\lambda_i\ne0$) y colocarlos
en la diagonal principal en orden descendente. Es decir, el
valor propio mayor en la posición $s_{11}$, el siguiente mas
grande en $s_{22}$, y así sucesivamente. Los valores propios
diferentes de cero son iguales para ${\textbf U}$ y ${\textbf V}$ con
lo que no importa de cual los tomemos. Puesto que solo hay
dos valores propios diferentes de cero y el orden de las
matrices ${\textbf U}$ y ${\textbf V}$ es $3\times3$, debemos añadir
una columna de ceros a ${\textbf S}$
$${\textbf S}=\left(\begin{array}{ccc}
  \sqrt{12} & 0 & 0 \\
  0 & \sqrt{10} & 0\\
      \end{array}\right)\,.$$

Ahora ya tenemos todas las matrices de la descomposición en
valores singulares:
$${\textbf A}={\textbf U}{\textbf S}{\textbf V}^T=\left(\begin{array}{cc}
  \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\
  \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\
      \end{array}\right)
      \left(\begin{array}{ccc}
  \sqrt{12} & 0 & 0 \\
  0 & \sqrt{10} & 0\\
      \end{array}\right)
      \left(\begin{array}{ccc}
  \frac{1}{\sqrt{6}} & \frac{2}{\sqrt{5}} & \frac{1}{\sqrt{30}}\\
  \frac{2}{\sqrt{6}} & -\frac{1}{\sqrt{5}} & \frac{2}{\sqrt{30}}\\
  \frac{1}{\sqrt{6}} & 0 & \frac{-5}{\sqrt{30}}
      \end{array}\right)=$$
  $$=\left(\begin{array}{ccc}
  \frac{12}{\sqrt{2}} & \frac{\sqrt{10}}{\sqrt{2}} & 0\\
  \frac{12}{\sqrt{2}} & -\frac{\sqrt{10}}{\sqrt{2}} & 0\\
      \end{array}\right)
      \left(\begin{array}{ccc}
  \frac{1}{\sqrt{6}} & \frac{2}{\sqrt{5}} & \frac{1}{\sqrt{30}}\\
  \frac{2}{\sqrt{6}} & -\frac{1}{\sqrt{5}} & \frac{2}{\sqrt{30}}\\
  \frac{1}{\sqrt{6}} & 0 & \frac{-5}{\sqrt{30}}
      \end{array}\right)=
      \left(\begin{array}{ccc}
  3 & 1 & 1 \\
  -1 & 3 & 1\\
      \end{array}\right)\,.$$

\section{Análisis espectral o análisis de Fourier}

\subsection{Introducción}
Una función periódica es aquella cuyos valores se repiten a intervalos
regulares. El tiempo entre las sucesivas repeticiones se denomina periodo $\tau$.
Normalmente lo definimos entre sucesivas crestas. Matemáticamente, una
función es periódica si $f(t)=f(t+T)$ para todo valor de $T$.


\begin{center}
%\includegraphics[width=0.5\textwidth]{funcion_periodica.png}
\end{center}
\begin{center}
Función periódica $f(t)=f(t+\tau)$.
\end{center}

La frecuencia de una función periódica se define como el inverso del periodo, $f=1/\tau$,
es decir el número de ciclos por unidad de tiempo (si es por segundo
hablamos de Hercios, Hz). Si un ciclo equivale a $2\pi$
radianes, entonces el número de radianes por segundo es lo que se concoce por la
frecuencia angular fundamental:
$$\omega=\frac{2\pi}{T}\,.$$

Las funciones periódicas también se pueden definir en el
espacio. Entonces el periodo se define como
$$\tau=\lambda/v_p\,,$$
donde $\lambda$ es la longitud de onda y $v_p=\lambda/\tau=\omega/k$ es la velocidad de fase.
La longitud de onda es una distancia entre estados de la onda que se repiten, e.j.
entre dos crestas. El número de onda $k$ es el número de ondas
contenidas en una unidad de distancia
$$k=\frac{2\pi}{\lambda}=\frac{\omega}{v_p}$$


El valor promedio de una función periódica es:
$$f_m=\frac{1}{\tau}\int\limits^{\tau}_0 f(t) dt\,,$$
y su valor cuadrático medio (o RMS, en inglés) es:
$$f_{rms}=\sqrt{\frac{1}{\tau}\int\limits^{\tau}_0 f^2(t) dt}\,,$$
donde las integrales se han definido en el intervalo 0,$\tau$, aunque se pueden
definir en cualquier intervalo que abarque un periodo, e.j. de -$\tau$/2 a $\tau$/2.

Una de las ondas periódicas mas utilizadas es la sinusoidal o cosenosoidal.
$$f(t)=A sen(\omega t + \theta)\,,$$
siendo $A$ la amplitud y $\theta$ su fase inicial.
En este caso el valor medio es cero y su rms es $A/\sqrt(2)$.
%
Recordar que existen dos frecuencias básicas: (i) Frecuencia de Nyquist $f_N=1/(2\Delta t)$
(la frecuencia mas alta que podemos resolver) y (ii) Frecuencia fundamental $f_0=1/(\Delta t N)=1/T$ (la frecuencia mas baja que podemos resolver).

\vspace{0.25cm}
\subsection{Serie de Fourier}

El principio básico del análisis de Fourier es que cualquier función periódica $f(t)$
definida en el intervalo $[0,T]$ se puede descomponer en suma de funciones simples, sinusoidales y cosinusoidales, o series de Fourier de la forma
$$f(t)=\bar{f(t)} + \sum\limits_p [A_p cos(\omega_p t) + B_p sin (\omega_p t)]\,,$$
donde $\overline{f(t)}$ es el valor medio de la serie temporal, $A_p$ y $B_p$ son constantes
denominados coeficientes de Fourier, y $\omega_p=2 \pi p f_0=2\pi p/\tau$ es múltiplo
de la frecuencia angular fundamental.

Si tenemos suficientes componentes de Fourier cada valor de la serie original se puede
reconstruir. La contribución que cada componente tiene sobre la varianza de la serie
temporal es una medida de la importancia de una frecuencia particular en la serie
original. El punto clave aqui es que el conjunto de coeficientes de Fourier con amplitudes $A_p$ y $B_p$ forman un espectro el cual define la contribución de cada componente oscilatoria $\omega_p$ sobre la `energía' total de la señal original. En concreto, el
spectro de potencia (Power spectrum) define la energía por unidad de banda de frecuencia de una serie temporal. Puesto que debemos definir dos amplitudes $A_p$ y $B_p$, hay dos grados de
libertad por estimación espectral.

Como ya hemos dicho el primer armónico ($p=1$) oscila
con frecuencia fundamental $\omega_1=2\pi f_1$. El armónico $N/2$, el cual nos da la
componente con la frecuencia más alta que puede ser resuelta tiene frecuencia
$f_N=N/2/N\Delta t=1/(2\Delta T)$ ciclos por unidad de tiempo y un periodo de $2\Delta t$.
Esta es la frecuencia de Nyquist.

\begin{framed}
{\noindent \textbf Integrales de funciones trigonométricas útiles:}
\\
$$\int {\rm cos} ax dx =\frac{{\rm sen} ax}{a} + C$$
\\
$$\int {\rm sen} ax dx =-\frac{{\rm cos} ax}{a} + C$$
\\
$$\int {\rm sen}^2 ax dx =\frac{x}{2}-\frac{\rm{sen} 2ax}{4a} + C$$
\\
$$\int {\rm cos}^2 ax dx =\frac{x}{2}+\frac{\rm{sen} 2ax}{4a} + C$$

\end{framed}

Las series de Fourier se definen como
$$f(t)=\frac{1}{2}A_0 +\sum\limits^\infty_{p=1}[A_p cos(\omega_p t) + B_p sen(\omega_p t)]\,,$$
en la cual
$$\omega_p=2\pi f_p=2\pi p f_1=2\pi p /T;\,\,\,\,p=1,2,...\,,$$
es la frecuencia de la componente $p$ésima en radianes por unidad de tiempo ($f_p$ es
en ciclos por unidad de tiempo) y $A_0/2$ es la media de la serie temporal.

Para obtener los coeficientes $A_p$, debemos multiplicar la expresión de la descomposición
de Fourier por $cos(\omega_r t)$ e integrar sobre la serie completa.
$$\int\limits^{T}_{0}f(t)cos(\omega_r t)dt=\frac{1}{2}A_0\int\limits^{T}_{0} cos(\omega_r t) dt + $$
$$+\sum\limits^\infty_{p=1} A_p \int\limits^{T}_{0}cos(\omega_p t)cos(\omega_r t) dt +$$
$$+\sum\limits^\infty_{p=1} B_p \int\limits^{T}_{0}sin(\omega_p t)cos(\omega_r t) dt\,.$$

Si usamos las siguientes condiciones de ortogonalidad:
$$\int\limits^{T}_{0}sin(\omega_p t)cos(\omega_r t) dt=0$$
$$\int\limits^{T}_{0}cos(\omega_p t)cos(\omega_r t) dt=
 \left\lbrace
  \begin{array}{l}
     T, p=r=0 \\
     T/2,p=r>0 \\
     0, p\ne r \\
  \end{array}
  \right.$$
$$\int\limits^{T}_{0}sin(\omega_p t)sin(\omega_r t) dt=
 \left\lbrace
  \begin{array}{l}
     0, p=r=0 \\
     T/2,p=r>0 \\
     0, p\ne r \\
  \end{array}
  \right.$$
entonces encontramos que para $r=0;p\ne r$ la ecuación de arriba se reduce a
$$\int\limits^{T}_{0}f(t)dt=\frac{A_0}{2}T\,,$$
es decir,
$$A_0=\frac{2}{T}\int\limits^{T}_{0}f(t)dt=2\overline{f(t)}\,,$$
dos veces el valor medio de la serie $f(t)$. Es por ello
que se añade el factor de $1/2$ en la serie de Fourier.
Es decir, para que el primer término de la serie de
Fourier sea igual a la media de la serie temporal
$\overline{f(t)}=1/2A_0$.

Cuando $p\ne0$ el único término no despreciable de la derecha de la
expresión de arriba sucede cuando $r=p$
$$\int\limits^{T}_{0}f(t)cos(\omega_p t)dt=\frac{A_p}{2}T\,,$$
y entonces
$$A_p=\frac{2}{T}\int\limits_0^{T} f(t) cos(\omega_p t) dt,\,\,\,\,p=1,2,...$$

Los otros coeficientes $B_p$ son obtenidos igualmente multiplicando por
$sen(\omega_r t)$ en lugar de $cos(\omega_r t)$
$$B_p=\frac{2}{T}\int\limits_0^{T} f(t) sen(\omega_p t) dt,\,\,\,\,p=1,2,...\,.$$


\begin{framed}
{\noindent \textbf Relaciones trigonométricas útiles:}
\\
$$cos(\alpha-\beta)=cos\alpha cos\beta + sen\alpha sin\beta$$
$$sen(\alpha-\beta)=sen\alpha cos\beta - cos\alpha sin\beta$$
$$tg\gamma=\frac{sen\gamma}{cos\gamma}$$

De estas relaciones de arriba se puede llegar a:
$$Acos\theta + Bsen\theta=Ccos(\theta-\theta_0)\,,$$
donde
$$C=\sqrt{A^2 + B^2}\,,$$
y
$$\theta_0=arctg{\left(\frac{B}{A}\right)}$$

A partir de esta relación
$$e^{i\theta}=cos\theta+isen\theta\,,$$
llegamos a
$$sen\theta=\frac{e^{i\theta}-e^{-i\theta}}{2i}$$
$$cos\theta=\frac{e^{i\theta}+e^{-i\theta}}{2}$$
\end{framed}

También podemos representar la serie de Fourier en notación compacta como:
$$f(t)=\frac{1}{2}C_0 +\sum\limits^\infty_{p=1}C_p cos(\omega_p t - \theta_p)\,,$$
en la cual la amplitud de la $p$ésima componente es
$$C_p=\sqrt{A_p^2+B_p^2}\,,\,\,\,\,p=1,2,....$$
donde $C_0=A_0 (B_0=0)$ es dos veces el valor promedio de la serie y
$$\theta_p=arctg[B_p/A_p]\,,\,\,\,\,p=1,2,...$$
es el ángulo de fase de la componente al tiempo $t=0$.
El ángulo de fase nos informa del "desfase" ({\it lag}) relativo de las componente
en radianes (o grados) medido en el sentido contrario a las agujas del reloj desde
el eje real definido por $B_p=0, A_p>0$. El correspondiente tiempo de desfase para la componente $p$ésima es $t_p=\theta_p/2\pi f_p$ en el cual $\theta_p$ esta medida en radianes.
La energía espectral se define como las amplitudes de los coeficientes de Fourier al cuadrado,
lo cual representa la varianza y entonces la energía
$$C^2_p=A_p^2 + B_p^2\,.$$
\\
\\
De igual forma con las relaciones trigonométricas de arriba
se puede expresar las series de Fourier en notación compleja.
Usando
$$sen\omega_p t=\frac{e^{i\omega_pt}-e^{-i\omega_pt}}{2i}\,\,\,\,\,\,\,y\,\,\,\,\,\,\,
cos\omega_p t=\frac{e^{i\omega_pt}+e^{-i\omega_pt}}{2}\,,$$
obtenemos
$$f(x)=\frac{1}{2}A_0 +\sum\limits^\infty_{p=1}[A_p cos(\omega_p t) + B_p sen(\omega_p t)]=$$
$$=\frac{1}{2}A_0 +\sum\limits^\infty_{p=1}\left[ A_p \frac{e^{i\omega_pt}+e^{-i\omega_pt}}{2} + B_p \frac{e^{i\omega_pt}-e^{-i\omega_pt}}{2i}\right]=$$
$$=\frac{1}{2}A_0+\sum\limits^\infty_{p=1}\left[\frac{A_p e^{i\omega_p t}}{2} + \frac{A_p e^{-i\omega_p t}}{2} -
\frac{iB_p e^{i\omega_p t}}{2} + \frac{iB_p e^{-i\omega_p t}}{2} \right]=$$
$$=\frac{1}{2}A_0+\sum\limits^\infty_{p=1} e^{i\omega_p t}\frac{A_p-iB_p}{2} +
                  \sum\limits^\infty_{p=1} e^{-i\omega_p t}\frac{A_p+iB_p}{2}=$$
$$=C^*_0 + \sum\limits^\infty_{p=1}C^*_pe^{i\omega_p t} + \sum\limits^\infty_{p=1}C^*_{-p} e^{-i\omega_p t}=
\sum\limits^\infty_{p=-\infty} C^*_p e^{i\omega_p t}\,,$$
donde hemos definido las siguientes relaciones entre los coeficientes de Fourier
complejos y reales:
$$C^*_0=\frac{1}{2}A_0\,,$$
$$C^*_p=\frac{1}{2}(A_p-iB_p)\,,$$
$$C^*_{-p}=\frac{1}{2}(A_p+iB_p)\,.$$

En resumen, podemos reconstruir la serie periódica $f(t)$ con la
transformada de Fourier
$$f(x)=\sum\limits^\infty_{p=-\infty} C^*_p e^{i\omega_p t}\,,$$
e inversamente podemos calcular los coeficientes de Fourier $C^*_p$
a partir de la $f(t)$
$$C^*_p=\frac{1}{2}(A_p-iB_p)=\frac{1}{2}\left(\frac{2}{T}\int\limits_0^{T} f(t) cos(\omega_p t) dt -
                                             i \frac{2}{T}\int\limits_0^{T} f(t) sen(\omega_p t) dt \right)=$$
$$=\frac{1}{T}\int\limits_0^{T} f(t) [cos(\omega_p t) - isen(\omega_p t) ]dt = \frac{1}{T}\int\limits_0^{T} f(t) e^{-i \omega_p t} dt\,,$$
es decir, podemos pasar del espacio temporal $f(t)$ al espacio espectral o de Fourier $C^*_p$
e inversamente regresar al espacio temporal de nuevo.

El teorema de Parseval es precisamente el que demuestra que
la el valor cuadrático medio de la serie de Fourier es
igual al error cuadrático medio de los coeficientes de
Fourier.
La varianza de la serie de Fourier es
$$\frac{1}{T}\int\limits^{T}_0 f^2(t) dt=\frac{1}{T}\int\limits^{T}_0
\left(\frac{1}{2}A_0 +\sum\limits^\infty_{p=1}[A_p cos(\omega_p t) + B_p sen(\omega_p t)]\right) dt=$$
$$=\frac{1}{T}\frac{1}{4} A^2_0 \int\limits^{T}_0 dt + \frac{1}{T}A_0 \sum\limits^\infty_{p=1} A_p \int\limits^{T}_0 cos(\omega_p t)dt + \frac{1}{T}A_0\sum\limits^\infty_{p=1}B_p \int\limits^{T}_0 sen(\omega_p t)dt +$$
$$+\frac{1}{T}\sum\limits^\infty_{p=1}A^2_p \int\limits^{T}_0 cos^2(\omega_p t)dt +
\frac{1}{T}\sum\limits^\infty_{p=1}B^2_p \int\limits^{T}_0 sen^2(\omega_p t)dt+$$
$$+\frac{1}{T}2\sum\limits^\infty_{p=1}A_p \sum\limits^\infty_{p=1}B_p\int\limits^{T}_0 sen(\omega_p t)cos(\omega_p t)dt=
\frac{1}{T}\frac{1}{4}A^2_0 T + \frac{1}{T}\sum\limits^\infty_{p=1}A^2_p\left[ \frac{t}{2} + \frac{sen(2\omega_p t)}{4\omega_p}\right]^T_0+$$
$$+\frac{1}{T}\sum\limits^\infty_{p=1}B^2_p\left[ \frac{t}{2} - \frac{sen(2\omega_p t)}{4\omega_p}\right]^T_0=
\frac{1}{4}A^2_0 + \frac{1}{2}\sum\limits^\infty_{p=1}A_p^2 + \frac{1}{2}\sum\limits^\infty_{p=1}B_p^2=$$
$$=\frac{1}{4}A^2_0 + \frac{1}{2}\sum\limits^\infty_{p=1} A_p^2 + B_p^2\,.$$

Utilizando las siguientes identidades
$$|C^*_p|^2=|C^*_{-p}|^2=\frac{1}{4}(A_p^2 + B_p^2)\,,$$
$$C^*_0=\frac{1}{2}A_0\,,$$
el teorema de Parseval en términos de los coeficientes de Fourier complejos
es
$$\frac{1}{T}\int\limits^{T}_0 f^2(t) dt=(C^*_0)^2 + \frac{1}{2}\sum\limits^\infty_{p=1} 4|C^*_p|^2=
(C^*_0)^2 + \sum\limits^\infty_{p=1} |C^*_p|^2 + \sum\limits^\infty_{p=1} |C^*_p|^2=$$
$$=(C^*_0)^2 + \sum\limits^\infty_{p=1} |C^*_p|^2 + \sum\limits^\infty_{p=1} |C^*_{-p}|^2=
\sum\limits^\infty_{p=-\infty} |C^*_p|^2$$

\newpage

\begin{framed}
El teorema de Parseval es una
ley de conservación que demuestra que la suma de los módulos cuadrados
de los coeficientes de Fourier complejos es igual al valor promedio de
$f^2(x)$
$$\frac{1}{T}\int\limits^{T}_0 f^2(t) dt=\frac{1}{4}A^2_0 + \frac{1}{2}\sum\limits^\infty_{p=1} A_p^2 + B_p^2=
\sum\limits^\infty_{p=-\infty} |C^*_p|^2\,.$$
\end{framed}

Esto da lugar a la relación entre la amplitud de las componentes de Fourier en el
dominio espectral (de frecuencia) y la varianza de la serie en el dominio temporal.
\\
\\
{\textbf \noindent NOTA:} En el análisis de Fourier es importante recalcar que
debemos de eliminar la tendencia de la serie antes de calcular los coeficientes.
Sino lo hacemos, el análisis de Fourier pondrá erroneamente la varianza
de la tendencia en las componentes de baja frecuencia de la expansión de
Fourier. En Matlab eso lo podemos hacer con el comando detrend.m o bien
simplemente extrayendo el promedio temporal.


\vspace{0.75cm}
{\textbf \noindent Ejemplo de cálculo de coeficientes de Fourier}

Imaginemos la siguiente onda cuadrada
representada por la función
$$f(t)=
 \left\lbrace
  \begin{array}{l}
     -1,\text{para} -\frac{1}{2}T\le t< 0 \\
     +1,\text{para} 0\le t< \frac{1}{2}T  \\
  \end{array}
  \right.$$

\begin{center}
%\includegraphics[width=0.5\textwidth]{onda_cuadrada.png}
\end{center}
\begin{center}
Onda cuadrada.
\end{center}

Puesto que función de arriba es impar, es decir cumple la condición de
simetría $-f(t)=f(-t)$, entonces
la serie de Fourier resultante solamente contendrá componentes sinusoidales.
Entonces
$$B_p=\frac{2}{T}\int\limits_{-T/2}^{T/2} f(t) sen(\omega_p t) dt=$$
$$=\frac{2}{T}\int\limits_{-T/2}^{0} f(t) sen(\omega_p t) dt + \frac{2}{T}\int\limits_{0}^{T/2} f(t) sen(\omega_p t) dt=$$
$$\frac{2}{T}\left[ (-1) \int\limits_{-T/2}^{0} sen(\omega_p t) dt +
                   (1) \int\limits_{0}^{T/2} sen(\omega_p t) dt \right]=$$

$$=\frac{2}{T}\left[ \left|\frac{cos(\omega_p t)}{\omega_p}\right|_{-T/2}^{0}
                     -\left|\frac{cos(\omega_p t)}{\omega_p}\right|_{0}^{T/2} \right]=$$
$$=\frac{2}{\omega_p T}\left[ 1 - cos(\omega_p T/2) - cos(\omega_p T/2) + 1  \right]=
\frac{2}{\omega_p T}\left[2 -2cos(\omega_p T/2)\right]=$$
$$=\frac{4}{\omega_p T}\left[1-cos(\omega_p T/2)\right]=
\frac{2}{\pi p}\left[1-cos(\pi p)\right]$$
\\
\\
Los coeficientes son cero ($B_p=0$) si $p$ es par y $B_p=4/\pi p$ si $p$ es impar.
Finalmente nuestra serie de Fourier es
$$f(t)=\sum\limits^\infty_{p=1}B_p sen(\omega_p t)=$$
$$=\sum\limits^\infty_{p=1}B_p sen\left(\frac{2\pi p}{T} t\right)=
B_1 sen\left(\frac{2\pi 1}{T} t\right) + B_3 sen\left(\frac{2\pi 3}{T} t\right) +$$
$$+B_5 sen\left(\frac{2\pi 5}{T} t\right) + ... =\frac{4}{\pi}sen\left(\frac{2\pi 1}{T} t\right) +$$
$$+\frac{4}{3\pi}sen\left(\frac{2\pi 3}{T} t\right) +\frac{4}{5\pi}sen\left(\frac{2\pi 5}{T} t\right)=$$
$$=\frac{4}{\pi}\left( \frac{sen{\omega_1 t}}{1} + \frac{sen{3\omega_1 t}}{3} + \frac{sen{5\omega_1 t}}{5}\right)+...$$

\vspace{0.75cm}
\begin{framed}
{\it \textbf \noindent Código Matlab para N=100 componentes}
\\
{\noindent} N=100;\\
t=-30:0.1:30;\\
f=0;\\
for p=1:2:N;\\
f=f+(4/(p*pi))*sin(((2*pi*p)/(10))*t);\\
end\\
figure; plot(t,f);\\
\end{framed}

\begin{center}
%\includegraphics[width=0.75\textwidth]{Fourier_onda_cuadrada.pdf}
\end{center}
\begin{center}
Serie de Fourier para Onda cuadrada. Arriba para 5 componentes, medio para 50 componentes, y abajo
para 100 componentes.
\end{center}

\vspace{0.25cm}
{\textbf \large \noindent Series de Fourier Discretas}

En general, vamos a muestrear de forma discreta el océano y consecuentemente
las series temporales que obtenemos son discretas en el tiempo. Segun el teorema
de Parseval, la varianza de estas series discretas
$$\sigma^2=\frac{1}{N-1}\sum\limits^N_{t=1}(f(t)-\overline{f(t)})^2$$
se puede obtener sumando las contribuciones individuales de los
armónicos de Fourier. La descomposición de series temporales
discretas en armónicos específicos da lugar al concepto de
espectro de Fourier. Para encontrar el espectro de Fourier debemos
calcular los coeficientes $A_p,B_p$ o, equivalentemente, las amplitudes
$C_p$ y el ángulo de fase $\theta_p$.

Supongamos la serie de Fourier para un registro finito de longitud par
$N$ definido en los tiempos $t_1, t_2,....,t_N$
$$f(t_n)=\frac{1}{2}A_0+\sum\limits^{N/2}_{p=1}[A_p cos(\omega_p t_n) + B_p sen(\omega_p t_n)]\,,$$
donde ya sabemos $\omega_p=2\pi f_p=2\pi p/T$. Sabiendo que $t_n=n \Delta t$, esta serie se puede
reescribir como
$$f(t_n)=\frac{1}{2}A_0+\sum\limits^{N/2}_{p=1}[A_p cos(2\pi p n \Delta t / \Delta t N) + B_p sen(2\pi p n \Delta t / \Delta t N)]=$$
$$=\frac{1}{2}A_0+\sum\limits^{N/2}_{p=1}[A_p cos(2\pi p n  / N) + B_p sen(2\pi p n / N)]=
\frac{1}{2}C_0 + \sum\limits^{N/2}_{p=1}[C_p cos[(2\pi p n  / N) - \theta_p]\,,$$
donde los términos $A_0/2$ y $C_0/2$ son los valores medios de toda la serie $f(t)$.
Los coeficientes se calculan de igual forma usando las condiciones de
ortogonalidad. La única diferencia es que en lugar de tratar con integrales
(serie contínua) tratamos con sumatorios (serie discreta)
$$A_p=\frac{2}{N}\sum\limits_{n=1}^{N} f(t_n) cos(2\pi p n /N),\,\,\,\,p=1,2,...,N/2$$
$$A_0=\frac{2}{N}\sum\limits_{n=1}^{N} f(t_n),\,\,\,\,B_0=0$$
$$A_{N/2}=\frac{1}{N}\sum\limits_{n=1}^{N} f(t_n)cos(n\pi),\,\,\,\,B_{N/2}=0$$
$$B_p=\frac{2}{N}\sum\limits_{n=1}^{N} f(t_n) sen(2\pi p n /N),\,\,\,\,p=1,2,...,N/2$$

El $N/2$ se debe a que es el armónico con mayor frecuencia que podemos resolver,
es decir, aquel que oscila con frecuencia de Nyquist. Para $p>N/2$ las funciones
trigonométricas simplemente darán coeficientes de Fourier repetidos ya obtenidos
en el intervalo $1\le p\le N/2$. Para calcular la serie discreta de Fourier, primero
debemos calcular los argumentos de las funciones trigonométricas $2\pi n p / N$
para cada entero $p$ y $n$. Segundo, evaluamos las funciones $cos(2\pi n p / N)$ y
$sen(2\pi n p / N)$, y sumamos para los términos $f(t_n)cos(2\pi n p / N)$ y
$f(t_n)sen(2\pi n p / N)$. Por último, incrementamos $p$ y repetimos los dos pasos
anteriores.

\vspace{0.5cm}
{\textbf \noindent Ejemplo de Series Temporales Discretas
(modificado de Emery and Thompson, p387)}

Considera la serie temporal de temperatura promedia mensual por un periodo de
tres años (ver tabla y Figura).
\\
\\
\begin{tabular}{|l|cccccccccccc|}

\hline
Temperatura ($^\circ C$) & 9.6 & 9.3 & 9.8 & 10.3 & 10.7 & 11.3 & 11.5 & 12 & 12 & 9.8 & 8.1 & 7.3\\
\hline
Año 2 & 7.6 & 8.7 & 10.5 & 12 & 14 & 14.3 & 15.8 & 16.2 & 15.4 & 12.9 & 11.9 & 8.5\\
\hline
Año 3 & 9.1 & 8.9 & 10 & 9.9 & 10.1 & 11.3 & 11.5 & 12 & 12 & 14 &14.3 & 14.8\\
\hline
\end{tabular}
\\
\\

Utilizando las expresiones de arriba podemos calcular las frecuencias $f_p$,
amplitudes $A_p$, $B_p$, $C_p$, las fases $\theta_p$ y finalmente la serie
de Fourier $f(t)$. Los valores para las primeras 8 componentes estan reflejados en la tabla
\\
\\
\begin{center}
\begin{tabular}{c c c c c c c}
p & $f\,(cpm)$ & $T\,(mes)$ & $A_p\,(^\circ C)$ & $B_p\,(^\circ C)$ & $C_p\,(^\circ C)$ & $\theta_p\,(^\circ)$ \\
\hline
0 & 0 & - & 22.68 & 0 & 22.68 & 0\\
1 & 0.0278 & 36.0 & -0.263 & -0.61 & 0.67 & -113\\
2 & 0.0556 & 18.0 & 1.95 & 0.55 & 2.03 & 15.76\\
3 & 0.1111 & 12.0 & -1.16 & -1.69 & 2.05 & -124.48\\
4 & 0.1389 & 9.0 & 0.91 & -0.25 & 0.95 & -15.33\\
5 & 0.1667 & 7.2 & 0.59 & -0.31 & 0.67 & -27.53\\
6 & 0.1944 & 6 & -0.11 & -0.28 & 0.30 & -112.69\\
7 & 0.2222 & 5.14 & 0.39 & -0.3 & 0.49 & -36.83\\
8 & 0.2500 & 4.5 & 0.34 & -0.31 & 0.46 & -42.78\\
\end{tabular}
\end{center}


\begin{center}
%\includegraphics[width=0.5\textwidth]{Fourier_series_fit.pdf}
\end{center}
\begin{center}
Serie temporal de temperatura promedia mensual por tres años de muestreo (línea azul)
y la reconstrucción con la serie de Fourier de 5 componentes (línea roja).
\end{center}

\vspace{0.25cm}
{\textbf \large \noindent Serie de Fourier para variables vectoriales (complejas)}

En este caso la transformada de Fourier se aplica a una cantidad vectorial en lugar
de una cantidad escalar como temperatura, salinidad, densidad, etc. Supongamos que tenemos
las dos componentes de la velocidad
$u$ y $v$ las cuales expandemos en series de Fourier
$$u(t)=\frac{1}{2}A_0+\sum\limits^{N/2}_{p=1}[A_p cos(\omega_p t_n) + B_p sen(\omega_p t_n)]$$
$$v(t)=\frac{1}{2}C_0+\sum\limits^{N/2}_{p=1}[C_p cos(\omega_p t_n) + D_p sen(\omega_p t_n)]\,,$$
lo cual se puede escribir en versión compleja como
$$R(t)=u(t)+i v(t)=\frac{1}{2}A_0+\sum\limits^{N/2}_{p=1}[A_p cos(\omega_p t_n) + B_p sen(\omega_p t_n)]
+$$
$$+ i \left(\frac{1}{2}C_0+\sum\limits^{N/2}_{p=1}[C_p cos(\omega_p t_n) + D_p sen(\omega_p t_n)]\right)=$$
$$=\left[\frac{1}{2}A_0 + i \frac{1}{2}C_0\right] + \sum\limits^{N/2}_{p=1} \left[ (A_p + iC_p) cos(\omega_p t_n) + (B_p + i D_p) sen(\omega_p t_n) \right]\,,$$
donde $\frac{1}{2}A_0 + i \frac{1}{2}C_0=\overline{u(t)} +i \overline{v(t)}$ es la velocidad media,
$\omega_p=2\pi f_p=2\pi p/N\Delta t$ la frecuencia angular, $t_n=n\Delta t$ es el eje de tiempo, y
$(A_p\,,B_p\,,C_p\,,D_p)$ y son las amplitudes y fases de cada componente de Fourier, tanto las reales como las imaginarias. A diferencia de
la serie de Fourier real en este caso las componentes van de $p=1$ hasta $p=N$ y, por lo tanto,
estamos cubriendo ambas frecuencias {\it positivas} y {\it negativas}.
Si extraemos la velocidad media
$$R'(t)=R(t)-[\overline{u(t)} +i \overline{v(t)}]=\sum\limits^{N/2}_{p=1} \left[ (A_p + iC_p) cos(\omega_p t_n) + (B_p + i D_p) sen(\omega_p t_n) \right]\,.$$

Ahora vamos a escribir la anomalía de la serie compleja $R'(t)$ en términos de dos componentes
rotatorias ortogonales, es decir, una componente que gira en el sentido de las
agujas del reloj con amplitud $R^{-}$ y otra que gira en el sentido opuesto
a las agujas del reloj y amplitud $R^{+}$
$$R'(t)=\sum\limits^{N/2}_{p=1} \left[ e^{i\omega_p t} + e^{-i\omega_p t}\right]=$$
$$    =\sum\limits^{N/2}_{p=1}  R_p^{+}\left[cos\left(\omega_p t_n\right) + i sen\left(\omega_p t_n\right )\right] +
       \sum\limits^{N/2}_{p=1}  R_p^{-}\left[cos\left(\omega_p t_n\right) - i sen\left(\omega_p t_n\right))\right]=$$
$$=\sum\limits^{N/2}_{p=1}\left[ (R_p^+ + R_p^{-})cos\left(\omega_p t_n\right) + (R_p^+ - R_p^{-})i sen\left(\omega_p t_n\right) \right]$$

Note que $e^{i\omega_p t}=cos\left(\omega_p t_n\right) + i sen\left(\omega_p t_n\right)$ rota en el
sentido contrario a las agujas del reloj y
 $e^{-i\omega_p t}=cos\left(\omega_p t_n\right) - i sen\left(\omega_p t_n\right)$  rota
 en el sentido de las agujas del reloj.
Si comparamos las dos expresiones obtenemos las siguientes identidades
$$A_{p} + i C_{p}=R_p^+ + R_p^{-}$$
$$B_{p} + i D_{p}=(R_p^+ - R_p^{-})i$$
y de ahí obtenemos que
$$R_p^+=\frac{1}{2}\left[ A_{p} + D_{p} +i(C_{p}-B_{p})\right]$$
$$R_p^-=\frac{1}{2}\left[ A_{p} - D_{p} +i(C_{p}+B_{p})\right]\,,$$
y las magnitudes de las componentes rotatorias es
$$|R_p^+|=\frac{1}{2}\left[ (A_{p} + D_{p})^2 +(C_{p}-B_{p})^2\right]^{1/2}$$
$$|R_p^-|=\frac{1}{2}\left[ (A_{p} - D_{p})^2 +(C_{p}+B_{p})^2\right]^{1/2}\,,$$
y las fases de las componentes rotatorias son
$$\epsilon_p^+=actan\left(\frac{C_{p}-B_{p}}{A_{p} + D_{p}}\right)\,,$$
$$\epsilon_p^-=actan\left(\frac{C_{p}+B_{p}}{A_{p} - D_{p}}\right)\,.$$

La rotación de las componentes {\it clockwise} y {\it anticlockwise}
dibujan una elipse en el plano $u$ vs $v$. Puesto que ambas componentes
rotan en sentido contrario pero con la misma frecuencia, habrán momentos
que ambás apuntaran en la misma dirección (aditivas), otras
ocasiones en dirección opuesta (cancelativas). Esos tiempos
de adición y cancelación definen el eje mayor de la elipse
$L_E=R_p^+ + R_p^-$ y el eje menor de la elipse $L_e=R_p^+ - R_p^-$.
La orientación (inclinación) y la fase de estas elipses a $t=0$ es
$$\theta_e=\frac{1}{2}(\epsilon_p^+ + \epsilon_p^-)\,,$$
$$\phi_e=\frac{1}{2}(\epsilon_p^+ - \epsilon_p^-)\,.$$

Una propiedad interesante es el coeficiente rotatorio
$$r(\omega)=\frac{R^+_p - R^-_p}{R^+_p + R^-_p}\,,$$
que toma valores entre 0 y 1. Para $r=-11$ tenemos movimiento en el sentido
de las agujas del reloj, para $r=0$ tenemos un flujo unidireccional, y para $r=+1$
tenemos movimiento en el sentido contrario de las agujas del reloj.

\vspace{0.5cm}
{\textbf \large \noindent Transformada Rápida de Fourier y espectros de potencia}

La FFT (por sus siglas en inglés) es un algoritmo para calcular la serie de Fourier
discreta de forma mas eficiente computacionalmente hablando. En
este caso la FFT se debería aplicar a series temporales con longitudes múltiples
de $2$. En caso contrario, es útil rellenar de ceros nuestra serie para obtener
longitudes múltiplos de $2$. A eso se le llama `padding'. Básicamente, el algoritmo
obtiene los coeficientes de la serie discreta de Fourier

$$f(t)=\sum\limits^\infty_{p=-\infty} C^*_p e^{i\omega_p t_n}\,\,\,\,\,\,\,;\,\,\,\,\,\,\,\omega_p=2 \pi p f_0=2\pi p/T\,,$$
$$F(p)=C^*_p=\frac{1}{2}(A_p-iB_p)=\frac{1}{N} \sum\limits^{N-1}_{n=0} f(t) e^{-i \omega_p t_n} =
        \frac{1}{N} \sum\limits^{N-1}_{n=0}f(t) [cos(\omega_p t_n) - isen(\omega_p t_n) ]\,.$$

La parte real de la FFT me da las amplitudes $A_p$ y la parte
imaginaria me da las amplitudes
$-B_p$
$$Re [F(p)]=A_p=\frac{2}{N}\sum\limits_{n=0}^{N-1} f(t_n) cos(2\pi p n /N),\,\,\,\,p=0,1,...,N/2$$
$$Im [F(p)]=-B_p=-\frac{2}{N}\sum\limits_{n=0}^{N-1} f(t_n) sen(2\pi p n /N),\,\,\,\,p=0,1,...,N/2$$

En general, debemos de normalizar las amplitudes $A_p$ y $B_p$ por la longitud del registro $N$.
Así que en Matlab la amplitud de la FFT es
$${\rm abs}({\rm fft}(f(t)))/N\,,$$
y la potencia de la FFT es
$${\rm abs}({\rm fft}(f(t)).^2/N^2\,.$$

La FFT ha descompuesto una señal de $N$ elementos, $f(t)$, en un conjunto de
$N/2 +1$ ondas cosinusoidales y $N/2 + 1$ ondas sinusoidales, con las frecuencias
definidas por el índice $p=0,1,...,N/2$, i.e. $\omega_p= 2\pi f_p = 2\pi p / T = 2\pi p / N \Delta t $.
Las amplitudes de los cosenos estan contenidas en $Re [F(p)]$ y las amplitudes
de los senos en $Im [F(p)]$. Note que las frecuencias son siempre {\it positivas},
es decir, los índices $k$ siempre van de cero a $N/2$. Las frecuencias entre
$N/2$ y $N-1$ son {\it negativas}. Recuerda que el espectro frecuencial de una
señal discreta es periódico, y entonces las frecuencias son negativas
entre $N/2$ y $N-1$ al igual que en el intervalo $-N/2$ y $-1$. Los puntos
$0$ y $N/2$ separan las frecuencias negativas de las positivas. Es por ello
que, generalmente, solamente centramos nuestra atención en la parte
positiva del espectro.
La magnitud (o norma) de la transformada de Fourier discreta es
$${\rm Magnitud}=|F(p)|=\sqrt{Re [F(p)]^2 + Im[F(p)]^2}\,,$$
y la fase es
$$Phase=tan^{-1}\left( \frac{Im[F(p)]}{Re [F(p)]} \right)\,.$$

La FFT organiza los coeficientes de Fourier
(imaginarios y reales) en frecuencias {\it negativas} y {\it positivas} y
reparte la varianza de la señal equitativamente entre ellas. En
$p=0$ tenemos la media de la serie temporal, aunque debido a que hemos eliminado la media
y la tendencia de la serie temporal no debemos de preocuparnos por ella. Entre
$p=1,...,N/2$ tenemos los valores de los coeficientes de
Fourier reales y entre $p=N/2+1,...,N-1$ tenemos los complejos conjugados
de los primeros $N/2$ coeficientes. Si calculamos el valor
absoluto de la transformada de Fourier (en Matlab ${\rm abs}({\rm fft}(f(t)))/N$)
estamos calculando $A_p^2 + B_p^2$ y si solo nos quedamos con
los primeros $N/2$ elementos de la FFT, debemos de multiplicar por un
factor de $2$ para conservar la energía espectral.

\vspace{0.25cm}
{\textbf \large \noindent Estimaciones espectrales o autoespectros}

\vspace{0.5cm}
{\it \textbf \noindent (1) Espectro de amplitud}

La gráfica de la magnitud de los coeficientes complejos $|C_p^*|$ de la
serie de Fourier
$$f(t)=\sum\limits^\infty_{p=-\infty} C^*_p e^{i\omega_p t_n}$$
frente a (versus) la frecuencia $\omega_p$ se denomina espectro de
amplitud de la función periódica $f(t)$.
En Matlab (para las primeras $N/2$ componentes),
$${\rm Amplitud}=2*|C^*_p|=abs({\rm fft}(f(t)))/N\,.$$

\vspace{0.5cm}
{\it \textbf \noindent (2) Espectro de densidad de potencia ({\it Power Spectral Density}, PSD)}

El espectro de densidad de potencia (PSD, por sus siglas en inglés) es
la potencia de la FFT por unidad de frecuencia
$${\it PSD}(p)=2*|C^*_p|^2/\Delta f$$
donde $\Delta f=1/N\Delta t$ es la frecuencia fundamental.

La gráfica de ${\rm PSD}(p)$ de la
frente a (versus) la frecuencia $\omega_p$ se denomina espectro de
densidad de potencia de la función periódica $f(t)$.
Si solo nos quedamos con las $N/2$ primeras componentes en
Matlab se escribe
$${\it PSD}(p)=2*abs({\rm fft}(f(t)))^2/N^2/\Delta f\,.$$

Esta normalización tiene su fundamento en el cumplimiento del teorema de Parseval, de tal
forma que la energía total de la señal en el dominio temporal $f(t)$
(por unidad de tiempo) sea igual a la energía total de la señal en el
dominio frecuencial definido por $C^*_p$:
$$\frac{1}{T}\sum\limits^N_{n=1}|f(t_n)|^2 \Delta t =\frac{1}{N}\sum\limits^N_{n=1}|f(t_n)|^2 =
var(f(t))=\sum\limits^{\infty}_{p=-\infty} |C^*_p|^2=\sum\limits^{N/2}_{p=0}PSD(p)*\Delta f\,.$$
\\
\\
Este teorema de conservacón de energía nos informa de que la
integral bajo la curva espectral ${\rm PSD}(p)$ debe ser igual a la varianza
total de la serie temporal.

\vspace{0.5cm}
{\textbf \noindent Efectos de los extremos en estimaciones espectrales}

En general, para calcular un espectro promedio debemos fragmentar nuestra serie temporal
en bloques de igual tamaño que contengan las frecuencias de interés, realizar
espectros individuales de dichos fragmentos, y promediar todos ellos. Este método
se le conoce como Welch. Los fragmentos pueden ser únicos, es decir, sin superposición
o bien pueden ser recursivos, es decir, cuando utilizamos superposición de fragmentos.
Por ejemplo, una superposición del 50\% significa que cada fragmento empieza en la
mitad del fragmento anterior. Además de eliminar el ruido, el método de Welch también reduce
la transferencia de energía de las frecuencias pico hacia frecuencias colindantes (`leakage' en
inglés). El método de Welch reduce el ruido causado por el uso de
datos imperfectos y por el efecto `leakage'. Aqui les muestro un ejemplo de como promediar un
espectro de densidad espectral con fregmentos de tamaño $M$:

\begin{framed}
{\textbf \noindent Matlab} \\
Ejemplo del método de pWelch para calcular espectros
de frecuencia promediados o suavizados de series temporales
o espaciales, u(t) o u(x), de tamaño N.
\\
\% Suavizado con pwelch (a mano y con
subrutina de librería MATLAB)\\
M=500;\\
M2=floor(M/2);\\
nintervals=N/M;\\
PWuu=zeros(M2+1,1);\\
s=0;\\
for k=0:M:N-M\\
      s=s+1;\\
      uu=[];\\

      \%seleccionamos la serie sobre la ventana M\\
      uu=u(k+1:k+M);\\

      \%Frecuencia de muestreo\\
      dfM=1./(M*dt);\\

      \%PSD\\
      spctuu0=[];\\
      spctuu=abs(fft(uu)).$^2$/M.$^2$/dfM;\\
      PSD=[spctuu(1),2*spctuu(2:M2),spctuu(M2+1)];\\

      PWuu = PWuu + PSD(:);\\
end\\

PWuu=PWuu/nintervals;\\
fPWuu=[0:M2]*dfM;\\

\%\\
\% Librería MATLAB\\
\%\\
\%Fs=M*dfM;\\
\%[PWuuMatlab,FMatlab]=pwelch(u(:),M,0,[],Fs,'psd','oneside');

\end{framed}

Este problema de transferencia de energía o `leakage' es intrínseco al problema de que las series temporales oceanográficas son 
finitas y, por lo tanto, no son necesariamenteperiódicas, condición necesaria en el análisis de Fourier. Veamos esto con un ejemplo
de una onda cosinusoidal periódica y no-periódica.


\begin{center}
%\includegraphics[width=0.8\textwidth]{leakage.pdf}
\end{center}

Podemos observar como en el caso de la serie no-periódica existe una
transferencia de energía del pico espectral hacia las frecuencias
colindantes de forma que se reduce la amplitud del pico de interés.


\vspace{0.25cm}
{\textbf \noindent Correlación}

\begin{framed}
{\rm RECORDATORIO:}
\\
La función de covarianza cruzada se debe definir en función de un desfase $\tau$ como
 $$C_{xy}(\tau)=\frac{1}{N-m}\sum\limits^{N-m}_{n=0}x(n\Delta t) y(n\Delta t +\tau)\,,$$
donde $m=0,1,...,M$ es el número de desfases $\tau_m=m\Delta t$ y $M<N$.
Su correspondiente correlación cruzada es
  $${\rho}_{xy}(\tau)=\frac{C_{xy}}{C_{xx}(0)C_{yy}(0)}=\frac{C_{xy}}{s_{x}s_{y}}\,,$$
donde $s_x$ y $s_y$ son las varianzas de las variables $x$ e $y$, respectivamente.
\end{framed}

Kundu (1976b) define la función de correlación cruzada desfasada
entre dos series de velocidad en las profundidades 1 y 2 como
$$\rho_{\tau}=\frac{\overline{u_1'(t)u_2'(t-\tau)}}
{\left[ \overline{u_1'(t)^2}\,\, \overline{u_2'(t)^2} \right]^{1/2}}\,,$$
donde las primas $'$ indican anomalias. Esta función fue utilizada para
estudiar la propagación vertical de ondas inercio-gravitatorias y la
velocidad de fase de estas ondas $c=\Delta_{12}/\tau$.
Kundu (1976a) introduce el
coeficiente de correlación complejo
$$\rho=\frac{\overline{w_1^*(t)w_2(t)}}
{\left[ \overline{w_1^*(t)w_1^*(t)}\,\, \overline{w_2^*(t)w_2(t)} \right]^{1/2}}\,,$$
donde $w=u+i v$, los asteriscos $*$ indican complejo conjugados, y los
subíndices $1$ y $2$ se refieren a dos estaciones de medida. La cantidad
$\rho$ es un número complejo cuya magnitud ($\le1$) nos da una medida de
correlación promedia y cuyo ángulo de fase da el angulo promedio, medido en el sentido
contrario a las agujas del reloj, del segundo vector con respecto del primero.
Por ejemplo, un ángulo de fase negativo entre las profundidades $50$ y $100\,{\rm m}$
implica que la señal llega primero a $z=-100\,{\rm m}$ y luego a
$z=-50\,{\rm m}$, es decir, podría tratarse de una onda interna cuyas fases se propagan
hacia arriba.

\vspace{0.5cm}
{\it \textbf \noindent (3) \noindent Espectro cruzado}

Con análisis de espectros cruzados pretendemos comprender la
relación entre dos series temporales en función de la
frecuencia. Por ejemplo, observamos en dos localizaciones
espectros con picos en las mismas frecuencias y queremos saber
si dichos armónicos estan relacionados.

Supongamos dos series de Fourier $x(t)$ e $y(t)$
$$x(t)=\bar{x}+\sum\limits^{N/2}_{p=1} A_{xk} cos(\omega_p t_n) + B_{xp}sen(\omega_p t_n)\,,$$
$$y(t)=\bar{y}+\sum\limits^{N/2}_{p=1} A_{yk}cos(\omega_p t_n) + B_{yp}sen(\omega_p t_n)\,.$$
Utilizando las condiciones ortogonalidad entre las funciones sinusoidales y
cosinusoidal, la covarianza entre las variables $x$ e $y$ es
$$\overline{x'y'}=\sum\limits^{N/2}_{p=1} \frac{1}{2}( A_{xp}A_{yp}  + B_{xp}B_{yp})=\sum\limits^{N/2}_{p=1}Co(p)\,,$$
donde $Co(p)$ es el co-espectro de $x$ e $y$.

Supongamos dos series de Fourier $x(t)$ e $y(t)$ definidas en la forma compleja
(el asterisco ha sido eliminado en esta notación)
$$x(t)=\bar{x}+\sum\limits^{N/2}_{p=1}
  C_{xp} e^{i\omega_p t_n}=\bar{x}+
  \sum\limits^{N/2}_{p=1}\frac{1}{2}\left(A_{xp} - iB_{xp} \right)e^{i\omega_p t_n}=\bar{x}+
  \sum\limits^{N/2}_{p=1}F_x(p)$$
$$y(t)=\bar{y}+\sum\limits^{N/2}_{p=1}
  C_{yk} e^{i\omega_p t_n}=\bar{y}+
  \sum\limits^{N/2}_{p=1}\frac{1}{2}\left(A_{yp} - iB_{yp} \right)e^{i\omega_p t_n}=\bar{y}+
  \sum\limits^{N/2}_{p=1}F_y(p)\,.$$

Si ahora calculamos las varianzas
$$\overline{x'^2}=\sum\limits^{N/2}_{p=-N/2}F_{xx}(p)\,,$$
donde
$$F_{xx}(p)=2\frac{1}{2}\left(A_{xp} - iB_{xp} \right)e^{i\omega_p t_n}\frac{1}{2}\left(A_{xp} + iB_{xp} \right)e^{-i\omega_p t_n}
=2F_x(p) F^*_x(p)=|C_{xp}|^2\,,$$
y el asterisco indica complejo conjugado.
Para la variable $y$ de igual forma obtenemos:
$$\overline{y'^2}=\sum\limits^{N/2}_{p=-N/2}F_{yy}\,;\,\,\,\,\,F_{yy}(p)=2F_y(p) F^*_y(p)=|C_{yp}|^2$$

De las expresiones anteriores se deduce que covarianza se puede calcular
en el espacio espectral como
$$\overline{x'y'}=Re\left[\sum\limits^{N/2}_{p=-N/2}F_{xy}(p)\right]\,,$$
donde
$$F_{xy}(p)=2\frac{1}{2}\left(A_{xp} - iB_{xp} \right)e^{i\omega_p t_n}\frac{1}{2}\left(A_{yp} + iB_{yp} \right)e^{-i\omega_p t_n}
=2F_x(p) F^*_y(p)=|C_{xp}||C_{yp}|e^{i(\theta_{xp}-\theta_{yp})}\,.$$
El factor $e^{i(\theta_{xp}-\theta_{yp})}$ aparece para considerar que ambas series periódicas
no estan en fase.

\vspace{0.5cm}
{\it \textbf \noindent (4) \noindent Espectro cruzado complejo}

Si escribimos $F_{xy}(p)$ en términos de los coeficientes de Fourier reales
$$F_x(p) F^*_y(p)=\frac{1}{2}\left(A_{xp} - iB_{xp} \right)e^{i\omega_p t_n}\frac{1}{2}\left(A_{yp} + iB_{yp} \right)e^{-i\omega_p t_n}=$$
$$=\frac{1}{4}\left[A_{xk}A_{yk}  + B_{xk}B_{yp} + i\left(A_{xp}B_{yp} - A_{yp}B_{xp}\right)\right]\,.$$
Para el caso de series $x(t)$ e $y(t)$ reales sabemos que las frecuencias
negativas son los complejos conjugados de las frecuencias positivas y entonces
$$A_k=A_{-k}\,\,\,\,\,{\rm y}\,\,\,\,\,B_k=B_{-k}\,,$$
y
$$F_x(p)F^*_y(p)=F_x(-p)F_y^*(-p)\,,$$
y como conclusión
$$F_{xy}(p)+F_{xy}(-p)=\frac{1}{2}\left[ A_{xp}A_{yp}  + B_{xp}B_{yp} + i\left(A_{xp}B_{yp} - A_{yp}B_{xp}\right)\right]\,,$$
que es espectro cruzado de $x$ e $y$ para el armónico $p$. De esta expresión encontramos que
$$F_{xy}(p)+F_{xy}(-p)=2F_{xy}(p)=Co(p) + i Q(p)\,,$$
donde
$Co(p)=\frac{1}{2}( A_{xp}A_{yp}  + B_{xp}B_{yp})$ es el co-espectro del armónico p
y $Q(p)=\frac{1}{2}(A_{xp}B_{yp} - A_{yp}B_{xp})$ es el espectro de cuadratura del
armónico $k$.

En notación compleja el espectro cruzado
$$F_{xy}(p)=C_{xp}C_{yp}e^{i(\theta_{xp}-\theta_{yp})}=C_{xp}C_{yp}\left(cos(\theta_{xp}-\theta_{yp})+ isen(\theta_{xp}-\theta_{yp}) \right)\,.$$

$$\theta_{xp}=\theta_{yp}\,\,\,\,\,\text{entonces}\,\,\,\,F_{xy}(p) \text{es real}$$
$$\theta_{xp}\ne\theta_{yp}=\pm\frac{\pi}{2}\,\,\,\,\,\text{entonces}\,\,\,\,F_{xy}(p) \text{es complejo}$$

Entonces el co-espectro (la parte real del espectro cruzado) esta en fase con
la señal y el espectro de cuadratura esta totalmente desfasado.

\vspace{0.5cm}
{\it \textbf \noindent (5) Espectro de coherencia}

Para una única componente $p$, el espectro de coherencia al cuadrado entre dos series $x$ e $y$ se define
$$Coh^2(p)=\frac{|F_{xy}(p)|^2}{F_{xx}F_{yy}}=\frac{|C_{xp}C_{yp}|^2}{C_{xp}^2C_{yp}^2}\,,$$
donde $|Coh^2(p)|^{1/2}$ es su magnitud y $\phi_{xy}(p)$ es el ángulo de desfase entre las
dos componentes $p$ de $x$ e $y$.

El espectro de coherencia al cuadrado nos indica el grado de correlación existente entre dos
señales. Dos señales estan altamente correlacionadas si la magnitud del espectro de
coherencia al cuadrado es $\simeq 1$ y su fase es $\phi_{xy}(p)\simeq 0$.

\section{Métodos de filtrado y suavizado}
\subsection{Convolución y funciones respuesta (ventanas {\it espectrales})}

La convolución de dos funciones $f(t)$ y $g(t)$ sobre un registro finito $[0,T]$ se define como
$$[f*g](t)=\frac{1}{T}\int\limits^{T}_{0}f(\tau)g(t-\tau) d\tau\,.$$
O también se puede expresar sobre un registro infinito como
$$[f*g](t)=\int\limits^{\infty}_{-\infty}f(\tau)g(t-\tau) d\tau = \int\limits^{\infty}_{-\infty}g(\tau)f(t-\tau) d\tau\,.$$

La convolución satisface las siguientes propiedades
$$f*g=g*f$$
$$f*(g*h)=(f*g)*h$$
$$f*(g+h)=(f*g)+(f*h)$$

Ahora retomemos las definiciones de serie de Fourier y la transformada de Fourier:
\begin{framed}
$$f(t)={\cal F}^{-1}[F(\omega)](t)=\sum\limits^{\infty}_{p=-\infty} F(\omega_p) e^{i\omega_p t};\,\,\,\,\,\text{serie de Fourier}$$
y
$$F(\omega_p)=C^*_p=\frac{1}{T}\int\limits^{T}_{0}f(t)e^{-i\omega_p t} dt;\,\,\,\,\,\text{Transformada de Fourier}$$
También se pueden escribir alternativamente en forma integral (contínua) como:
$$f(t)=\int\limits^{\infty}_{-\infty} F(\omega) e^{i\omega t} d\omega\,,$$
y
$$F(\omega)=\int\limits^{\infty}_{-\infty}f(t)e^{-i\omega t} dt$$

\end{framed}

Vamos ahora a deducir el teorema de la convolución. Para ello vamos a partir de la
definición de convolución:
$$f*g=\frac{1}{T}\int\limits^{T}_{0}f(\tau)g(t-\tau) d\tau=
      \frac{1}{T}\int\limits^{T}_{0}f(\tau)\sum\limits^{\infty}_{p=-\infty} G(\omega_p) e^{i\omega_p (t-\tau)} d\tau=$$
$$=\sum\limits^{\infty}_{p=-\infty} G(\omega_p) \left[\frac{1}{T}\int\limits^{T}_{0}f(\tau)e^{-i\omega_p \tau} d\tau \right]
e^{i\omega_p t}=\sum\limits^{\infty}_{p=-\infty} G(\omega_p)F(\omega_p)e^{i\omega_p t}={\cal F}^{-1}[G(\omega)F(\omega)](t)\,.$$
Si aplicamos transformada de Fourier a ambos lados del igual obtenemos:
$${\cal F}(f*g)=G(\omega_p)F(\omega_p)={\cal F}[g(t)]{\cal F}[f(t)]\,,$$
es decir, la transformada de Fourier de la convolución de $f$ y $g$ es equivalente a multiplicar en
el espacio espectral las transformadas de Fourier de las funciones individuales.
La correlación cruzada desfasada de $f(t)$ y $g(t)$ en forma integral se puede definir como
$$C_{fg}(\tau)=\frac{1}{T}\int\limits^{T}_{0} f(\tau) g(t+\tau) d\tau={\cal F}^{-1}[G(\omega)F(-\omega)](t)=
{\cal F}^{-1}[G(\omega)F^*(\omega)](t)\,,$$
es decir, si multiplicamos la transformada de Fourier de una función por el complejo
conjugado de la transformada de Fourier de otra función es equivalente a
la transformada de Foruier de la correlación cruzada desfasada entre ellas.
Este se le conoce por el teorema de correlación.
Para el caso particular que sea la misma función $g(t)$ la que se correlaciona, entonces:
$${\cal F}[C_{gg}(\tau)]=G(\omega)G^*(\omega)=|G(\omega)|^2\,,$$
es decir, la transformada de Fourier de la autocorrelación es igual al
espectro de potencia de la función $g(t)$. Este se denomina el teorema de Weiner-Khinchin.

El concepto de convolución es útil en el filtrado de señales periódicas. En
general vamos a convolucionar nuestra señal $f(t)$ con la denominada función
respuesta $r(t)$. La función $r(t)$ es típicamente una función pico
que cae a cero en ambas direcciones desde el máximo (o pico).

\begin{center}
%\includegraphics[width=0.5\textwidth]{convolucion_senal_respuesta.pdf}
\end{center}

Puesto que la función respuesta es mas ancha que algunas estructuras de
pequeña escala de nuestra señal original, estas serán suavizadas tras
realizar la convolución.
\\
\\
{\textbf NOTA:}
Por el teorema de convolución filtrar en el
dominio temporal convolucionando es equivalente a multiplicar la
transformada de Fourier de la señal con la transformada de Fourier
de la función respuesta.

Las ventanas mas comunes para suavizar señales son las de
\\
\\
(1) `Boxcar'
\begin{equation*}
  r(t)=
  \left\lbrace
  \begin{array}{l}
     1 \text{ if } 0\le t\le T \\
     0 \text{ if } t>T \\
  \end{array}
  \right.
\end{equation*}

\begin{center}
%\includegraphics[width=0.5\textwidth]{Boxcar.png}
\end{center}

La transformadad de Fourier es la función {\it sinc}
$$R(\omega)={\it sinc}=\frac{sin\left(\frac{\omega T}{2}\right)}{\frac{\omega T}{2}}\,.$$

\begin{center}
%\includegraphics[width=0.5\textwidth]{sinc.pdf}
\end{center}

Esta función respuesta tiende a cero cuando $\omega T / 2$ se acerca a
cero, es decir, para $\omega T=2 n \pi;\,\,\text{para}\,\,n=1,2,3,...$. Esta
no es una ventana o función respuesta recomendable debido a los lóbulos
de menor amplitud alrededor del pico. En general respuestas tipo ondas
sinusoidales o cosinusoidales amortiguadas a ambos lados del pico no son
deseables.

\vspace{0.75cm}
{\noindent}(2) Hanning
 \begin{equation*}
  r(t)=\left\lbrace
  \begin{array}{l}
     \frac{1}{2}\left( 1-cos\frac{2\pi t}{T}\right) \text{ if } -T/2 \le t \le T/2 \\
     0 \text{ if } {\rm contrario} \\
  \end{array}
  \right.
  \end{equation*}
(3) Hamming
 \begin{equation*}
  r(t)=\left\lbrace
  \begin{array}{l}
     \left(0.54  + 0.46 cos(\frac{\pi t}{T}) \right) \text{ if } -T/2 \le t \le T/2 \\
     0 \text{ if } {\rm contrario} \\
  \end{array}
  \right.
  \end{equation*}

\begin{center}
%\includegraphics[width=0.75\textwidth]{respuesta_ventanas.png}
\end{center}

Es evidente que este tipo de filtro es un suavizado o filtro pasa bajo. Sin embargo,
siempre podemos recuperar facilmente la señal de alta frecuencia (filtro pasa
altas) restando a la señal original la serie suavizada con convolución.

Imaginemos que tenemos una serie temporal $u(t)$ con
un paso temporal de $dt=1\,{\rm h}$. Entonces, para
suavizar $u(t)$ de tal forma que se eliminen las
señales con periodos
menores de $T=48\,{\rm h}$, es decir, un filtro pasa
baja con frecuencia de corte $1/48\,{\rm h}^{-1}$ debemos
convolucionar $u(t)$ con una función de resouesta $r(t)$ o
ventana. En este ejemplo {\it Matlab} se muestra como
programar un suavizado
\begin{framed}
{\textbf \noindent Matlab}:\\
\%Crea función respuesta r(t) o ventana `'espectral''\\
T=48; %Periodo de corte (se filtran ondas con periodos <48h).\\
p1=hanning(48);\\
r1=p1./sum(p1); \%Normaliza (0,1)\\
p2=hamming(48);\\
r2=p2./sum(p2); \%Normaliza (0,1)\\

\%Convolución u(t) y r(t)\\
\%'same' mantiene las dimensiones de la serie filtrada\\
us1=conv(u,r1,'same');\\
us2=conv(u,r2,'same');\\

\end{framed}

Ya hemos visto que podemos suavizar una señal simplemente
con la convolución en el dominio temporal de la señal con
una ventana o función respuesta (Boxcar, Hanning, Hamming,etc.).
Suavizar una señal es comparable a un filtro de pasa baja,
es decir, un filtro que solamente deja pasar las frecuencias
bajas y elimina (pone a cero) las altas frecuencias.
De forma ideal los filtros en el dominio frecuencial los
representamos como: \\

(1) Pasa baja (filtra las altas frecuencias),
\begin{equation*}
|R(\omega)|=
  \left
  \lbrace
  \begin{array}{l}
     1 \text{ si } |\omega| \le \omega_c \\
     0 \text{ si } \omega_c \le \omega   \\
  \end{array}
  \right.
  \end{equation*}
\\
(2) Pasa banda
(filtra las frecuencias fuera de la banda)
\begin{equation*}
|R(\omega)|=\left\lbrace
  \begin{array}{l}
     1 \text{ si } \omega_{c1}\le|\omega|\le \omega_{c2} \\
     0 \text{ si } \text({\rm lo}\,\,\,{\rm contrario}) \\
  \end{array}
  \right.
  \end{equation*}
\\
(3) Pasa alta (filtra las bajas frecuencias)
\begin{equation*}
|R(\omega)|=\left\lbrace
  \begin{array}{l}
     0 \text{ si } |\omega|\le \omega_c \\
     1 \text{ si } \omega_c\le\omega \\
  \end{array}
  \right.
  \end{equation*}

\begin{center}
%\includegraphics[width=0.5\textwidth]{esquemas_filtros.png}
\end{center}

\subsection{Promedio corrido}

Veamos primero un ejemplo sencillo de filtrado paso bajo con un promedio corrido de
dos puntos. Este sería el caso de filtrar utilizando la función {\it smooth.m} de
Matlab. Para dos puntos sería $>> f_{suavizada}=smooth(f,2);$.
\\
Promedio corrido con dos puntos es simplemente el valor promedio
$$y(n)=\frac{s(n) + s(n-1)}{2}\,,$$
donde $s(n)$ es una señal periódica, $s(-1)=s(N)$, $N\ge 2$. La función $y(n)$
es una versión suavizada con altas frecuencias eliminadas y bajas frecuencias mantenidas.
Para ver esto definimos $s(n)=sen(2\pi f n/N)$, y entonces
$$y(n)=\frac{1}{2} s(n) + \frac{1}{2} s(n-1)=\frac{1}{2} sen(2\pi f n/N) + \frac{1}{2} sen(2\pi f (n-1)/N)=$$
$$=\frac{1}{2}sen(2\pi f n/N) + \frac{1}{2}\left[ sen(2\pi f n/N)cos(2\pi f/N) - cos(2\pi f n/N)sen(2\pi f/N)\right]=$$
$$=\frac{1}{2}\left[ 1+cos(2\pi f/N)\right]sen(2\pi f n/N) - \frac{1}{2} cos(2\pi f n/N)sen(2\pi f/N)=$$
$$=A_1sen(2\pi f n/N) - A_2cos(2\pi f n/N)\,,$$
donde
$$A_1=\frac{1}{2}\left[ 1+cos(2\pi f/N)\right]\,\,;\,\,\,\,\,A_2=sen(2\pi f/N)\,.$$

Para bajas frecuencias, es decir, $f\sim 0$ se cumple que $A_1\sim 1$ y $A_2 \sim 0$ y
entonces
$$y(n)\sim sen(2\pi f n/N)=s(n)$$
y las bajas frecuencias son prácticamente mantenidas. Por el contrario para
altas frecuencias, es decir, $f\sim N/2$, $A_1\sim 0$, $A_2\sim0$, y entonces
$y(n)\sim0$ y consecuentemente las altas frecuencias son prácticamente
eliminadas.

La fórmula general para el promedio corrido es
$$y(n)=\frac{1}{M}\sum\limits^{(M-1)/2}_{p=-(M-1)/2} s(n+p)\,,$$
donde $y( )$ es el valor de la serie filtrada y $s( )$
es la serie original sin filtrar, $M$ es el número
de puntos usados en el promedio. Por ejemplo, en un
promedio corrido de $5$ puntos, el valor en el
punto $30$ será
$$y(30)=\frac{s(28)+s(29)+s(30)+s(31)+s(32)}{5}\,.$$

Es evidente que este tipo de filtro es un suavizado o filtro pasa bajo. Sin embargo,
podemos recuperar facilmente la señal de alta frecuencia restando a la señal
original la serie filtrada.

\begin{framed}
{\textbf \noindent Matlab}:
\\
\%Suavizado/filtrado con ventana corrida ({\it moving average})\\
us3=[];\\
Ws=48;\\
for n=Ws:length(u)\\
    us3\_Hamming(n)=sum(u(n-Ws+1:n).*hamming(Ws))/Ws;\\
    us3\_Hanning(n)=sum(u(n-Ws+1:n).*hanning(Ws))/Ws;\\
end\\

\end{framed}


\subsection{Filtros generales coseno}

Supongamos un simple filtro simétrico obtenido como la
convolución entre una función de pesos $r(t)$ y la
señal $x(t)$
$$y_n=\sum\limits^{\infty}_{p=-\infty} r_p x_{n-p}\,\,\,\,{\rm donde}\,\,\,\,r_p=r_{-p}\,,$$
son pesos elegidos adecuadamente.
El efecto de filtrado se observa mejor en el dominio
frecuencial. Queremos calcular la transformada de
Fourier de una serie temporal $f(t)$, la cual ha
sido desfasada un tiempo $\Delta t = a$:
$$f(t\pm a)=\int\limits^{\infty}_{-\infty} F(\omega)e^{i\omega (t\pm a)} d{\omega}=
\int\limits^{\infty}_{-\infty}  \left[F(\omega) e^{i \omega t}\right]e^{\pm i \omega a} d{\omega}$$

De esta expresión deducimos que la transformada de Fourier de una serie desfasada
por un intervalo de tiempo $\Delta t$ es igual a la transformada de Fourier de la
serie no desfasada multiplicada por un factor
$$e^{\pm i\omega \Delta t}\,.$$
Usando este resultado, la transformada de Fourier de
$y_n$ se puede escribir como
$$Y(\omega)={\cal F}[y_n]=\sum\limits^{\infty}_{p=-\infty} r_p e^{-i\omega_p \Delta t} X(\omega)\,,$$
donde $X(\omega)$ y $Y(\omega)$ son la transformada de Fourier de $y(t)$ y $x(t)$, y la función respuesta en el dominio frecuencial es
$$R(\omega)=\frac{Y(\omega)}{X(\omega)}=\sum\limits^{\infty}_{p=-\infty} r_p e^{-i\omega_p \Delta t}\,.$$

{\noindent}Puesto que $r_p=r_{-p}$  y
$${\rm cos}(x)=\frac{e^{ix} + e^{-ix}}{2}\,,$$
podemos escribir la función respuesta del filtrado deseado
como
$$R(\omega)=\sum\limits^{\infty}_{p=-\infty} r_p e^{-i\omega_p \Delta t}=
r_0 + \sum\limits^{\infty}_{p=1} r_p e^{i\omega_p \Delta t}
+ \sum\limits^{\infty}_{p=1} r_{-p} e^{-i\omega_p \Delta t} =
r_0 + \sum\limits^{\infty}_{p=1} r_p \left[ e^{i\omega_p \Delta t} + e^{-i\omega_p \Delta t}\right]=
$$
$$
=r_0 + 2\sum\limits^{\infty}_{p=1} r_p \left[ \frac{e^{i\omega_p \Delta t} + e^{-i\omega_p t}}{2}\right]=
r_0 + 2\sum\limits^{\infty}_{p=1}r_p cos(\omega_p \Delta t)$$

En general los pesos $r_p$ se van a calcular utilizando la
siguiente expresión:
$$r_p=\frac{1}{\omega_N}\int\limits^{\omega_N}_0 R(\omega_p) cos(\omega_p \Delta t
)d{\omega}$$

Por ejemplo para un filtro pasa bajo $R(\omega)=1$ para $0<|\omega_p|\le \omega_c$ y la
integral para calcular los pesos queda
$$r_p=\frac{1}{\omega_N}\int\limits^{\omega_c}_0 R(\omega_p) cos(\omega_p \Delta t
)d{\omega}=\frac{\omega_c}{\omega_N}\frac{sen(\omega_c p \Delta t)}{\omega_c p \Delta t}=
      \frac{1}{\omega_N}\frac{sen(\pi p \omega_c / \omega_N)}{\pi p / \omega_N}=$$
$$=\frac{sen(\pi p \omega_c / \omega_N)}{\pi p}\,\,\,\,\,\,\,\,\,\,p=1,2,...,N$$

Para $p=0$ entonces
$$r_0=\frac{1}{\omega_N}\int\limits^{\omega_c}_0 R(\omega_p) cos(\omega_p \Delta t
)d{\omega}=\frac{1}{\omega_N}\int\limits^{\omega_c}_0 d{\omega}=\frac{\omega_c}{\omega_N}\,.$$
Y la función respuesta es
$$R(\omega)=\frac{\omega_c}{\omega_N} + 2\sum\limits^{\infty}_{p=1}
\frac{sen(\pi p \omega_c / \omega_N)}{\pi p}cos(\pi p \omega / \omega_N)$$

Veamos que forma tiene este filtro en el dominio frecuencial, asumiendo
un número finito de $N$ coeficientes de Fourier, i.e., $p=1,2,...,N$,
frecuencia de Nyquist $f_N=1$ y frecuencia de corte $f_c=1$:

\begin{center}
%\includegraphics[width=0.5\textwidth]{filtro_general_coseno.pdf}
\end{center}

En la figura observamos oscilaciones
con longitud de onda
$$\lambda=\frac{4f_N}{2N+1}\,,$$

Esta longitud de onda coincide con el ancho de banda de transición del filtro,
es decir, del pico hasta la base indicado en la figura por las líneas rojas.
Para filtrar únicamente debemos:
\\
(i) multiplicar la respuesta espectral $R(f)$
por la transformada de Fourier de la señal y regresar con la transformada
inversa
$$x(t)[{\it filtrado}]={\cal F}^{-1}[R(f)X(f)]\,,$$
\\
(ii) convolucionar la respuesta en el dominio temporal $r(t)$ por
la serie temporal.
$$x(t)[{\rm filtrado}]=[r(t)*x](t)\,.$$
\\
{\noindent}Si queremos un filtro pasa alta, usamos $r_p({\rm pasa}\,\,\,{\rm alto})=1-r_p$.
Y la función respuesta sería
$$R(\omega)[{\rm pasa}\,\,\,{\rm alto}]=1-R(\omega)=1-\frac{\omega_c}{\omega_N} - 2\sum\limits^{\infty}_{p=1}
\frac{sen(\pi p \omega_c / \omega_N)}{\pi p}cos(\pi p \omega / \omega_N)$$
\\
Veamos ahora de nuevo el promedio corrido pero esta vez
usando el método de Fourier. De nuevo decir que el promedio
corrido reemplaza el valor central de la ventana por el promedio
de los valores que rodean a ese punto.
Para este ejemplo los pesos son siempre iguales $r_p=1/T$ para el intervalo
$-N<p<N$, donde $T=1/(2N+1)$ es el tamaño de
la ventana `boxcar'. De esta forma
\\
$$T=2N+1=3\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
R(\omega)=\frac{1}{3} + \frac{2}{3}cos(\omega \Delta t)
\,\,\,\,\,\,\,\,\,\,\,\,0<\omega<\pi/\Delta t$$

$$T=2N+1=5\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
R(\omega)=\frac{1}{5} + \frac{2}{5}cos(\omega \Delta t)+ \frac{2}{5}cos(2\omega \Delta t)
\,\,\,\,\,\,\,\,\,\,\,\,0<\omega<\pi/\Delta t$$

$$T=2N+1=7\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
R(\omega)=\frac{1}{7} + \frac{2}{7}cos(\omega \Delta t)+ \frac{2}{7}cos(2\omega \Delta t)+ \frac{2}{7}cos(3\omega \Delta t)
\,\,\,\,\,\,\,\,\,\,\,\,0<\omega<\pi/\Delta t$$

\begin{center}
%\includegraphics[width=0.5\textwidth]{respuesta_promedio_corrido.pdf}
\end{center}

Como volvemos a observar, las transformadas de Fourier de funciones
de peso cuadradas (`boxcar') no son adecuadas debido a las oscilaciones
o lóbulos menores.

\subsection{Filtro Lanczos pasabaja}

Sea $R(f)$ la función respuesta de un filtro pasa baja, en
donde $f$ corresponde a la frecuencia en ciclos por unidad de tiempo,
$f_N$ es la frecuencia de Nyquist, y $fc$ es la frecuencia de corte.

\begin{center}
%\includegraphics[width=0.5\textwidth]{filtro_ideal_lanczos.png}
\end{center}

Lanzcos se dio cuenta que las oscilaciones de los filtros cosinusoidales con
longitud de onda $\lambda(f)=4f_N/2N+1$ podían ser reducidas si se realiza un
suavizado de la fucnión respuesta $H(f)$. Para ello realizó un
promedio corrido de tamaño igual a la longitud de onda de las oscilaciones,
es decir, $\lambda$. Esto se puede escribir como
$$\widetilde{R}(f)=\frac{1}{\lambda(f)}\int\limits^{f+\lambda/2}_{f-\lambda/2} R(f) d{f}\,,$$
donde ya hemos visto que
$$R(f)=\frac{f_c}{f_N} + 2\sum\limits^{N}_{p=1}
r_p cos(\pi p f / f_N)\,.$$

Un filtro de media corrida no tiene efecto en el promedio, entonces
$$\widetilde{R}(f)=\frac{f_c}{f_N} + \frac{1}{\lambda}
\int\limits^{f+\lambda/2}_{f-\lambda/2} 2 \sum\limits^{N}_{p=1}
r_pcos(\pi p f / f_N)d{f}=$$
$$=\frac{f_c}{f_N} + \frac{2}{\lambda}\sum\limits^{N}_{p=1} r_p
\left[\frac{1}{\pi p /f_N} sen\left( \frac{\pi p
f}{f_N}\right)\right]^{f+\lambda/2}_{f-\lambda/2}=$$
$$=\frac{f_c}{f_N}+ \frac{2}{\lambda}\sum\limits^{N}_{p=1}
r_p\frac{f_N}{\pi p}\left[sen\left( \frac{\pi p
(f+\lambda/2)}{f_N}\right)-sen\left( \frac{\pi p
(f-\lambda/2)}{f_N}\right) \right]=$$
$$=\frac{f_c}{f_N}+ \frac{2}{\lambda}\sum\limits^{N}_{p=1}
r_p\frac{f_N}{\pi p}\left[2cos\left( \frac{\pi p
f}{f_N}\right)sen\left( \frac{\lambda \pi p}{2f_N}\right) \right]=
\frac{f_c}{f_N}+ \frac{2}{\lambda}\sum\limits^{N}_{p=1}
r_p\frac{f_N}{\pi p}\left[2cos\left( \frac{\pi p
f}{f_N}\right)sen\left( \frac{2 \pi p}{2N+1}\right) \right]=$$
$$=
\frac{f_c}{f_N}+ \frac{2(2N+1)}{4f_N}\sum\limits^{N}_{p=1}
r_p\frac{f_N}{\pi p}\left[2cos\left( \frac{\pi p
f}{f_N}\right)sen\left( \frac{2 \pi p}{2N+1}\right) \right]=$$
$$=
\frac{f_c}{f_N}+ 2\sum\limits^{N}_{p=1}
r_p\frac{2N+1}{2 \pi p}\left[cos\left( \frac{\pi p
f}{f_N}\right)sen\left( \frac{2 \pi p}{2N+1}\right) \right]=$$
$$=
\frac{f_c}{f_N}+ 2\sum\limits^{N}_{p=1}
r_p\frac{1}{\frac{2 \pi p}{2N+1}}\left[cos\left( \frac{\pi p
f}{f_N}\right)sen\left( \frac{2 \pi p}{2N+1}\right) \right]=
\frac{f_c}{f_N}+ 2\sum\limits^{N}_{p=1}
r_p \sigma_p cos\left( \frac{\pi p
f}{f_N}\right)\,,$$
donde
$$\sigma_p=\frac{sen\left( \frac{2 \pi p}{2N+1}\right)}{\frac{2 \pi p}{2N+1}}=
sinc\left(\frac{2 \pi p}{2N+1} \right)\,,$$
es una función {\it sinc} como la respuesta espectral de un
filtro rectangular o `boxcar'. A este factor de suavizado se
le suele llamar peso sigma.

\begin{center}
%\includegraphics[width=0.5\textwidth]{filtro_lanczos.pdf}
\end{center}

En la figura se observa que la frecuencia de corte es la la frecuencia
que corta el 50\% de la magnitud de la respuesta. La frecuencia
efectiva es $f=f_c+\lambda/2$.

\begin{framed}
{\noindent \textbf MATLAB:}\\
{\textbf Programa para crear el filtro de Lanczos (Autor: Dr. Modesto Ortiz)}\\
{\noindent}N=10;   \%tamano filtro\\
p=1:N;  \%indices de tiempo \\
fc=0.5; \%frecuencia de corte a 0.5\\
fn=1.0; \%frecuencia de Nyquist \\
f=-fn:0.01:fn; \%generamos vector de frecuencias (positivas y negativas)\\
k=length(f);   \%dimension vector frecuencias \\
\%funcion pesos o respuesta en el dominio temporal\\
rp=sin(pi*p*fc/fn)./(pi*p);\\
\%calculamos el factor sigma de suavizado (factores lanczos)\\
sigma=sin(2*pi*p/(2*N+1))./(2*pi*p/(2*N+1)); \\
\%Calculamos la funcion respuesta de un filtro general en el
\%dominio espectral\\
R=[];\\
for m=1:k; \%recorremos todas las frecuencias\\
    R(m) = fc/fn + \\
    2*sum(rp.*sigma.*cos(p*pi*f(m)/fn));\\
end\\
lambda=4*fn/(2*N+1);\\
y0=[-0.2:0.1:1.2];x0=0.4*ones(1,length([-0.2:0.1:1.2]));\\
y1=[-0.2:0.1:1.2];x1=0.4+lambda*ones(1,length([-0.2:0.1:1.2]));\\
\%Graficamos\\
f1=figure;\\
plot(f,R,'b'); \\
hold on;\\
plot(x0,y0,'r-');\\
hold on;\\
plot(x1,y1,'r-');\\
grid on;\\
xlabel('f(ciclos/tiempo)','fontsize',18);\\
ylabel('R(f)','fontsize',18);\\
set(gca,'fontsize',18);\\
axis([-1 1 -0.2 1.2]);\\
\end{framed}

\subsection{Filtro Lanczos pasabanda}

En el dominio de las frecuencias, el filtro de pasa-banda se obtiene
convolucionando el filtro pasa-bajas con la transformada de Fourier de la
función coseno:
$$\widetilde{R}_b(f)=\widetilde{R}(f)*\left[\delta(f-f_o) + \delta(f+f_0) \right]\,,$$
donde * significa convolución,
$${\cal F}[cos(2\pi f_0 x)](f)=\int\limits^{\infty}_{-\infty} e^{-2\pi i f x} cos(2\pi
f_0 x) dx=\int\limits^{\infty}_{-\infty} e^{-2\pi i f x}
\left(\frac{e^{2\pi i f_0 x} +e^{-2\pi i f_0 x}}{2}\right) dx = $$
$$=\frac{1}{2}\int\limits^{\infty}_{-\infty}\left[e^{-2\pi i (f-f_0) x} + e^{-2\pi i (f+f_0) x} \right] dx$$
$$=\frac{1}{2}\left[ \delta(f-f_0) + \delta(f+f_0)\right]\,,$$
y la delta de dirac se define como
$$\delta(x)={\cal F}[1](f\pm f_0)=\int\limits^{\infty}_{-\infty} e^{-2\pi i (f\pm f_0) x} dx$$

Vemos que la transformada de Fourier del coseno se ha
multiplicado por un factor de 2 para que la respuesta
del filtro sea unitaria (normalización). El resultado
de la convolución es
$$\widetilde{R}_b(f)=\widetilde{R}(f)*\left[\delta(f-f_o) + \delta(f+f_0) \right]=
\widetilde{R}(f-f_0)+\widetilde{R}(f+f_0)=$$
$$=\frac{f_c}{f_N}+ 2\sum\limits^{N}_{p=1}
{r_p} \sigma_p cos\left( \frac{\pi p (f-f_0)}{f_N}\right) +
\frac{f_c}{f_N}+ 2\sum\limits^{N}_{p=1}
{r_p} \sigma_p cos\left( \frac{\pi p (f+f_0)}{f_N}\right)=$$
$$=2\frac{f_c}{f_N}+ 2\sum\limits^{N}_{p=1}
{r_p} \sigma_p \left[ cos\left( \frac{\pi p f}{f_N} - \frac{\pi p f_0}{f_N}\right) +
cos\left( \frac{\pi p f}{f_N} + \frac{\pi p f_0}{f_N}\right)\right]=$$
$$=2\frac{f_c}{f_N}+ 2\sum\limits^{N}_{p=1} r_p \sigma_p\left[
2cos\left( \frac{\pi p f}{f_N}\right) cos\left( \frac{\pi p f_0}{f_N}\right)\right]=$$
$$=2\frac{f_c}{f_N}+ 4\sum\limits^{N}_{p=1} r_p \sigma_p\left[
cos\left( \frac{\pi p f}{f_N}\right) cos\left( \frac{\pi p f_0}{f_N}\right)\right]$$

\begin{center}
%\includegraphics[width=0.5\textwidth]{filtro_lanczos_banda.pdf}
\end{center}

\begin{framed}
{\noindent \textbf MATLAB:}\\
{\textbf Programa para crear el filtro de Lanczos pasa-banda (Autor: Dr. Modesto Ortiz)}\\
{\noindent}N=10;   \%tamano filtro\\
p=1:N;  \%indices de tiempo \\
fc=0.2; \%frecuencia de corte a 0.5\\
f0=0.2; \%frecuencia central del filtro\\
fn=1.0; \%frecuencia de Nyquist \\
f=-fn:0.01:fn; \%generamos vector de frecuencias (positivas y negativas)\\
k=length(f);   \%dimension vector frecuencias \\

\%funcion pesos o respuesta en el dominio temporal
\\
rp=sin(pi*p*fc/fn)./(pi*p);\\
\%calculamos el factor sigma de suavizado (factores lanczos)
sigma=sin(2*pi*p/(2*N+1))./(2*pi*p/(2*N+1)); \\
%Calculamos la funcion respuesta de un filtro general en el
\%dominio espectral
\\
R=[];\\
for m=1:k; %recorremos todas las frecuencias\\
    R(m) = 2*fc/fn + 4*sum(rp.*sigma.*cos(p*pi*f0/fn)\\
    .*cos(p*pi*f(m)/fn));\\
end\\
lambda=4*fn/(2*N+1);\\
y0=[-0.2:0.1:1.2];x0=0.4*ones(1,length([-0.2:0.1:1.2]));\\
y1=[-0.2:0.1:1.2];x1=0.4+lambda*ones(1,length([-0.2:0.1:1.2]));\\
\%Graficamos\\
f1=figure;\\
plot(f,R,'b'); \\
hold on;\\
plot(x0,y0,'r-');\\
hold on;\\
plot(x1,y1,'r-');\\
grid on;\\
xlabel('f(ciclos/tiempo)','fontsize',18);\\
ylabel('R(f)','fontsize',18);\\
set(gca,'fontsize',18);\\
axis([-1 1 -0.2 1.2]);\\
\end{framed}

Para el filtro pasa bajas:
$$x(t)[{\rm filtrado}]=[r_p \sigma_p * x](t)\,,$$
o
$$x(t)[{\rm filtrado}]={\cal F}^{-1}[\widetilde{R}(f)X(f)]\,.$$

Para el filtro pasa-banda:
$$x(t)[{\rm filtrado}]=[r_p \sigma_p cos\left( \frac{\pi p f_0}{f_N}\right)*x](t)\,,$$
o
$$x(t)[{\rm filtrado}]={\cal F}^{-1}[\widetilde{R}_b(f)X(f)]\,.$$


Todo esto se puede programar facilmente en {\it Matlab} como se
muestra en el siguiente:

\begin{framed}
{\noindent \textbf MATLAB:}\\
{\textbf Programa para calcular los coeficientes de Lanczos pasa-baja y pasa-banda (Autor: Dr. Modesto Ortiz)}\\
{\noindent}function [h,Fh,Rh]=lnz\_co(fo,fc,fn,N,df);\\
\% INPUT:\\
\% fo = frecuencia central del filtro\\
\% fc = frecuencia de corte (al 50\% de la señal)= medio ancho de banda del filtro.\\
\% fn = frecuencia de Nyquist de los datos que se van a filtrar.\\
\% df = 1/(N*dt) de los datos que vamos a filtrar\\
\% N  = longitud del filtro\\
\% El número total de coeficientes es (N+1+N). Con esta definicion\\
\% aseguramos que le número total de coeficientes sea impar. Esto significa\\
\% que el filtro no cambia la fase de la señal filtrada.\\
\end{framed}


\% OUTPUT\\

\begin{framed}
\% h = coeficientes del filtro Lanczos en el dominio del tiempo\\
\% Fh = vector de frecuencias\\
\% Rh = respuesta en el dominio espectral\\

\%*************************************************************\\
Fh=[]; Rh=[];\\
lambda = 4*fn/(2*N+1); \% ancho de banda de transición\\

\% este valor debe ser menor que 1\\
lambda/(2*fc)\\

p= -N : N;\\
rp=sin(pi*p*fc/fn)./(pi*p);  \%coeficientes de Fourier del pulso cuadrado.\\
\%esta operación da una división por cero pero\\
\%no te asustes, se evalua con la funcion sinc.\\
\%rp=(fc/fn)*sinc(p*fc/fn);  \% esta es la misma pero no da division por cero\\

Sigma=sin(2*pi*p/(2*N+1))./(2*pi*p/(2*N+1)); \%factores de Lanczos, division por cero\\
\%Sigma=sinc(2*p/(2*N+1)); \% lo mismo pero sin division por cero\\
Co=fc/fn;\\
ind=find(p==0); \% se encuentra el índice de la división por cero. \\
h=rp.*Sigma;\\
h(ind)=Co; \% se suple el valor del coeficiente cero y ya tenemos
% los coeficientes del filtro pasa bajas.
\\
\%*************\\
if abs(fo) $>$ 0\\
 banda=cos(2*pi*p*fo/(2*fn));\\
 h=2*h.*banda;  \% coeficientes del filtro lanczos pasa banda centrado en fo\\
end\\
\%*************\\
\end{framed}

\begin{framed}
\% Por comodidad generamos la respuesta en frecuencia del filtro.\\

 Fh=-fn:df:fn; Fh=Fh';\\
\% for k=1:length(Fh);\\
\% Rh(k)=sum( h.*cos( 2*pi*p*Fh(k)/(2*fn) ) );\\
\% end;\\
\% \\
xdum=length(Fh); ydum=length(p);\\
A = zeros(xdum,ydum);\\
dum=2*pi/(2*fn);\\
A = cos(dum*Fh*p);\\

M=length(h);\\
h=reshape(h,M,1);\\

Rh = A*h;\%multiplicamos la matriz de cosenos por la senal (= a suma comentada de arriba)\\

\end{framed}

Y una vez tenemos los coeficientes solamente tenemos que convolucionar o
multiplicar:

\begin{framed}
{\noindent \textbf MATLAB:}\\
{\textbf Programa para filtrar con Lanczos en el dominio temporal (Autor: Dr. Modesto Ortiz)}\\
\noindent \% Función lnz\_fi.m\\
\%INPUT:\\
\% z = serie de tiempo sin filtrar \\
\% h = coeficientes del filtro\\
\%OUTPUT\\
\% zf = serie de tiempo filtrada \\
\%*****************************\\
{\noindent}function [zf] = lnz\_fi(z,h);\\
m = length(z);\\
zf = conv (z , h); \\
\% se obtiene la serie filtrada convolucionando la serie original con los pesos o coeficientes del filtro.\\
zf = wkeep(zf,m); \% esta rutina trunca los extremos de zf para que el número de \\
\%datos de la serie filtrada sea igual al número de datos de la serie original. El objetivo de \\
\%esto es poder graficar en un mismo eje de tiempo ambas series para poder observar la \\
\%bondad del filtro.\\
\end{framed}

Aqui se muestra un ejemplo sintético del uso de las subrutinas presentadas.

\begin{framed}
{\noindent \textbf MATLAB:}\\
{\textbf Ejemplo de filtrado con Lanczos (Autor: Dr. Modesto Ortiz)}\\
{\noindent}dt=1; k=200; T=k*dt; df=1/(k*dt); t=(0:1:k-1)*dt; \%eje de tiempo.\\
f1=0.005; f2=0.1; f3=0.25; \% tres frecuencias\\
z=cos(2*pi*f1*t)+cos(2*pi*f2*t)+cos(2*pi*f3*t); \%serie de tiempo sintética.
\\
\%filtramos pasa-banda para quedarnos con la frecuencia f2. \\
fo=f2; \%frecuencia central del filtro\\
\%Medio ancho de banda del filtro \\
fc=0.03;\\
fn=0.5; \% frecuencia de Nyquist\\
N = 30; \% recordemos que el número total de coeficientes será N+1+N. \\
$[{\rm h},{\rm Fh},{\rm Rh}]$=lnz\_co(fo,fc,fn,N,df);\\
zf = lnz\_fi(z,h); \%serie filtrada y listo.
\end{framed}

\section{Temas selectos}
\subsection{\textbf \noindent Análisis Armónico}
Se trata de un ajuste por mínimos cuadrados de una serie temporal dominada por
armónicos específicos. Por ejemplo, en el caso del océano, es muy común
encontrar en series temporales de temperatura, salinidad, velocidad, etc.. señales
de las mareas que no son nada mas que corrientes periódicas generadas por fuerzas
astronómicas con una frecuencia de oscilación determinada (24h, 12h, etc...).
El método consiste en elejir las frecuencias de los armónicos y usar mínimos
cuadrados para ajustarlos a la serie temporal. Supongamos $M$ armónicos a ajustar

$$y(t_n) = \overline{y(t)} + \sum\limits^{M}_{q=1}C_q cos(\omega_q t_n -\theta_q)+y_r(t_n)\,,$$

donde $\overline{y(t)}$ es el promedio de la serie, y $y_r(t_n)$ es el residuo de la
serie temporal (donde hay el resto de armónicos presentes en la serie),
$\omega_q=2\pi q/N\Delta t$. En términos de las amplitudes $A_q$ y $B_q$

$$y(t_n) = \overline{f(t)} + \sum\limits^{M}_{q=1}[A_q cos(\omega_q t_n)+B_q sen(\omega_q t_n)] +y_r(t_n)\,,$$
donde
$$C_q=\sqrt{A_q^2+B_q^2}\,,\,\,\,\,q=1,2,....$$
$$\theta_q=arctg[B_q/A_q]\,,\,\,\,\,q=1,2,...$$

Antes de empezar el análisis debemos de extraer la media, $\overline{y}$, a la serie
temporal. El método de mínimos cuadrados consiste en minimizar la suma de los
errores cuadrados $SEC$, es decir,
$$SEC= \sum\limits^N_{n=1} y_r^2(t_n) = \sum\limits^N_{n=1}  \left( y(t_n) - \left[ \overline{y(t)} +
 \sum\limits^{M}_{q=1} A_q cos(\omega_q t_n)+B_q sen(\omega_q t_n) \right] \right)^2 =$$
$$=\sum\limits^N_{n=1}  \left(y(t_n) - \left[ \overline{y(t)} +
\sum\limits^{M}_{q=1} A_q cos(2\pi q n/ N)+B_q sen(2\pi q n/ N) \right] \right)^2$$

Como siempre derivamos respecto los coeficientes e igualamos a cero para obtener un sistema de
$2M+1$ equaciones
$$\frac{\partial{SEC}}{\partial{A_q}}=0=2\sum\limits^N_{n=1} \left(y(t_n) - \left[ \overline{y(t)} +
\sum\limits^{M}_{q=1} A_q cos(2\pi q n/ N)+B_q sen(2\pi q n/ N) \right]-cos(2\pi q n/ N) \right)$$
$$\frac{\partial{SEC}}{\partial{B_q}}=0=2\sum\limits^N_{n=1} \left(y(t_n) - \left[ \overline{y(t)} +
\sum\limits^{M}_{q=1} A_q cos(2\pi q n/ N)+B_q sen(2\pi q n/ N) \right]-sen(2\pi q n/ N) \right)$$

Soluciones del sistema requiere una equación matricial de la forma
${\textbf D}{\textbf z}={\textbf y}$, donde
$${\textbf D}=\left( \begin{array}{cccccccccccccc}
  N & c_1 & c_2 & ... & c_M & s_1 & s_2 & ... & s_M \\
  c_1 & cc_{11} & cc_{12} & ... & cc_{1M} & cs_{11} & cs_{12} & ... & cs_{1M} \\
  c_2 & cc_{21} & cc_{22} & ... & cc_{2M} & cs_{21} & cs_{22} & ... & cs_{2M} \\
  ... & ... & ... & ... & ... & ... & ... & ... & ... \\
  ... & ... & ... & ... & ... & ... & ... & ... & ... \\
  c_M & cc_{M1} & cc_{M2} & ... & cc_{MM} & cs_{M1} & cs_{M2} & ... & cs_{MM} \\
  s_1 & sc_{11} & sc_{12} & ... & sc_{1M} & ss_{11} & ss_{12} & ... & ss_{1M} \\
  s_2 & sc_{21} & sc_{22} & ... & sc_{2M} & ss_{21} & ss_{22} & ... & ss_{2M} \\
  ... & ... & ... & ... & ... & ... & ... & ... & ... \\
  ... & ... & ... & ... & ... & ... & ... & ... & ... \\
  s_M & sc_{M1} & sc_{M2} & ... & sc_{MM} & ss_{M1} & ss_{M2} & ... & ss_{MM} \\
      \end{array} \right)$$
y

$${\textbf y}=\left( \begin{array}{ccc}
yc_0 \\
yc_1 \\
yc_2 \\
... \\
... \\
yc_M \\
ys_1 \\
ys_2 \\
... \\
... \\
ys_M \\
\end{array} \right)\,\,\,\,\,\,\,\,\,\,
{\textbf z}=\left( \begin{array}{ccc}
A_0\\
A_1\\
A_2\\
...\\
...\\
A_M\\
B_1\\
B_2\\
...\\
...\\
B_M
\end{array} \right)$$

Los coeficientes de las matrices son:

$$yc_i=\sum\limits^N_{n=1}y(t_n)cos(\omega_i t_n)\,\,\,\,\,,\,\,\,\,\,ys_i=\sum\limits^N_{n=1}y(t_n)sen(\omega_i t_n)$$
$$c_i=\sum\limits^N_{n=1}cos(\omega_i t_n)\,\,\,\,\,,\,\,\,\,\,s_i=\sum\limits^N_{n=1}sen(\omega_i t_n)$$
$$cc_{ij}=cc_{ji}=\sum\limits^N_{n=1}[cos(\omega_i t_n)cos(\omega_j t_n)]$$
$$ss_{ij}=ss_{ji}=\sum\limits^N_{n=1}[sen(\omega_i t_n)sen(\omega_j t_n)]$$
$$cs_{ij}=sc_{ji}=\sum\limits^N_{n=1}[cos(\omega_i t_n)sen(\omega_j t_n)]\,,$$
donde $t_n= n \Delta$, $\omega_i = 2\pi f_i$ es la frecuencia angular
de las componentes de interés $i$, y $\phi_i(n)=\omega_i t_n$ es el argumento
de las funciones de Fourier.

\vspace{0.25cm}
{\textbf \noindent Ejemplo de Ajuste de armónicos
(Emery and Thompson, p395)}

Asumamos la siguiente serie temporal de temperatura promedia mensual.
\\

\begin{tabular}{|l|cccccccccccc|}
\hline
T ($^\circ C$) & 7.6 & 7.4 & 8.2 & 9.2 & 10.2 & 11.5 & 12.4 & 13.4 & 13.7 & 11.8 & 10.1 & 9\\
\hline
Año 2 & 8.9 & 9.5 & 10.6 & 11.4 & 12.9 & 12.7 & 13.9 & 14.2 & 13.5 & 11.4 & 10.9 & 8.1\\
\hline
\end{tabular}
\\
\\

Deseamos encontrar las componentes mareales dominantes en la serie temporal de
temperatura. A simple vista podemos ver que existe una frecuencia dominante
semianual. Por tanto, vamos buscar las amplitudes y frecuencias de interés,
es decir, de las componentes anual y semianual que tienen unas frecuencias de
$f_1=1/12$ meses ($=0.0833\,{\rm cpm}$) y
$f_2=1/24$ meses ($=0.1667\,{\rm cpm}$). Los argumentos de las funciones de
Fourier son $\phi_1(n)=\omega_1 t_n=2\pi(1/12)*n*\Delta t=(\pi/6)*n*1=n\pi/6$  y
$\phi_2(n)=\omega_2 t_n=2\pi(1/6)*n*\Delta t=(\pi/3)*n*1=n\pi/3$
Para este problema las matrices
son
\\

$${\textbf D}=\left( \begin{array}{ccccc}
  N & c_1 & c_2 & s_1 & s_2 \\
  c_1 & cc_{11} & cc_{12} & cs_{11} & cs_{12}\\
  c_2 & cc_{21} & cc_{22} & cs_{21} & cs_{22} \\
  s_1 & sc_{11} & sc_{12} & ss_{11} & ss_{12} \\
  s_2 & sc_{21} & sc_{22} & ss_{21} & ss_{22} \\
      \end{array} \right)=$$
      $$=\scriptsize
 \left( \begin{array}{ccccc}
 N & c_1 & c_2 & s_1 & s_2 \\
  c_1 & \sum\limits^N_{n=1}[cos(\phi_1(n))cos(\phi_1(n))] & cc_{12} & cs_{11} & cs_{12}\\
  c_2 & cc_{21} & \sum\limits^N_{n=1}[cos(\phi_1(n))cos(\phi_2(n))] & cs_{21} & cs_{22} \\
  s_1 & sc_{11} & sc_{12} & \sum\limits^N_{n=1}[sen(\phi_1(n))sen(\phi_1(n))] & ss_{12} \\
  s_2 & sc_{21} & sc_{22} & ss_{21} & \sum\limits^N_{n=1}[sen(\phi_2(n))sen(\phi_2(n))] \\
      \end{array} \right) =
      $$
$$
=
 \left( \begin{array}{ccccc}
  24 & 0 & 0 & 0 & 0 \\
  0 & 12 & 0 & 0 & 0\\
  0 & 0 & 12 & 0 & 0 \\
  0 & 0 & 0 & 12 & 0 \\
  0 & 0 & 0 & 0 & 12 \\
      \end{array} \right)
$$

La matriz
$${\textbf y}=\left( \begin{array}{ccc}
\sum\limits^N_{n=1}y(t_n)cos(\omega_0 t_n) \\
\sum\limits^N_{n=1}y(t_n)cos(\omega_1 t_n) \\
\sum\limits^N_{n=1}y(t_n)cos(\omega_2 t_n) \\
\sum\limits^N_{n=1}y(t_n)sen(\omega_1 t_n) \\
\sum\limits^N_{n=1}y(t_n)sen(\omega_2 t_n) \\
\end{array} \right)=
\left( \begin{array}{ccc}
262.5 \\
-21.45 \\
-5.4 \\
-23.76 \\
-0.51 \\
\end{array} \right)\,\,^\circ{C}$$

Finalmente encontramos las amplitudes de los armónicos resolviendo el sistema
$${\textbf z}=({\textbf D}^T{\textbf D})^{-1}{\textbf D}^{T}{\textbf y}={\textbf D}^{-1}{\textbf y}=\left( \begin{array}{ccc}
10.93\\
-1.78\\
-0.45\\
-1.98\\
-0.04
\end{array} \right)$$

El coeficiente de correlación entre la señal original y la serie de Fourier con
2 armónicos es $r^2=0.92$, es decir, solamente con 2 armónicos podemos explicar
el 92\% de la varianza total.

\subsection{\textbf \noindent Demodulación compleja}

Este método es utilizado para conocer el comportamiento de una componente
o armónico con frecuencia particular $\omega$, tal como la marea diurna, o
semidiurna, o las ondas inerciales. Aqui vamos a mostrar la forma
clásica de demodular que consiste en ajustar por fragmentos de la serie
un armónico teórico utilizando mínimos cuadrados. Cada fragmento de la
serie debe, como mínimo, contener un ciclo del armónico a demodular.
Para cada segmento, la anomalía de la componente de velocidad a
la frecuencia de interés $\omega$ es
$${\textbf u} - \overline{\textbf u}=[u(t)-\overline{u(t)} +iv(t)-\overline{v(t)}]=$$
$$R^+ e^{i(\omega t + \epsilon^+)} + R^- e^{-i(\omega t + \epsilon^-)}\,,$$
donde $\overline{u(t)}$, $\overline{v(t)}$ son las componentes de la velocidad
promedio, $R^+,\,\,R^-$ y $\epsilon^+\,\,\epsilon^-$ son las amplitudes y fases
de las componentes rotatorias que giran en el sentido de las agujas del reloj (+)
y en el sentido contrario (-). La serie temporal esta definida para cada
$t_k\,(k=1,2,....,N)$ y las soluciones son encontradas resolviendo el sistema
de ecuaciones
$${\textbf z}={\textbf D}^{-1}{\textbf y}\,,$$
donde
$${\textbf y}=\left( \begin{array}{c}
u(t_1) \\
u(t_2) \\
... \\
u(t_n) \\
v(t_1) \\
v(t_2) \\
... \\
v(t_n) \\
\end{array} \right)\,;\,\,\,
{\textbf z}=\left( \begin{array}{c}
R^+cos(\epsilon^+) \\
R^+sen(\epsilon^+)\\
R^-cos(\epsilon^-) \\
R^-sen(\epsilon^-)\\
\end{array} \right)=
\left( \begin{array}{c}
ACP \\
ASP\\
ACM \\
ASM\\
\end{array} \right)\,,$$
y la matriz ${\textbf D}$ es
$${\textbf D}=\left( \begin{array}{cccc}
cos(\omega t_1) & -sen(\omega t_1) & cos(\omega t_1) & sen(\omega t_1) \\
cos(\omega t_2) & -sen(\omega t_2) & cos(\omega t_2) & sen(\omega t_2) \\
... \\
cos(\omega t_n) & -sen(\omega t_n) & cos(\omega t_n) & sen(\omega t_n) \\
sen(\omega t_1) & cos(\omega t_1) & -sen(\omega t_1) & cos(\omega t_1) \\
sen(\omega t_2) & cos(\omega t_2) & -sen(\omega t_2) & cos(\omega t_2) \\
... \\
sen(\omega t_n) & cos(\omega t_n) & -sen(\omega t_n) & cos(\omega t_n) \\
\end{array} \right)\,.$$

Una vez los valores de ${\textbf z}$ son encontrados a partir de la solución
de mínimos cuadrados de arriba, podemos encontrar los parámetros de la
elipse como:
$$R^+=\sqrt{\left( ASP^2 + ACP^2\right)}\,;\,\,\,\,\,R^-=\sqrt{\left( ASM^2 + ACM^2\right)}$$
$$\epsilon^+=tan^{-1}\left(\frac{ASP}{ACP}\right)\,;\,\,\,\,\,\epsilon^-=tan^{-1}\left(\frac{ASM}{ACM}\right)$$

Por ejemplo, si queremos demodular la amplitud y fase de las ondas inerciales
observadas en un anclaje situado en latitudes medias, debemos de usar una
frecuencia $\omega=2\Omega sen\phi$ y ajustar por mínimos cuadrados
segmentos de $24\,{\rm h}$ sin superposición. La serie temporal medida
por el anclaje debería de ser horaria para que existan mas datos por
segmento que parámetros a ajustar.

Otra forma, tal vez mas sencilla, es la siguiente. Imaginemos que la serie
original es $X(t)$ y se asume como una señal periódica con frecuencia
igual a la de interés mas otras cosas que llamamos $Z(t)$
$$X(t)=A(t)cos\left(\omega t + \varphi(t)\right) + Z(t)= \frac{1}{2}A(t)
       \left[e^{i(\omega t + \varphi(t))} + e^{-i(\omega t + \varphi(t))} \right] + Z(t)\,,$$
donde la amplitud $A(t)$ y la fase $\varphi(t)$ de la señal periódica
se asumen que dependen del tiempo pero que varían "lentamente" en comparación
a la frecuencia $\omega$.

Para demodular tenemos que:
\\

(1) Multiplicar $X(t)$ por $e^{-i\omega t}$:
$$Y(t)=X(t)e^{-i\omega t}=\frac{1}{2}A(t)
       \left[e^{i(\omega t + \varphi(t))} + e^{-i(\omega t + \varphi(t))} \right]e^{-i\omega t} + Z(t)e^{-i\omega t}=$$
$$=\frac{1}{2}A(t)e^{i(\omega t + \varphi(t))}e^{-i\omega t} + \frac{1}{2}A(t)e^{-i(\omega t + \varphi(t))}e^{-i\omega t} +
Z(t)e^{-i\omega t}=$$
$$=\underbrace{\frac{1}{2}A(t)e^{i\varphi(t)}}_{(a)} + \underbrace{\frac{1}{2}A(t)e^{-i(2\omega t + \varphi(t))}}_{(b)} +
    \underbrace{Z(t)e^{-i\omega t}}_{(c)}\,.$$

El término (a) varía lentamente ya que $\varphi(t)$ también lo hace y no tiene energía (potencia espectral)
a la frecuencia de demodulación $\omega$ o arriba de ella. El término (b) oscila a dos veces la frecuencia
de demodulación, i.e., $2\omega$. El término (c) varía a la frecuencia $\omega$. Debido a que
$Z(t)$ no tiene energía a la frecuencia $\omega$, entonces el término (c) no tendrá tampoco energía
en la frecuencia cero, i.e., $\omega=0$.
\\

(2) Filtro pasa-bajas de la serie $Y(t)$ para eliminar las ondas con frecuencia $\omega$ o por encima de
$\omega$. Esto eliminará prácticamente los términos (b) y (c), y suavizará (a). El resultado es
$$Y_s(t)=\frac{1}{2}A_s(t)e^{i\varphi_s(t)}\,,$$
donde el subíndice $_s$ significa suavizado o filtro pasa-bajas.
\\

(3) Extraer $A_s(t)$ y $\varphi_s(t)$:
$$\frac{1}{2}A_s(t)=|Y_s(t)|=2\left( Re[Y_s]^2 + Im[Y_s]^2\right)^{1/2}$$
$$e^{i\varphi(t)}=2\frac{Y_s(t)}{A_s(t)}\,;\,\,\,\,\,\varphi_s(t)=tan^{-1}\left(\frac{Im[Y_s]}{Re[Y_s]}\right)$$

Al suavizar hacemos dos cosas. Primero, eliminamos los términos no deseados
(a) y (b). El tipo de filtrado o suavizado determina la anchura de la banda de frecuencias
de las oscilaciones retenidas. Por ejemplo, si usamos un triángulo (ventana triangular)
de longitud $2T-1$ donde $T=2\pi/\omega$ es el periodo de demodulación, entonces para
la banda para la potencia-media ($3\,{\rm dB}$ desde el pico) será
$\omega \in [T/(1+0.44295),T/(1-0.44295)]$. Potencia media se refiere a la frecuencia
a la cual la potencia se ha reducido a la mitad de su valor medio de la banda. Segundo,
el filtrado suaviza las series de amplitud y la fase.
\\

(4) La elección de la frecuencia de demodulación $\omega$ se puede validar ajustando
localmente una línea a la fase,$\varphi\simeq a + bt$. Típicamente esto lo haremos
en fragmentos de longiutd $T$. De esta forma si seleccionamos el origen en el tiempo
central de cada fragmento (tal que $a\simeq 0$) obtenemos que $cos(\omega t + \varphi)\simeq cos(\omega t + bt)=cos(\hat{\omega} t)$.
La frecuencia ajustada $\hat{\omega}=\omega + a$ es una validación de la elección inicial de
nuestra frecuencia de demodulación $\omega$.
\begin{center}
%\includegraphics[width=0.5\textwidth]{half_power_bandwidth.pdf}
\end{center}



## Acknowledgments

Ayuda de Muchos

## Open research

Disponible para todos

## References {.unnumbered}

:::{#refs}

:::
